{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I0ekpXp0UE1"
      },
      "source": [
        "# Introduction to RL\n",
        "\n",
        "This study started by asking the following question, can we actually do SFT with just thinking examples on the thinking models? Why do we need this new loss function?\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/dpo/3.webp?123\" width=600px>\n",
        "\n",
        "So I started by revisiting the basics: sigmoid, cross-entropy, KL divergence.\n",
        "\n",
        "The first section contains all the theoritcal information and basic that I needed to understand how things work under the cover.\n",
        "\n",
        "The second section contains implementation details to better understand how to implement it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnhJ7sZIBeSI"
      },
      "source": [
        "## References\n",
        "I read, borrowed, and copied code from these references.\n",
        "\n",
        "* [understanding-the-math-behind-grpo-deepseek-r1-zero](https://medium.com/yugen-ai-technology-blog/understanding-the-math-behind-grpo-deepseek-r1-zero-9fb15e103a0a)\n",
        "* [DeepSeek GRPO related paper](https://arxiv.org/abs/2402.03300)\n",
        "* [The Illustrated Deepseek-R1](https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1)\n",
        "* [Huggingface LLM RL course](https://huggingface.co/learn/llm-course/chapter12/1?fw=pt)\n",
        "* [unsloth: RL Slides](https://docs.google.com/presentation/d/1Jh5p_JDXt4eLD0ireaHJjJNpzqSF8E1WTwIHeojyjNU/edit?usp=sharing)\n",
        "* [unsloth: RL Notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb)\n",
        "* [DPO from scatch](https://github.com/charbull/LLMs-from-scratch/blob/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb)\n",
        "* [State of LLM Reasoning](https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training)\n",
        "* [Reinforcement finetuning with GRPO](https://learn.deeplearning.ai/courses/reinforcement-fine-tuning-llms-grpo)\n",
        "* [Magistral paper](https://mistral.ai/static/research/magistral.pdf)\n",
        "* [DPO paper](https://arxiv.org/pdf/2305.18290)\n",
        "* [Gemma3 blog](https://huggingface.co/blog/gemma3)\n",
        "* [TRL GRPO with HF](https://colab.sandbox.google.com/github/huggingface/cookbook/blob/main/notebooks/en/fine_tuning_llm_grpo_trl.ipynb)\n",
        "* [To run on Mac Locally using MLX](https://www.linkedin.com/pulse/fine-tuning-gemma-3-1b-build-tool-calling-agent-mihir-jha--nqrof/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3-CaJjav7ns"
      },
      "source": [
        "## Logistic (Sigmoid) function\n",
        "\n",
        "The **logistic function**, also very commonly known as the **sigmoid function** (though \"sigmoid\" can refer to a broader class of S-shaped functions), is a mathematical function that produces an S-shaped curve (a sigmoid curve).\n",
        "\n",
        "Its primary characteristic and the reason it's so widely used in machine learning is that it takes **any real-valued number as input and squashes it into a value between 0 and 1.**\n",
        "\n",
        "**Mathematical Formula:**\n",
        "\n",
        "The most common form of the logistic function is:\n",
        "\n",
        "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
        "\n",
        "Where:\n",
        "*   $x$ is the input value (can be any real number).\n",
        "*   $e$ is Euler's number (the base of the natural logarithm, approximately 2.71828).\n",
        "\n",
        "Sometimes you might also see an equivalent form:\n",
        "$\\sigma(x) = \\frac{e^x}{e^x + 1}$\n",
        "(You can get this by multiplying the numerator and denominator of the first form by $e^x$).\n",
        "\n",
        "**Key Properties and What it Means:**\n",
        "\n",
        "1.  **S-Shaped Curve:** If you plot it, it looks like a stretched \"S\".\n",
        "    *   For very large negative inputs ($x \\rightarrow -\\infty$), the output $\\sigma(x)$ approaches 0.\n",
        "    *   For very large positive inputs ($x \\rightarrow +\\infty$), the output $\\sigma(x)$ approaches 1.\n",
        "    *   When the input $x=0$, the output $\\sigma(0) = \\frac{1}{1 + e^0} = \\frac{1}{1+1} = 0.5$. This is the midpoint of the S-curve.\n",
        "\n",
        "2.  **Output Range (0, 1):**\n",
        "    *   This is crucial. Because its output is always between 0 and 1 (exclusive of 0 and 1 unless $x$ is $\\pm\\infty$), it's perfectly suited for representing **probabilities**.\n",
        "    *   An output of 0.7 can be interpreted as a 70% probability of something occurring.\n",
        "\n",
        "3.  **Monotonically Increasing:** As the input $x$ increases, the output $\\sigma(x)$ also increases (or stays the same, but never decreases).\n",
        "\n",
        "4.  **Differentiable:** It has a smooth, well-behaved derivative, which is important for optimization algorithms like gradient descent used in training machine learning models. The derivative is $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$.\n",
        "\n",
        "\n",
        "The core idea of logistic regression is to model the probability of a binary outcome (e.g., yes/no, 0/1, spam/not-spam). It does this by first calculating a linear combination of the input features, and this linear combination **is the logit**.\n",
        "\n",
        "Here's the breakdown:\n",
        "\n",
        "1.  **Input Features (Independent Variables):**\n",
        "    You have a set of input features (or independent variables) for each data point. Let's say you have `n` features:\n",
        "    $x_1, x_2, ..., x_n$\n",
        "\n",
        "    *   Example: If you're predicting if a student passes an exam (binary outcome: Pass/Fail), your features might be:\n",
        "        *   $x_1$: hours studied\n",
        "        *   $x_2$: previous exam score\n",
        "        *   $x_3$: attendance percentage\n",
        "\n",
        "2.  **Weights (Coefficients) and Bias (Intercept):**\n",
        "    The logistic regression model learns a set of **weights** (also called coefficients) for each feature, and a **bias** term (also called the intercept).\n",
        "    *   Weights: $w_1, w_2, ..., w_n$\n",
        "    *   Bias: $b$ (or $w_0$)\n",
        "\n",
        "    These weights represent the importance or influence of each feature on the outcome. The bias term allows the model to have some baseline tendency even when all feature values are zero. These weights and the bias are learned during the model training process using your labeled data.\n",
        "\n",
        "3.  **The Linear Combination (Calculating the Logit):**\n",
        "    The logit, often denoted as $z$, is calculated as a weighted sum of the input features plus the bias term. This is a linear equation, just like in linear regression:\n",
        "\n",
        "    $z = (w_1 \\cdot x_1) + (w_2 \\cdot x_2) + ... + (w_n \\cdot x_n) + b$\n",
        "\n",
        "    Or, if you include the bias as $w_0$ and assume an $x_0$ that is always 1:\n",
        "    $z = w_0 \\cdot x_0 + w_1 \\cdot x_1 + w_2 \\cdot x_2 + ... + w_n \\cdot x_n$\n",
        "    $z = \\sum_{i=0}^{n} w_i \\cdot x_i$\n",
        "    (where $x_0 = 1$)\n",
        "\n",
        "    **This value $z$ is the logit.**\n",
        "\n",
        "    *   The logit can be any real number (positive, negative, or zero).\n",
        "        *   A large positive logit suggests a high likelihood of the positive class.\n",
        "        *   A large negative logit suggests a high likelihood of the negative class.\n",
        "        *   A logit near zero suggests uncertainty.\n",
        "\n",
        "4.  **Why is it called \"Logit\"? (The Log-Odds Connection)**\n",
        "    The term \"logit\" comes from the fact that this value $z$ is considered to be the **logarithm of the odds** (log-odds) of the positive class occurring.\n",
        "\n",
        "    *   Let $P$ be the probability of the positive class (e.g., $P(\\text{Pass})=1$).\n",
        "    *   The odds of the positive class are: $\\text{Odds} = \\frac{P}{1-P}$\n",
        "    *   The logit is then: $z = \\log\\left(\\frac{P}{1-P}\\right)$\n",
        "\n",
        "5.  **Converting the Logit to a Probability:**\n",
        "    After calculating the logit $z$, the logistic regression model then applies the **logistic (sigmoid) function** to it to squash this value into a probability between 0 and 1:\n",
        "\n",
        "    $P(\\text{positive class}) = \\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
        "\n",
        "    This $P$ is the model's predicted probability that the data point belongs to the positive class.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Let's say we're predicting if a loan application will be approved (Approved=1, Rejected=0).\n",
        "Features:\n",
        "*   $x_1$: Credit Score (e.g., 700)\n",
        "*   $x_2$: Income (in thousands, e.g., 50)\n",
        "\n",
        "Learned weights and bias from training:\n",
        "*   $w_1 = 0.01$ (for Credit Score)\n",
        "*   $w_2 = 0.05$ (for Income)\n",
        "*   $b = -5.0$ (bias)\n",
        "\n",
        "For a new applicant with Credit Score = 720 and Income = 60:\n",
        "\n",
        "1.  **Calculate the Logit ($z$):**\n",
        "    $z = (w_1 \\cdot x_1) + (w_2 \\cdot x_2) + b$\n",
        "    $z = (0.01 \\cdot 720) + (0.05 \\cdot 60) + (-5.0)$\n",
        "    $z = 7.2 + 3.0 - 5.0$\n",
        "    $z = 5.2$\n",
        "\n",
        "    So, the logit for this applicant is 5.2.\n",
        "\n",
        "2.  **Convert Logit to Probability (using sigmoid):**\n",
        "    $P(\\text{Approved}) = \\sigma(5.2) = \\frac{1}{1 + e^{-5.2}}$\n",
        "    $P(\\text{Approved}) \\approx \\frac{1}{1 + 0.0055}$\n",
        "    $P(\\text{Approved}) \\approx \\frac{1}{1.0055} \\approx 0.9945$\n",
        "\n",
        "The model predicts a ~99.45% probability of loan approval for this applicant.\n",
        "\n",
        "**In summary:** The logit in logistic regression is obtained by taking a linear combination (weighted sum + bias) of the input features. This logit value then serves as the input to the logistic (sigmoid) function, which transforms it into a probability. The weights used in this linear combination are what the model learns during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NCca2Va1CFQ"
      },
      "source": [
        "## Log probability\n",
        "\n",
        "### Numerical Stability (Avoiding Underflow/Overflow):\n",
        "* Probabilities are small: The probability of a specific long sequence of tokens (like a sentence or a piece of code) is the product of the probabilities of each token given the previous ones. For example, $ P(sequence) = P(token_1) * P(token_2|token_1) * P(token_3|token_1, token_2) * .... $\n",
        "\n",
        "Each of these individual token probabilities is between 0 and 1. When you multiply many numbers between 0 and 1 together, the result becomes extremely small, very quickly.\n",
        "\n",
        "* Underflow: Standard floating-point numbers in computers have limited precision. If a number becomes too small, it gets rounded down to zero (underflow). Once you hit zero, all subsequent multiplications will also be zero, and you lose all information.\n",
        "\n",
        "* Logarithms convert products to sums: $log(A * B * C) = log(A) + log(B) + log(C)$.\n",
        "Logarithms of numbers between 0 and 1 are negative. Summing negative numbers is much more numerically stable. The numbers might become large negative values, but they are less likely to hit the limits of floating-point representation as quickly as products hit zero.\n",
        "Example: 0.1 * 0.01 * 0.2 = 0.0002.\n",
        "log(0.1) + log(0.01) + log(0.2) ≈ -2.30 + (-4.60) + (-1.60) = -8.50.\n",
        "For longer sequences, the product would get much smaller, while the sum of logs would just get more negative.\n",
        "\n",
        "* Logarithms convert fractions to substractions: $\\log\\left(\\frac{a}{b}\\right) = \\log a - \\log b$\n",
        "\n",
        "### Mathematical Convenience and Simplicity:\n",
        "\n",
        "* Easier derivatives for optimization: Many loss functions involve probabilities (e.g., cross-entropy loss). When you take the derivative of a log-probability term, it often simplifies nicely. For instance, the derivative of log(f(x)) is f'(x)/f(x). This 1/f(x) term can cancel out parts of the original probability expression, especially when dealing with distributions like the softmax (which involves exponentials that are undone by the logarithm). Example (Softmax): The output of a softmax layer is $ p_i = exp(z_i) / Σ_j exp(z_j) $. The cross-entropy loss for a correct class c is -log(p_c). Taking the derivative of -log(p_c) with respect to z_i (the logits) becomes much simpler than differentiating -p_c.\n",
        "\n",
        "* Maximizing log-likelihood: In statistics, we often want to find parameters that maximize the likelihood (or probability) of observing our data. Since the logarithm is a monotonically increasing function, maximizing log(P(data|parameters)) is equivalent to maximizing P(data|parameters), but log(P) is usually much easier to work with mathematically.\n",
        "\n",
        "### Information Theoretic Interpretation:\n",
        "* Cross-Entropy: The negative log-probability of the true class/token is directly related to the cross-entropy loss. Cross-entropy measures the \"distance\" or \"inefficiency\" when encoding events from one distribution using an optimal code for another distribution. Minimizing -log(P(correct_token)) is equivalent to minimizing the cross-entropy between the model's predicted distribution and the true one-hot distribution.\n",
        "\n",
        "* KL Divergence: The DPO/GRPO loss formulation (and the underlying reward model it implicitly optimizes for) is closely related to the Kullback-Leibler (KL) divergence. KL divergence, which measures the difference between two probability distributions, is also defined using logarithms: $ KL(P || Q) = Σ P(x) log(P(x)/Q(x))$. The terms $ log(π_θ(y|x) / π_ref(y|x)) $ in the GRPO loss are essentially log probability ratios, which feature prominently in KL divergence.\n",
        "\n",
        "### Gradient Scaling:\n",
        "\n",
        "Using log probabilities can sometimes lead to better-behaved gradients during training. Gradients derived from products of many small probabilities can become vanishingly small, hindering learning. Logarithms help spread out these values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg40cPv92bT-"
      },
      "source": [
        "## Cross Entropy\n",
        "\n",
        "The Core Idea: Measuring the \"Surprise\" or \"Difference\"\n",
        "Imagine you have:\n",
        "\n",
        "* The True Distribution (P): This represents the actual, real-world probabilities of different events or classes. In supervised learning, this is your ground truth label.\n",
        "* The Predicted Distribution (Q): This is what your model predicts for the probabilities of those same events or classes.\n",
        "\n",
        "Cross-entropy measures how different the predicted distribution Q is from the true distribution P. More specifically, it quantifies the average number of bits needed to identify an event drawn from P if you use a coding scheme optimized for Q instead of P.\n",
        "* If Q is very similar to P, the cross-entropy will be low.\n",
        "* If Q is very different from P, the cross-entropy will be high.\n",
        "\n",
        "Why is it a \"Loss\" Function?\n",
        "\n",
        "In machine learning, we want our model's predictions (Q) to be as close as possible to the true labels (P). So, we use cross-entropy as a loss function:\n",
        "High cross-entropy means our model is making poor predictions (its Q is far from P). This results in a large loss, signaling the model to adjust its parameters significantly.\n",
        "\n",
        "Low cross-entropy means our model is making good predictions (its Q is close to P). This results in a small loss.\n",
        "The goal of training is to minimize this cross-entropy loss.\n",
        "The Formula\n",
        "For discrete probability distributions P and Q over a set of C classes (or events):\n",
        "\n",
        "$ H(P, Q) = - Σ_i P(i) * log(Q(i)) $\n",
        "\n",
        "Where:\n",
        "\n",
        "* $H(P, Q)$ is the cross-entropy between distributions P and Q.\n",
        "* $Σ_i$ means \"sum over all possible classes/events i from 1 to C\".\n",
        "* P(i) is the true probability of class i.\n",
        "* Q(i) is the model's predicted probability for class i.\n",
        "* log is usually the natural logarithm (ln), though base 2 is also used in pure information theory contexts (giving units of bits). In ML, the base doesn't fundamentally change the optimization.\n",
        "\n",
        "\n",
        "The Most Common Case in Machine Learning: Single-Label Classification\n",
        "In many classification tasks (e.g., image classification: \"cat,\" \"dog,\" \"bird\"), an input belongs to only one true class.\n",
        "True Distribution (P): This is represented as a one-hot encoded vector.\n",
        "If there are 3 classes and the true class is \"dog\" (say, class 2), then P = [0, 1, 0].\n",
        "\n",
        "This means P(class_1_cat) = 0, P(class_2_dog) = 1, P(class_3_bird) = 0.\n",
        "Predicted Distribution (Q): This comes from your model's output layer, typically after a softmax function, which ensures the probabilities sum to 1.\n",
        "For example, Q = [0.1 (cat), 0.7 (dog), 0.2 (bird)].\n",
        "How the Formula Simplifies for Single-Label Classification:\n",
        "Let c be the index of the true class.\n",
        "Then P(c) = 1, and P(i) = 0 for all i ≠ c.\n",
        "\n",
        "The sum $Σ_i P(i) * log(Q(i))$ becomes:\n",
        "\n",
        "$[P(1)*log(Q(1))] + [P(2)*log(Q(2))] + ... + [P(c)*log(Q(c))] + ...\n",
        "= [0*log(Q(1))] + [0*log(Q(2))] + ... + [1*log(Q(c))] + ...\n",
        "= log(Q(c)) $\n",
        "\n",
        "So, for single-label classification, the cross-entropy loss simplifies to:\n",
        "Cross-Entropy Loss = -log(Q(c))\n",
        "\n",
        "Where Q(c) is the model's predicted probability for the correct class.\n",
        "Intuition Behind -log(Q(c)):\n",
        "Goal: We want Q(c) (the probability assigned to the correct class) to be as close to 1 as possible.\n",
        "Behavior of -log(x) for $ 0 < x ≤ 1 $:\n",
        "* If Q(c) is close to 1 (e.g., 0.99, model is confident and correct), -log(Q(c)) is close to 0 (low loss).\n",
        "* If Q(c) is close to 0 (e.g., 0.01, model is confident but wrong, or very unsure), -log(Q(c)) is a large positive number (high loss).\n",
        "\n",
        "This loss function heavily penalizes the model if it assigns a low probability to the correct answer. It drives the model to increase the probability it assigns to the true class.\n",
        "\n",
        "Example:\n",
        "Suppose 3 classes: Cat, Dog, Bird.\n",
        "* True label: Dog. So P = [0 (Cat), 1 (Dog), 0 (Bird)].\n",
        "\n",
        "Model 1 (Good prediction): Q1 = [0.1 (Cat), 0.8 (Dog), 0.1 (Bird)]\n",
        "* $ Loss = -log(Q1(Dog)) = -log(0.8) ≈ 0.22 $ (low loss)\n",
        "\n",
        "Model 2 (Bad prediction):  Q2 = [0.7 (Cat), 0.1 (Dog), 0.2 (Bird)]\n",
        "* $ Loss = -log(Q2(Dog)) = -log(0.1) ≈ 2.30 $ (high loss)\n",
        "\n",
        "Model 3 (Very confident but wrong): Q3 = [0.9 (Cat), 0.05 (Dog), 0.05 (Bird)]\n",
        "* $Loss = -log(Q3(Dog)) = -log(0.05) ≈ 3.00 $ (even higher loss)\n",
        "\n",
        "**Connection to Maximum Likelihood Estimation (MLE):**\n",
        "Minimizing the cross-entropy loss (specifically -log(Q(c))) is equivalent to maximizing the log-likelihood of the observed true labels given the model's predictions. We want to find model parameters that make the observed data (the true labels) most probable.\n",
        "\n",
        "In summary:\n",
        "\n",
        "Cross-entropy is a measure of the difference between two probability distributions. In machine learning, it's used as a loss function to train classification models by penalizing them when their predicted probability for the true class is low, thereby pushing the model to make predictions that are closer to the true distribution. For the common single-label classification case, it beautifully simplifies to the negative log-probability of the correct class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQnbS7bH3tRS"
      },
      "source": [
        "## KL Divergence and Cross entropy?\n",
        "\n",
        "why do we use KL divergence instead of cross-entropy in GRPO?\n",
        "\n",
        "### 1. Cross-Entropy's Usual Role: Fixed Targets\n",
        "\n",
        "*   In typical classification or SFT for language models, **cross-entropy loss** (often simplified to $-\\log P(\\text{correct_token})$ or $-\\log P(\\text{correct_class})$) is used when you have a **ground truth target**.\n",
        "*   For SFT, the \"ground truth\" is the next token in a human-written demonstration. You're trying to make your model $\\pi_\\theta$ predict that specific token with high probability.\n",
        "    *   Example SFT Loss: $\\text{Loss} = -\\log \\pi_\\theta(y_{\\text{target}} | x)$ (simplified for a whole sequence).\n",
        "*   Here, $\\pi_\\theta$ is being directly compared to a fixed, known \"correct\" output. The goal is for the model's predicted distribution $Q$ to match the true distribution $P$ (which is often a one-hot vector representing the target).\n",
        "\n",
        "### 2. The Goal of DPO/GRPO: Relative Preference & Regularization\n",
        "\n",
        "*  DPO/GRPO don't operate with a single \"correct\" answer. They work with **preference pairs**: $y_c$ (chosen) is preferred over $y_r$ (rejected) for a given prompt $x$.\n",
        "*   The objective isn't just to maximize the probability of $y_c$. It's to:\n",
        "    1.   <font color='red'> Maximize the probability of $y_c$ *relative to* $y_r$.</font>\n",
        "    2.  Do so in a way that doesn't drastically change the model from its initial, well-behaved state (the reference model $\\pi_{\\text{ref}}$, usually the SFT model).\n",
        "*   <font color='red'> This \"not drastically changing\" part is crucial</font>. It's where the idea of **KL Divergence** becomes central. In the original Reinforcement Learning from Human Feedback (RLHF) formulation, the objective is to maximize reward *while constraining the KL divergence between the policy $\\pi_\\theta$ and the reference policy $\\pi_{\\text{ref}}$*:\n",
        "    *   RLHF Objective (conceptual): $\\text{Maximize } E[\\text{Reward}(y)]$ subject to $D_{KL}(\\pi_\\theta || \\pi_{\\text{ref}}) \\le \\beta_{KL}$\n",
        "    *   <font color='red'>This KL constraint prevents \"reward hacking\" (where the model finds trivial solutions to get high reward while destroying its general language abilities) by keeping it close to $\\pi_{\\text{ref}}$</font>.\n",
        "\n",
        "### 3. DPO's Key Insight: Implicit Reward and the KL Connection\n",
        "\n",
        "*   DPO provides a way to achieve the goals of KL-regularized RLHF *without* explicitly training a reward model or performing reinforcement learning.\n",
        "*   It shows that the optimal policy $\\pi^*$ for the KL-constrained reward maximization problem above can be written as: $ \\pi^*(y|x) = \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta_{DPO}} r(x,y)\\right)$\n",
        "where $r(x,y)$ is the (unknown) true reward function, $\\beta_{DPO}$ is a scaling factor (like temperature), and $Z(x)$ is a partition function (normalizer).\n",
        "*   Rearranging this, we can express the reward function in terms of the policies:\n",
        "$r(x,y) \\propto \\log\\left(\\frac{\\pi^*(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right) $\n",
        "Ignoring $\\beta_{DPO}$ and $Z(x)$ for proportionality as $Z(x)$ is constant for a given $x$ when comparing two responses).\n",
        "*   Effectively, the \"advantage\" or implicit reward of a response $y$ by policy $\\pi_\\theta$ over $\\pi_{\\text{ref}}$ is proportional to:\n",
        "$\\log \\pi_\\theta(y|x) - \\log \\pi_{\\text{ref}}(y|x)$ (which is $\\log\\left(\\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right)$)\n",
        "*   This **log-probability ratio** is a core component that also appears in the definition of KL divergence:\n",
        "$$ D_{KL}(\\pi_\\theta || \\pi_{\\text{ref}}) = \\sum_y \\pi_\\theta(y|x) \\log\\left(\\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right) $$\n",
        "\n",
        "### 4. How the DPO/GRPO Loss Uses This KL-Related Term\n",
        "\n",
        "The DPO/GRPO loss is (for a single chosen $y_c$ and rejected $y_r$ for simplicity, GRPO averages this over multiple $y_r$):\n",
        "$$ \\text{Loss} = -\\log\\left(\\sigma\\left( \\beta_{DPO} \\left[ (\\log \\pi_\\theta(y_c|x) - \\log \\pi_{\\text{ref}}(y_c|x)) - (\\log \\pi_\\theta(y_r|x) - \\log \\pi_{\\text{ref}}(y_r|x)) \\right] \\right)\\right) $$\n",
        "(where $\\sigma$ denotes the sigmoid function)\n",
        "\n",
        "*   The terms $(\\log \\pi_\\theta(y|x) - \\log \\pi_{\\text{ref}}(y|x))$ are precisely these implicit reward estimates, derived from the logic connecting to KL-regularized RL.\n",
        "*   <font color='red'>  The loss function aims to maximize the difference between the implicit reward of $y_c$ and $y_r$. </font>\n",
        "*   By optimizing these log-probability ratios, DPO/GRPO implicitly finds a policy $\\pi_\\theta$ that is optimal under the Bradley-Terry model (which models preferences based on reward differences) *and* inherently respects the KL divergence constraint from the original RLHF problem.\n",
        "\n",
        "### Why KL Divergence is \"Under the Hood\" and Not Just Cross-Entropy:\n",
        "\n",
        "1.  **No Absolute Target:**  <font color='red'> We aren't saying $y_c$ is the 100% correct answer to be matched via cross-entropy. We're saying it's *better than* $y_r$. This is a relative judgment. </font>\n",
        "2.  **Reference Model $\\pi_{\\text{ref}}$ is Crucial:**\n",
        "    *   The $\\pi_{\\text{ref}}$ is not just an arbitrary starting point; it's the **anchor**. The DPO/GRPO loss structure encourages $\\pi_\\theta$ to learn the preferences while staying \"close\" (in a KL-divergence sense) to $\\pi_{\\text{ref}}$.\n",
        "    *   If $\\pi_{\\text{ref}}$ already gives $y_c$ a high probability and $y_r$ a low one, $\\pi_\\theta$ doesn't need to change much for that pair.\n",
        "    *   If $\\pi_{\\text{ref}}$ gives $y_r$ a high probability (i.e., the SFT model prefers the bad response), $\\pi_\\theta$ needs to work harder (increase its log-probability ratio for $y_c$ and decrease it for $y_r$) to satisfy the preference.\n",
        "3.  **Implicit KL Regularization:**\n",
        "    *   The DPO/GRPO loss formulation implicitly optimizes for a reward model that explains the human preferences while ensuring the resulting policy $\\pi_\\theta$ doesn't stray too far (in terms of KL divergence) from the reference SFT policy $\\pi_{\\text{ref}}$.\n",
        "    *  <font color='red'>This regularization is key to stable and effective alignment. If you just used a cross-entropy-like loss to maximize $P(y_c)$ and minimize $P(y_r)$ without the $\\pi_{\\text{ref}}$ normalization, the model could easily overfit to the preference data and suffer from \"catastrophic forgetting\" of its general abilities </font>.\n",
        "\n",
        "### In Essence:\n",
        "\n",
        "*   **Cross-entropy** is suitable when you want your model's distribution $Q$ to directly match a fixed target distribution $P$ (e.g., one-hot labels in SFT or classification).\n",
        "*   **KL divergence** (or objectives derived from it, as in DPO/GRPO) is suitable when you want to optimize a model $\\pi_\\theta$ relative to a reference $\\pi_{\\text{ref}}$ based on preferences or rewards, implicitly controlling how far $\\pi_\\theta$ moves from $\\pi_{\\text{ref}}$.\n",
        "\n",
        "The GRPO/DPO loss structure cleverly bakes in this KL-regularized reward optimization directly from preference data, without needing to explicitly train a reward model or compute KL divergence during the loss calculation. The $\\log(\\pi_\\theta(y|x)/\\pi_{\\text{ref}}(y|x))$ terms are the signature of this underlying KL-divergence-aware objective.\n",
        "\n",
        "\n",
        "### Magistral RL changes\n",
        "\n",
        "The [Magistral paper](https://arxiv.org/pdf/2506.10910) outlines few modifications for GRPO:\n",
        "\n",
        "* KL divergence is eliminated: The KL divergence penalty constrains the online policy from deviating too much from a reference policy, helping to maintain alignment with the initial model. However, in GRPO, the policy diverges substantially regardless, and maintaining a copy of the reference model\n",
        "for KL computation incurs a compute cost we find unjustified. We remove the KL penalty entirely.\n",
        "\n",
        "* Loss normalization. To avoid introducing length biases between generations in one group, we normalize the loss by first adding token-wise loss for all tokens and all generations and then dividing by the total length of generations in the group $\\sum_{i=1}^G |o_i|$. Why they do it? In our batch, some generated responses are long and some are short. If we just average the loss per sequence, the longer sequences will have a much larger impact on the final gradient because their loss is summed over more tokens. How they do it? They change the final loss calculation from a mean-per-sequence to a mean-per-token. This is done by taking a length-weighted average of the per-sequence losses. Each token across the entire batch contributes equally to the loss, regardless of which sequence it came from.\n",
        "\n",
        "\n",
        "\n",
        "    1. The Problem: \"Length Biases\"\n",
        "\n",
        "    When training an AI with Reinforcement Learning (RL), the model generates an answer (a \"generation\") and receives a \"reward\" or \"loss\" (a penalty).\n",
        "\n",
        "    Imagine the model is asked a complex math problem. It makes two attempts (two \"generations\" in a \"group\"):\n",
        "\n",
        "    *   **Generation 1 (Short & Wrong):** \"The answer is 5.\"\n",
        "        *   *Length:* 4 tokens.\n",
        "        *   *Loss (Penalty):* Let's say it gets a high penalty of **-10**.\n",
        "\n",
        "    *   **Generation 2 (Long & Wrong):** \"Let me think... if I take the first number and multiply it by the second, and then add the third, I believe the final result, after careful consideration, is probably 5.\"\n",
        "        *   *Length:* 30 tokens.\n",
        "        *   *Loss (Penalty):* Because this answer is also wrong, it should also get a high penalty. But if the penalty is calculated for each token, the total penalty might be huge, say **-75**.\n",
        "\n",
        "    **The Bias:** Without normalization, the training algorithm sees a penalty of -75 for the long answer and only -10 for the short one. It might incorrectly conclude: **\"Generating long answers is extremely bad! I should always be brief, even if I'm wrong.\"**\n",
        "\n",
        "    This is a \"length bias.\" The model is being unfairly punished for its verbosity, not its incorrectness. This is especially bad for \"reasoning models\" which are *supposed* to generate long, step-by-step chains of thought.\n",
        "\n",
        "    2. The Solution: \"Loss Normalization\"\n",
        "\n",
        "    The paper's method fixes this by calculating the **average loss per token**.\n",
        "\n",
        "    Here's how it works, using the same example:\n",
        "\n",
        "      **\"first adding token-wise loss for all tokens and all generations\"**\n",
        "        *   They take the total penalty from all attempts.\n",
        "        *   **Total Loss** = (Loss from Gen 1) + (Loss from Gen 2) = (-10) + (-75) = **-85**.\n",
        "\n",
        "      **\"and then dividing by the total length of generations in the group\"**\n",
        "        *   They count the total number of tokens (words) across all attempts.\n",
        "        *   **Total Length** = (Length of Gen 1) + (Length of Gen 2) = 4 + 30 = **34 tokens**.\n",
        "\n",
        "      **Calculate the Normalized Loss:**\n",
        "        *   **Normalized Loss** = Total Loss / Total Length = -85 / 34 = **-2.5 per token**.\n",
        "\n",
        "\n",
        "* Advantage normalization. We estimate the advantage of each token simply as\n",
        "$\\hat{A}_{i, t} = \\hat{A}_i = r_i - \\mu$, where $\\mu$ is the mean of rewards within a group.\n",
        "Following [andrychowicz2020](https://arxiv.org/abs/2006.05990), we additionally normalize the advantages in each minibatch as $\\hat{A}_{i, t}^{\\text{norm}} = (\\hat{A}_i - \\hat{A}^{\\text{mean}}) / \\hat{A}^{\\text{std}}$ where $\\hat{A}^{\\text{mean}}$ and $\\hat{A}^{\\text{std}}$ are the sequence-wise mean and standard deviation of the advantages $\\hat{A}_i$ in a minibatch.\n",
        "\n",
        "\n",
        "### Appendix Loss function\n",
        "How did we get to this loss function?\n",
        "\n",
        "$$ \\text{Loss} = -\\log\\left(\\sigma\\left( \\beta_{DPO} \\left[ (\\log \\pi_\\theta(y_c|x) - \\log \\pi_{\\text{ref}}(y_c|x)) - (\\log \\pi_\\theta(y_r|x) - \\log \\pi_{\\text{ref}}(y_r|x)) \\right] \\right)\\right) $$\n",
        "\n",
        "\n",
        "The derivation combines a few key ideas:\n",
        "1.  **Modeling Preferences with a Latent Reward Model:** We assume there's some underlying (latent) reward function $r(x,y)$ that humans use to judge responses.\n",
        "2.  **Bradley-Terry Model:** This model is commonly used for pairwise comparisons. It states that the probability of item $A$ being preferred over item $B$ is a logistic function of the difference in their underlying \"scores\" or \"strengths.\"\n",
        "3.  **DPO's Connection to RLHF:** DPO showed that the optimal policy $\\pi^*$ for a KL-regularized reward maximization objective (like in RLHF) can be expressed in terms of a reference policy $\\pi_{\\text{ref}}$ and this latent reward $r(x,y)$.\n",
        "4.  **Maximum Likelihood Estimation:** We want to find model parameters that maximize the likelihood of observing the human preference data.\n",
        "\n",
        "Here's the step-by-step derivation:\n",
        "\n",
        "**Step 1: The Bradley-Terry Model for Preferences**\n",
        "\n",
        "Let $r(x,y)$ be the (unknown) true reward score for a response $y$ given prompt $x$. The Bradley-Terry model posits that the probability of a human preferring $y_c$ (chosen) over $y_r$ (rejected) is given by a sigmoid function of the difference in their rewards: $\\sigma(\\text{score_difference})$\n",
        "\n",
        "$$ P(y_c \\succ y_r | x) = \\sigma(\\beta_{DPO} [r(x, y_c) - r(x, y_r)]) $$\n",
        "\n",
        "Where:\n",
        "*   $y_c \\succ y_r$ means $y_c$ is preferred over $y_r$.\n",
        "*   $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the logistic sigmoid function.\n",
        "*   $\\beta_{DPO}$ is a hyperparameter (like an inverse temperature) that scales the reward difference. A higher $\\beta_{DPO}$ means the preferences are more deterministic based on the reward difference.\n",
        "\n",
        " **Converting Scores/Values to Probabilities:**\n",
        "    *   In DPO/GRPO, the term inside the sigmoid function:\n",
        "        $\\beta_{DPO} \\left[ (\\log \\pi_\\theta(y_c|x) - \\log \\pi_{\\text{ref}}(y_c|x)) - (\\log \\pi_\\theta(y_r|x) - \\log \\pi_{\\text{ref}}(y_r|x)) \\right]$\n",
        "        represents a \"score difference\" or \"advantage\" of the chosen response $y_c$ over the rejected response $y_r$, according to the current policy $\\pi_\\theta$ relative to the reference $\\pi_{\\text{ref}}$, scaled by $\\beta_{DPO}$.\n",
        "    *   This score difference can be any real number (large positive, large negative, or near zero).\n",
        "    *   The logistic function (sigmoid) takes this score difference and converts it into a probability: the probability that $y_c$ is preferred over $y_r$.\n",
        "        *   If the score difference is very large and positive (meaning $y_c$ is much \"better\" than $y_r$), the sigmoid output will be close to 1.\n",
        "        *   If the score difference is very large and negative (meaning $y_c$ is much \"worse\" than $y_r$), the sigmoid output will be close to 0.\n",
        "        *   If the score difference is close to 0 (meaning $y_c$ and $y_r$ are similarly \"good\"), the sigmoid output will be close to 0.5.\n",
        "\n",
        "\n",
        "**Step 2: DPO's Insight – Expressing Reward in Terms of Policies**\n",
        "\n",
        "The DPO paper shows (building on the optimal solution to KL-regularized RL) that the implicit reward can be related to the policy $\\pi_\\theta$ and the reference policy $\\pi_{\\text{ref}}$ as follows. The optimal policy $\\pi^*$ is:\n",
        "\n",
        "$$ \\pi^*(y|x) = \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y|x) \\exp(\\frac{1}{\\beta_{DPO}} r(x,y)) $$\n",
        "\n",
        "Solving for $r(x,y)$:\n",
        "\n",
        "$$ \\frac{1}{\\beta_{DPO}} r(x,y) = \\log\\left(\\frac{\\pi^*(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right) - \\log Z(x) $$\n",
        "\n",
        "So, $r(x,y) = \\beta_{DPO} \\left( \\log\\left(\\frac{\\pi^*(y|x)}{\\pi_{\\text{ref}}(y|x)}\\right) - \\log Z(x) \\right)$.\n",
        "\n",
        "When we consider the *difference* in rewards $r(x, y_c) - r(x, y_r)$, the $\\log Z(x)$ term (which is a normalizer constant for a given $x$) cancels out:\n",
        "\n",
        "$$ r(x, y_c) - r(x, y_r) = \\beta_{DPO} \\left[ \\log\\left(\\frac{\\pi^*(y_c|x)}{\\pi_{\\text{ref}}(y_c|x)}\\right) - \\log\\left(\\frac{\\pi^*(y_r|x)}{\\pi_{\\text{ref}}(y_r|x)}\\right) \\right] $$\n",
        "\n",
        "Let's define a shorthand for the log-policy ratio term, which acts as our model's estimate of the scaled reward (or \"advantage\" over reference):\n",
        "Let $\\hat{r}_\\theta(x,y) = \\log \\pi_\\theta(y|x) - \\log \\pi_{\\text{ref}}(y|x)$.\n",
        "This term represents how much more (or less) likely the current policy $\\pi_\\theta$ makes the response $y$ compared to the baseline $\\pi_{\\text{ref}}$, in log-space.\n",
        "\n",
        "So, the difference in *our model's estimate* of the (unscaled) rewards is:\n",
        "$$ \\hat{r}_\\theta(x, y_c) - \\hat{r}_\\theta(x, y_r) = \\left( \\log \\pi_\\theta(y_c|x) - \\log \\pi_{\\text{ref}}(y_c|x) \\right) - \\left( \\log \\pi_\\theta(y_r|x) - \\log \\pi_{\\text{ref}}(y_r|x) \\right) $$\n",
        "\n",
        "**Step 3: Plugging the Model's Reward Estimate into the Bradley-Terry Model**\n",
        "\n",
        "We now substitute our model's estimate of the reward difference into the Bradley-Terry preference probability:\n",
        "\n",
        "$$ P(y_c \\succ y_r | x; \\theta) = \\sigma\\left( \\beta_{DPO} \\left[ (\\log \\pi_\\theta(y_c|x) - \\log \\pi_{\\text{ref}}(y_c|x)) - (\\log \\pi_\\theta(y_r|x) - \\log \\pi_{\\text{ref}}(y_r|x)) \\right] \\right) $$\n",
        "\n",
        "This equation now gives the probability of observing the preference $(y_c, y_r)$ given our current policy $\\pi_\\theta$, the reference policy $\\pi_{\\text{ref}}$, and the hyperparameter $\\beta_{DPO}$.\n",
        "\n",
        "**Step 4: Maximum Likelihood Estimation and the Loss Function**\n",
        "\n",
        "To train our policy $\\pi_\\theta$, we want to maximize the likelihood of the observed human preferences. For a dataset $D = \\{(x^{(i)}, y_c^{(i)}, y_r^{(i)})\\}$ of preference pairs, we want to maximize:\n",
        "\n",
        "$$ L(\\theta) = \\prod_{i} P(y_c^{(i)} \\succ y_r^{(i)} | x^{(i)}; \\theta) $$\n",
        "\n",
        "Maximizing the likelihood is equivalent to maximizing the log-likelihood (because $\\log$ is a monotonic function):\n",
        "\n",
        "$$ \\log L(\\theta) = \\sum_{i} \\log P(y_c^{(i)} \\succ y_r^{(i)} | x^{(i)}; \\theta) $$\n",
        "\n",
        "In machine learning, we typically *minimize* a loss function. So, we minimize the negative log-likelihood (NLL):\n",
        "\n",
        "$$ \\text{Loss}(\\theta) = -\\log L(\\theta) = -\\sum_{i} \\log P(y_c^{(i)} \\succ y_r^{(i)} | x^{(i)}; \\theta) $$\n",
        "\n",
        "For a single preference pair $(x, y_c, y_r)$, the loss contribution is:\n",
        "\n",
        "$$ \\text{Loss}_{\\text{single_pair}} = -\\log P(y_c \\succ y_r | x; \\theta) $$\n",
        "\n",
        "Substituting the expression for $P(y_c \\succ y_r | x; \\theta)$ from Step 3:\n",
        "\n",
        "$$ \\text{Loss}_{\\text{single_pair}} = -\\log\\left(\\sigma\\left( \\beta_{DPO} \\left[ (\\log \\pi_\\theta(y_c|x) - \\log \\pi_{\\text{ref}}(y_c|x)) - (\\log \\pi_\\theta(y_r|x) - \\log \\pi_{\\text{ref}}(y_r|x)) \\right] \\right)\\right) $$\n",
        "\n",
        "This is precisely the DPO loss function.\n",
        "\n",
        "**Summary of the Components:**\n",
        "\n",
        "*   **$-\\log(\\cdot)$:** The negative log-likelihood part. Minimizing this maximizes the probability of the observed preferences.\n",
        "*   **$\\sigma(\\cdot)$:** The sigmoid function, which maps the difference in \"scores\" to a probability between 0 and 1, as per the Bradley-Terry model.\n",
        "*   **$\\beta_{DPO}$:** A hyperparameter (temperature) that scales the difference. It controls how strongly the model should adhere to the preferences.\n",
        "*   **$(\\log \\pi_\\theta(y_c|x) - \\log \\pi_{\\text{ref}}(y_c|x))$**: The \"score\" or \"advantage\" of the chosen response under the current policy $\\pi_\\theta$ relative to the reference policy $\\pi_{\\text{ref}}$. We want to make this term larger.\n",
        "*   **$(\\log \\pi_\\theta(y_r|x) - \\log \\pi_{\\text{ref}}(y_r|x))$**: The \"score\" or \"advantage\" of the rejected response. We want to make this term smaller.\n",
        "*   **The subtraction between these two \"score\" terms**: This is the core difference that the sigmoid acts upon. We want this difference to be positive and large, indicating that the chosen response is significantly \"better\" than the rejected one according to our policy (relative to the reference).\n",
        "\n",
        "For GRPO, you would compute this loss for each $(y_c, y_{r_j})$ pair within a group (one chosen, multiple rejected) and then average these losses.\n",
        "\n",
        "### Appendix: Latent\n",
        "what does the term latent means in the `latent reward`?\n",
        "\n",
        "In the term **\"latent reward,\"**\n",
        "\n",
        "*   **\"Reward\"** refers to a numerical score that quantifies how good or desirable a particular model output (response $y$) is for a given input (prompt $x$). A higher reward means a better response.\n",
        "*   **\"Latent\"** means **hidden, unobserved, or underlying**. It's not directly provided or measured.\n",
        "\n",
        "So, a **\"latent reward\"** is a **hypothetical, underlying numerical score** that we assume humans use (consciously or unconsciously) when they express preferences, but which we don't get to see directly.\n",
        "\n",
        "**Think of it this way:**\n",
        "\n",
        "When a human evaluator says, \"For prompt $x$, response $y_c$ is better than response $y_r$,\" they aren't typically thinking:\n",
        "\"Ah, $y_c$ has a reward score of 75.3 and $y_r$ has a reward score of 62.1, therefore $y_c$ is better.\"\n",
        "\n",
        "Instead, they make a qualitative judgment. However, the DPO/RLHF framework *assumes* that this qualitative judgment is driven by some underlying (latent) reward values. The preference $y_c \\succ y_r$ arises because, according to this latent system, $r(x, y_c) > r(x, y_r)$.\n",
        "\n",
        "**Why is it \"latent\"?**\n",
        "\n",
        "1.  **Not Explicitly Provided:** Humans provide *pairwise preferences* (this is better than that), not explicit numerical reward scores for each individual response. If they did provide scores, it wouldn't be \"latent.\"\n",
        "2.  **Inferred, Not Measured:** The goal of alignment algorithms like DPO (or training a separate reward model in traditional RLHF) is to *infer* or *learn a model of* this latent reward system based on the observed preference data. The algorithm tries to find a function that, if it *were* the true latent reward function, would best explain the preferences humans have shown.\n",
        "3.  **A Theoretical Construct:** It's a useful mathematical and conceptual tool. By assuming the existence of this latent reward, we can use mathematical models (like the Bradley-Terry model) to connect observable preferences to the parameters of our language model.\n",
        "\n",
        "**Analogy:**\n",
        "\n",
        "Imagine you're trying to figure out someone's favorite ice cream flavors.\n",
        "*   **Observable Data (Preferences):** You ask them, \"Do you prefer chocolate or vanilla?\" They say \"Chocolate.\" \"Do you prefer strawberry or chocolate?\" They say \"Chocolate.\" \"Vanilla or Mint?\" They say \"Vanilla.\"\n",
        "*   **Latent \"Flavor Score\":** You assume they have some internal, unstated \"flavor score\" for each ice cream. Chocolate has a higher latent score than vanilla for them. Vanilla has a higher latent score than mint.\n",
        "*   **Your Goal:** You try to build a model of their preferences that implicitly assigns these latent scores based on their choices. You might conclude: LatentScore(Chocolate) > LatentScore(Vanilla) > LatentScore(Mint). You don't know the exact numbers, but you know their relative order and can build a system that predicts their future choices.\n",
        "\n",
        "**In DPO/GRPO:**\n",
        "\n",
        "*   The language model $\\pi_\\theta$ itself, through the term $\\log \\pi_\\theta(y|x) - \\log \\pi_{\\text{ref}}(y|x)$, becomes an *implicit representation* of this (scaled) latent reward.\n",
        "*   The algorithm isn't trying to output an explicit reward number for every response. Instead, it's adjusting the probabilities $\\pi_\\theta(y|x)$ such that the *differences* in these implicit rewards (between chosen and rejected responses) align with the human preference data, as modeled by the Bradley-Terry setup.\n",
        "\n",
        "So, \"latent reward\" is a key concept that bridges the gap between the qualitative, pairwise preference data we collect from humans and the quantitative optimization process used to align language models. It's the \"hidden variable\" we're trying to uncover or model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ioeHn6F3Eox"
      },
      "source": [
        "## Putting everything together: GRPO in the DeepSeek paper\n",
        "\n",
        "$$J_{GRPO}(\\theta) = \\mathbb{E}_{q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{old}}(O|q)} \\left[ \\frac{1}{G} \\sum_{i=1}^{G} \\frac{1}{|o_i|} \\sum_{t=1}^{|o_i|} \\left( \\min \\left( \\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})} \\hat{A}_{i,t}, \\text{clip} \\left( \\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})}, 1-\\epsilon, 1+\\epsilon \\right) \\hat{A}_{i,t} \\right) \\right) - \\beta D_{KL}(\\pi_\\theta || \\pi_{ref}) \\right]$$\n",
        "\n",
        "Here's the dissection:\n",
        "\n",
        "1.  **$J_{GRPO}(\\theta)$**:\n",
        "    *   The objective function to be maximized.\n",
        "    *   $\\theta$ are the parameters of the policy $\\pi_\\theta$ being optimized.\n",
        "\n",
        "2.  **$\\mathbb{E}_{q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{old}}(O|q)} [\\dots]$**:\n",
        "    *   $\\mathbb{E}$: Expectation over samples.\n",
        "    *   $q \\sim P(Q)$: A query/prompt $q$ is sampled.\n",
        "    *   The remaining part suggests that for a single query $q$, a **group or batch of $G$ different output sequences $o_i$ are sampled using the old policy $\\pi_{\\theta_{old}}$**. This is a key difference from the standard PPO notation which typically shows one $o$ per $q$ in the outer expectation. This \"G\" might hint at the \"Grouped\" aspect of \"GRPO,\" but the original GRPO from DeepSeek is a DPO variant and doesn't directly use this PPO-style objective.\n",
        "\n",
        "3.  **$\\frac{1}{G} \\sum_{i=1}^{G} \\dots$**:\n",
        "    *   This is an **average over the $G$ sampled output sequences** for the given query $q$.\n",
        "\n",
        "4.  **$\\frac{1}{|o_i|} \\sum_{t=1}^{|o_i|} \\dots$**:\n",
        "    *   For each of the $G$ sequences $o_i$, this is an average over all timesteps (tokens) $t$ within that sequence.\n",
        "\n",
        "5.  **$\\min \\left( \\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})} \\hat{A}_{i,t}, \\text{clip} \\left( \\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})}, 1-\\epsilon, 1+\\epsilon \\right) \\hat{A}_{i,t} \\right)$**:\n",
        "    *   This is the **standard PPO-Clip term**, applied at the token level for each sequence $o_i$ in the group.\n",
        "    *   $\\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})}$: The probability ratio $r_{i,t}(\\theta)$ for token $o_{i,t}$ of sequence $o_i$.\n",
        "    *   A is the advantage estimate for token $o_{i,t}$ of sequence $o_i$. This advantage would likely be derived from a reward model that scores the entire sequence $o_i$.\n",
        "    *   $\\epsilon$: The clipping hyperparameter.\n",
        "\n",
        "6.  **$- \\beta D_{KL}(\\pi_\\theta || \\pi_{ref})$**:\n",
        "    *   This is an **explicit KL divergence penalty term**.\n",
        "    *   $D_{KL}(\\pi_\\theta || \\pi_{ref})$: The Kullback-Leibler divergence between the current policy $\\pi_\\theta$ and some **reference policy $\\pi_{ref}$**.\n",
        "        *   **Crucially, this $\\pi_{ref}$ is distinct from $\\pi_{\\theta_{old}}$**.\n",
        "        *   $\\pi_{\\theta_{old}}$ is the policy used for data collection in the current iteration.\n",
        "        *   $\\pi_{ref}$ is typically a fixed, trusted policy, often the original SFT model. This term aims to prevent $\\pi_\\theta$ from straying too far from this trusted reference, maintaining its general capabilities and style.\n",
        "    *   $\\beta$: A hyperparameter that controls the strength of this KL penalty. A larger $\\beta$ means a stronger penalty for deviating from $\\pi_{ref}$.\n",
        "    *   The negative sign means we are *subtracting* this penalty, so we are trying to *minimize* the KL divergence (since the overall objective $J$ is maximized).\n",
        "\n",
        "**Interpretation and Potential Context:**\n",
        "\n",
        "This objective function describes a PPO-like algorithm that:\n",
        "1.  **Uses PPO's clipped surrogate objective** to encourage actions leading to higher advantage, while limiting the magnitude of policy updates.\n",
        "2.  **Processes a group of $G$ responses per prompt** in its expectation calculation (this is unusual for the standard PPO formulation, which usually averages over state-action pairs without an explicit \"group\" like this for a single prompt, unless $G=1$). If $G > 1$, this could be a way to get a more stable estimate of the expected clipped advantage for a given prompt by sampling multiple continuations.\n",
        "3.  **Includes an explicit KL divergence penalty** against a fixed reference policy $\\pi_{ref}$. This is a common addition to PPO in RLAIF/RLHF to provide stronger regularization and prevent the policy from drifting too far from the base SFT model. The PPO clipping itself provides some regularization against $\\pi_{\\theta_{old}}$, but the explicit KL penalty against a more global $\\pi_{ref}$ is often beneficial.\n",
        "\n",
        "**How it differs from the actual GRPO (DeepSeek's DPO variant):**\n",
        "\n",
        "*   **Core Mechanism:** The GRPO you asked about earlier is a **Direct Preference Optimization (DPO)** variant. DPO directly optimizes a policy based on preference pairs (chosen vs. rejected) and a loss function derived from the Bradley-Terry model and the relationship between optimal policies and reward functions. It *doesn't* use advantage estimates, explicit PPO-style clipping, or explicit KL divergence terms in its loss function (though the KL divergence is implicitly controlled).\n",
        "*   **Data Source:** DPO/GRPO learn from fixed datasets of (prompt, chosen\\_response, rejected\\_response(s)). The formula you've given now implies data collection using $\\pi_{\\theta_{old}}$ and calculation of advantage estimates $\\hat{A}_{i,t}$, which is characteristic of RL algorithms like PPO that typically use a reward model.\n",
        "*   **\"Grouped\" aspect:**\n",
        "    *   In DeepSeek's GRPO, \"Grouped\" refers to using one chosen response and *multiple* rejected responses for the *same prompt* in the preference data, and the loss is averaged over these chosen-rejected pairs.\n",
        "    *   In the formula you just provided, the \"Grouped\" aspect ($G$ sequences) seems to be about sampling multiple outputs from $\\pi_{\\theta_{old}}$ for a given prompt during the PPO update phase.\n",
        "\n",
        "**Conclusion for the new formula:**\n",
        "\n",
        "The formula you've provided represents a sophisticated PPO-based objective that incorporates:\n",
        "*   Standard PPO clipping for stable updates relative to the data-collection policy ($\\pi_{\\theta_{old}}$).\n",
        "*   An additional, explicit KL divergence penalty to regularize the policy against a fixed reference model ($\\pi_{ref}$).\n",
        "*   A potential modification to sample and average over a group of $G$ responses generated by $\\pi_{\\theta_{old}}$ for each prompt, perhaps for variance reduction or a more robust gradient estimate.\n",
        "\n",
        "This is a plausible objective for RLAIF/RLHF, but it's important to distinguish it from the DPO-based GRPO algorithm developed by DeepSeek. If this formula *is* being called \"GRPO\" by someone, it would be a different algorithm using the same acronym.\n",
        "\n",
        "### Appendix on Clipping\n",
        "\n",
        "\n",
        "1.  **`value_to_clip`**:\n",
        "    $\\frac{\\pi_\\theta(o_t|q, o_{<t})}{\\pi_{\\theta_{old}}(o_t|q, o_{<t})}$\n",
        "\n",
        "    *   This is the **probability ratio**, often denoted as $r_t(\\theta)$.\n",
        "    *   $\\pi_\\theta(o_t|q, o_{<t})$: The probability of taking action $o_t$ (generating token $o_t$) in the current state (given query $q$ and previous tokens $o_{<t}$) according to the **current policy $\\pi_\\theta$** (the one being updated).\n",
        "    *   $\\pi_{\\theta_{old}}(o_t|q, o_{<t})$: The probability of taking the *same* action $o_t$ in the *same* state according to the **old policy $\\pi_{\\theta_{old}}$** (the policy that was used to generate the experience/trajectory).\n",
        "    *   **Interpretation of the ratio:**\n",
        "        *   If $r_t(\\theta) > 1$: The current policy makes this action more likely than the old policy did.\n",
        "        *   If $r_t(\\theta) < 1$: The current policy makes this action less likely than the old policy did.\n",
        "        *   If $r_t(\\theta) = 1$: The current policy assigns the same probability to this action as the old policy did.\n",
        "\n",
        "2.  **`lower_bound`**:\n",
        "    $1-\\epsilon$\n",
        "\n",
        "    *   $\\epsilon$ (epsilon) is a small, positive hyperparameter (e.g., 0.1, 0.2).\n",
        "    *   So, $1-\\epsilon$ is a value slightly less than 1 (e.g., $1 - 0.2 = 0.8$).\n",
        "    *   This sets the minimum allowed value for the probability ratio $r_t(\\theta)$ in this clipped term.\n",
        "\n",
        "3.  **`upper_bound`**:\n",
        "    $1+\\epsilon$\n",
        "\n",
        "    *   This is a value slightly greater than 1 (e.g., $1 + 0.2 = 1.2$).\n",
        "    *   This sets the maximum allowed value for the probability ratio $r_t(\\theta)$ in this clipped term.\n",
        "\n",
        "**How the `clip` function works here:**\n",
        "\n",
        "The `clip` function takes the calculated probability ratio $r_t(\\theta)$ and constrains it:\n",
        "\n",
        "*   If $r_t(\\theta) < (1-\\epsilon)$: The output of `clip(...)` becomes $1-\\epsilon$.\n",
        "*   If $r_t(\\theta) > (1+\\epsilon)$: The output of `clip(...)` becomes $1+\\epsilon$.\n",
        "*   If $(1-\\epsilon) \\le r_t(\\theta) \\le (1+\\epsilon)$: The output of `clip(...)` is just $r_t(\\theta)$ itself (it's already within the bounds).\n",
        "\n",
        "**What happens after the clipping:**\n",
        "\n",
        "The result of this `clip` operation (the clipped probability ratio) is then multiplied by the advantage estimate $A_t$:\n",
        "\n",
        "Clipped Term = $\\text{clipped_ratio} \\times A_t$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "247isFxDixH8"
      },
      "source": [
        "## How to run this\n",
        "\n",
        "1) In a colab if you have enough compute\n",
        "\n",
        "2) Connect to a local runtime\n",
        "\n",
        "```sh\n",
        "jupyter-server --ServerApp.allow_origin='https://colab.sandbox.google.com' --port=8888 --ServerApp.port_retries=0\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K80rPtu0pA_5",
        "outputId": "0462001b-c08f-4931-ee8d-cd7901870c65"
      },
      "outputs": [],
      "source": [
        "%pip install treescope"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EIV6QgcARiX"
      },
      "source": [
        "## Loss Implementation details\n",
        "\n",
        "![Image](https://huggingface.co/datasets/trl-lib/documentation-images/resolve/main/grpo_visual.png)\n",
        "\n",
        "Start with a base model, the reference policy model $\\pi_{ref}$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUftd9uKi6xP",
        "outputId": "185deb91-6839-4c0b-ae44-724f37ae2e97"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# import treescope\n",
        "# treescope.register_as_default()\n",
        "# treescope.basic_interactive_setup(autovisualize_arrays=True)\n",
        "\n",
        "\n",
        "def set_device():\n",
        "  if torch.backends.mps.is_available():\n",
        "    device_str = \"mps\"\n",
        "  elif torch.cuda.is_available():\n",
        "    device_str = \"cuda\"\n",
        "  else:\n",
        "    device_str = \"cpu\"\n",
        "\n",
        "  device = torch.device(device_str)\n",
        "  print(\"Using Device:\", device)\n",
        "  return device\n",
        "\n",
        "device = set_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80oXOYZC2dd0"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Initialize model and tokenizer\n",
        "model_str = 'babylm/babyllama-100m-2024'\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_str)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_str)\n",
        "\n",
        "# pad on the left so we can append new tokens on the right\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.truncation_side = \"left\"\n",
        "# print(base_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O--xlIzC1QJP",
        "outputId": "3f00fcd9-d4eb-4f47-962e-714ad0ba0e67"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "prompt = \"The quick brown fox jumped over the \"\n",
        "# Define ANSI escape codes for colors for better readability\n",
        "BLUE = '\\033[94m'\n",
        "RESET_COLOR = '\\033[0m'\n",
        "\n",
        "# Tokenize the prompt\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\")\n",
        "print(input_ids)\n",
        "\n",
        "# Generate next 2 tokens\n",
        "with torch.no_grad():\n",
        "    outputs = base_model.generate(\n",
        "        **input_ids,\n",
        "        max_new_tokens=2,\n",
        "        pad_token_id=tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(\n",
        "    outputs[0], skip_special_tokens=True\n",
        ")\n",
        "generated_portion = generated_text[len(prompt):]\n",
        "print(f\"Generated text: {prompt}{BLUE}{generated_portion}{RESET_COLOR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udVz5LdH1sJm"
      },
      "source": [
        "### Create reference and policy models\n",
        "\n",
        "The **reference model** is the base LLM, and remains unchanges throughout training.\n",
        "\n",
        "The **policy** model is the same model with a LoRA adapter applied - the weights of the LoRA adapter get updated throughout the RFT training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzpZGMs11fHY"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Create a copy of the base model to use as the reference model\n",
        "ref_model = copy.deepcopy(base_model)\n",
        "\n",
        "# Initialize LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    init_lora_weights=False,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA to model\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "# print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoZJByJ82s3X"
      },
      "source": [
        "### Calculating the policy loss ratio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6d91B4g2srS"
      },
      "outputs": [],
      "source": [
        "def prepare_inputs(prompt, completion):\n",
        "    # Tokenization\n",
        "    prompt_tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    completion_tokens = tokenizer(completion, return_tensors=\"pt\")\n",
        "\n",
        "    # Combined input\n",
        "    input_ids = torch.cat(\n",
        "        [\n",
        "            prompt_tokens[\"input_ids\"],\n",
        "            completion_tokens[\"input_ids\"]\n",
        "        ],\n",
        "        dim=1\n",
        "    )\n",
        "    attention_mask = torch.cat(\n",
        "        [\n",
        "            prompt_tokens[\"attention_mask\"],\n",
        "            completion_tokens[\"attention_mask\"]\n",
        "        ],\n",
        "        dim=1\n",
        "    )\n",
        "\n",
        "    prompt_length = prompt_tokens[\"input_ids\"].shape[1]\n",
        "    completion_length = completion_tokens[\"input_ids\"].shape[1]\n",
        "    total_length = prompt_length + completion_length\n",
        "\n",
        "    # Create a mask to identify the tokens that\n",
        "    # were generated by the model in the full sequence.\n",
        "    # we need this later to compute the loss only for the output tokens\n",
        "    # generated by the model.\n",
        "    completion_mask = torch.zeros(total_length, dtype=torch.float32)\n",
        "    completion_mask[prompt_length:] = 1.0\n",
        "\n",
        "    return input_ids, attention_mask, completion_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_xVoF7J9X91"
      },
      "source": [
        "The following helper function allows:\n",
        "\n",
        "1.  Asks the model: \"For each position in this input sequence, what are your predicted scores (logits) for every word in your dictionary?\"\n",
        "2.  Converts these scores into log-probabilities for every word in the dictionary at each position.\n",
        "3.  Then, for each position, it looks up the *actual word* that was in the input sequence and picks out the log-probability the model assigned to *that specific word* at *that specific position*.\n",
        "\n",
        "**Why is this useful?**\n",
        "\n",
        "*   **Calculating Loss:** This is a fundamental step in calculating the negative log-likelihood loss (or cross-entropy loss) for sequence models during training. The sum of these log-probabilities (often with a negative sign) forms the loss for the sequence.\n",
        "*   **Evaluating Perplexity:** Perplexity, a common metric for evaluating language models, is derived from these log-probabilities.\n",
        "*   **Policy Evaluation in RL:** In algorithms like PPO or DPO/GRPO, you need to calculate $\\log \\pi(a|s)$ (log-probability of an action given a state). For language models, an \"action\" is generating a token, and this function directly computes those log-probabilities for a given sequence (which could be generated by the policy or be part of a reference).\n",
        "*   **Rescoring or Analysis:** You might want to know how likely a model finds a given piece of text.\n",
        "\n",
        "This function is a very common utility when working with the outputs of language models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeYbwfYr2jhr"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_log_probs(model, input_ids, attention_mask):\n",
        "    # outputs.logits: This attribute of the outputs object typically holds the\n",
        "    # raw, unnormalized prediction scores (logits) for each token in the\n",
        "    # vocabulary, at each position in the input sequence.\n",
        "    # Shape of outputs.logits is (batch_size, sequence_length, vocab_size).\n",
        "    # For each token position in the sequence, it gives a score for every\n",
        "    # possible token in the model's vocabulary.\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    # size [1, 11, 16000] (batch, seq_len, vocab_size)\n",
        "    # print('outputs.logits.shape', outputs.logits.shape)\n",
        "\n",
        "    # Computing the log-probability of each token in the sequence\n",
        "    # outputs.logits is the logits for all tokens in the vocabulary for each\n",
        "    # position in the sequence\n",
        "    # softmax: Converts the raw logits into probabilities (values between 0 and\n",
        "    # 1 that sum to 1 across the vocabulary).\n",
        "    # dim=-1: This specifies that the log_softmax operation should be applied\n",
        "    # along the last dimension, which is the vocab_size dimension.\n",
        "    # This means for each position in the sequence, it calculates the\n",
        "    # log-probabilities across all possible vocabulary tokens.\n",
        "    log_probs = F.log_softmax(outputs.logits, dim=-1)\n",
        "    # size [1, 11, 16000] (batch, seq_len, vocab_size)\n",
        "    # print('log_probs.shape', log_probs.shape)\n",
        "\n",
        "    # Extract the log-probability for the actual token that\n",
        "    # was generated at each position in the sequence.\n",
        "    # (batch_size, sequence_length, vocab_size).\n",
        "    # gather select the values from a tensor along the specified dimension\n",
        "    # (dim=-1, vocab dim) using indices provided by another tensor (index)\n",
        "    log_probs_inputs = log_probs.gather(\n",
        "        dim=-1,\n",
        "        index=input_ids.unsqueeze(-1)\n",
        "    ).squeeze(-1)\n",
        "    # size [1, 11] (batch, seq_len)\n",
        "    # print('log_probs_inputs.shape', log_probs_inputs.shape)\n",
        "    return log_probs_inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XzUddJ4SMQy"
      },
      "source": [
        "**1. The `dim` argument in `log_softmax`**\n",
        "\n",
        "The `log_softmax` function (and `softmax`) needs to know **across which dimension to normalize the scores into probabilities (or log-probabilities).** Probabilities must sum to 1 over a set of mutually exclusive events.\n",
        "\n",
        "*   `outputs.logits` has a shape like `(batch_size, sequence_length, vocab_size)`.\n",
        "*   For a language model, at each `(batch_index, sequence_position)`, we have a distribution over the `vocab_size` possible tokens. This means the probabilities (and thus log-probabilities) should be normalized across the `vocab_size` dimension.\n",
        "\n",
        "The **softmax function** is applied to this vector of logits $z$ to convert it into a probability distribution. For each logit $z_j$ in the vector $z$:\n",
        "\n",
        "$$ \\text{softmax}(z_j) = P_j = \\frac{e^{z_j}}{\\sum_{k=1}^{V} e^{z_k}} $$\n",
        "\n",
        "\n",
        "\n",
        "**2. Why the `gather` step is necessary after `log_softmax`**\n",
        "\n",
        "Even after correctly calculating `log_probs` with `dim=-1`, so that `log_probs` has the shape `(batch_size, sequence_length, vocab_size)`, we are still not done if the goal is to get the log-probability of the *actual input tokens*.\n",
        "\n",
        "*   `log_probs[b, s, :]` gives you a vector of log-probabilities for **all** possible tokens in the vocabulary at position `(b, s)`.\n",
        "*   However, we are usually interested in the log-probability that the model assigned to the **specific token that actually occurred** at that position in our `input_ids`.\n",
        "\n",
        "**Example:**\n",
        "Let `vocab_size = 50000`.\n",
        "Let `input_ids[0, 5]` be token ID `123` (e.g., the word \"hello\").\n",
        "Then `log_probs[0, 5, :]` is a vector of 50,000 log-probabilities.\n",
        "We want to extract `log_probs[0, 5, 123]`, which is the log-probability the model assigned to \"hello\" at that specific position.\n",
        "\n",
        "This is exactly what the `gather` operation does:\n",
        "`log_probs.gather(dim=-1, index=input_ids.unsqueeze(-1))`\n",
        "\n",
        "It uses the token IDs from `input_ids` as indices to pick out the corresponding log-probabilities from the full `log_probs` tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2mTzCrn3W4B"
      },
      "outputs": [],
      "source": [
        "def grpo_loss(model, ref_model, prompt, completion, advantage):\n",
        "    input_ids, attention_mask, completion_mask = prepare_inputs(\n",
        "        prompt, completion\n",
        "    )\n",
        "\n",
        "    # Model forward\n",
        "    token_log_probs = compute_log_probs(\n",
        "        model, input_ids, attention_mask\n",
        "    )\n",
        "    # we dont want backward on the ref model since its frozen and used as a ref.\n",
        "    with torch.no_grad():\n",
        "      ref_token_log_probs = compute_log_probs(\n",
        "            ref_model, input_ids, attention_mask\n",
        "    )\n",
        "\n",
        "    # ratio = pi_model / pi_ref = exp(log(pi_model) - log(pi_ref))\n",
        "    ratio = torch.exp(token_log_probs - ref_token_log_probs)\n",
        "\n",
        "    # Scale the ratio by the advantage function\n",
        "    policy_loss = ratio * advantage\n",
        "\n",
        "    # We want to maximize reward, so we make the loss negative\n",
        "    # because optimizers minimize loss.\n",
        "    per_token_loss = -policy_loss\n",
        "\n",
        "    # Only compute loss over the output tokens\n",
        "    loss = (per_token_loss * completion_mask).sum() / completion_mask.sum()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pgqCB2t3Yys",
        "outputId": "55d7c6a7-a2f8-4593-c737-383fdda855ca"
      },
      "outputs": [],
      "source": [
        "grpo_loss(model, ref_model, prompt, \"fence and\", advantage=2.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqQo3o10ZL2z"
      },
      "source": [
        "During the first step of training, the reference and policy models are identical. So the loss comes from the advantage of the reponse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1ea13ju3aZn",
        "outputId": "fdf0f536-7957-49b6-e514-f5472d91dba4"
      },
      "outputs": [],
      "source": [
        "# At step 1, the model and reference model are the same\n",
        "# So the loss is the advantage function because the ratio of\n",
        "# per-token log-probabilities is 1\n",
        "grpo_loss(ref_model, ref_model, prompt, \"fence and\", advantage=2.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fes0QnEAZTCj",
        "outputId": "db1bbeec-268f-4e9d-a186-3038f12c167c"
      },
      "outputs": [],
      "source": [
        "completion = \"fence and\"\n",
        "\n",
        "input_ids, attention_mask, completion_mask = prepare_inputs(\n",
        "    prompt, completion\n",
        ")\n",
        "with torch.no_grad():\n",
        "    token_log_probs = compute_log_probs(\n",
        "        model, input_ids, attention_mask\n",
        "    )\n",
        "    ref_token_log_probs = compute_log_probs(\n",
        "        ref_model, input_ids, attention_mask\n",
        "    )\n",
        "\n",
        "ratio = torch.exp(token_log_probs - ref_token_log_probs)\n",
        "print(ratio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFv6GzE0ZbqJ"
      },
      "source": [
        "### Adding clipping to the policy loss\n",
        "\n",
        "Clipping has no effect during the first step of training - the loss is still the advantage:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtZz-IwkZfzB"
      },
      "outputs": [],
      "source": [
        "def grpo_loss_with_clip(model, ref_model, prompt, completion, advantage, epsilon=0.2):\n",
        "    input_ids, attention_mask, completion_mask = prepare_inputs(\n",
        "        prompt, completion\n",
        "    )\n",
        "\n",
        "    # Model forward\n",
        "    token_log_probs = compute_log_probs(\n",
        "        model, input_ids, attention_mask\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        ref_token_log_probs = compute_log_probs(\n",
        "            ref_model, input_ids, attention_mask\n",
        "    )\n",
        "\n",
        "    # ratio = pi_model / pi_ref = exp(log(pi_model) - log(pi_ref))\n",
        "    ratio = torch.exp(token_log_probs - ref_token_log_probs)\n",
        "\n",
        "    # Scale the ratio by the advantage function\n",
        "    unclipped = ratio * advantage\n",
        "    clipped = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantage\n",
        "\n",
        "    policy_loss = torch.min(unclipped, clipped)\n",
        "\n",
        "    # We want to maximize reward, so we make the loss negative\n",
        "    # because optimizers minimize loss.\n",
        "    per_token_loss = -policy_loss\n",
        "\n",
        "    # Only compute loss over the output tokens\n",
        "    loss = (per_token_loss * completion_mask).sum() / completion_mask.sum()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an2lMwh5ZnLg",
        "outputId": "4344d95a-22ce-47af-f2f2-309830a0fc42"
      },
      "outputs": [],
      "source": [
        "grpo_loss_with_clip(\n",
        "    model,\n",
        "    ref_model,\n",
        "    prompt,\n",
        "    \"fence and\",\n",
        "    advantage=2.0,\n",
        "    epsilon=0.2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrqctrqqZz0I",
        "outputId": "d61d2b53-d7c5-4746-f142-605b6d578761"
      },
      "outputs": [],
      "source": [
        "# If we pass the reference model as also the model we're training, the ratio will be 1,\n",
        "# so your loss will be the advantage.\n",
        "grpo_loss_with_clip(\n",
        "    ref_model,\n",
        "    ref_model,\n",
        "    prompt,\n",
        "    \"fence and\",\n",
        "    advantage=2.0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62Ea1_SGZ2sD"
      },
      "source": [
        "Check how many of the output tokens were clipped:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OU649xTvZ4lb",
        "outputId": "65a00856-a118-4f71-9e8e-40464c9a867c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "completion = \"fence and\"\n",
        "\n",
        "input_ids, attention_mask, _ = prepare_inputs(prompt, completion)\n",
        "with torch.no_grad():\n",
        "    token_log_probs = compute_log_probs(\n",
        "        model, input_ids, attention_mask\n",
        "    )\n",
        "    ref_token_log_probs = compute_log_probs(\n",
        "        ref_model, input_ids, attention_mask\n",
        "    )\n",
        "\n",
        "with torch.no_grad():\n",
        "    epsilon = 0.2\n",
        "    ratio = torch.exp(token_log_probs - ref_token_log_probs)\n",
        "    ratio_unclipped = ratio\n",
        "    ratio_clipped = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
        "\n",
        "print('ratio unclipped', ratio_unclipped[0][9:])\n",
        "print('ratio clipped', ratio_clipped[0][9:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy4SnQh9fpoT"
      },
      "source": [
        "### Adding KL Divergence to the loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1Iyxy6icWD4"
      },
      "outputs": [],
      "source": [
        "def grpo_loss_with_kl(model, ref_model, prompt, completion, advantage, epsilon=0.2, beta=0.1):\n",
        "    input_ids, attention_mask, completion_mask = prepare_inputs(\n",
        "        prompt, completion\n",
        "    )\n",
        "\n",
        "    # Model forward\n",
        "    token_log_probs = compute_log_probs(\n",
        "        model, input_ids, attention_mask\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        ref_token_log_probs = compute_log_probs(\n",
        "            ref_model, input_ids, attention_mask\n",
        "    )\n",
        "\n",
        "    # ratio = p_model / p_ref = exp(log(p_model) - log(p_ref))\n",
        "    ratio = torch.exp(token_log_probs - ref_token_log_probs)\n",
        "\n",
        "    # Scale the ratio by the advantage function\n",
        "    unclipped = ratio * advantage\n",
        "    clipped = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantage\n",
        "\n",
        "    policy_loss = torch.min(unclipped, clipped)\n",
        "\n",
        "    # Compute the per-token KL divergence to encourage the model\n",
        "    # to stay close to the reference model\n",
        "    delta = token_log_probs - ref_token_log_probs\n",
        "    per_token_kl = torch.exp(-delta) + delta - 1\n",
        "\n",
        "    # We want to maximize reward, so we make the loss negative\n",
        "    # because optimizers minimize loss.\n",
        "    per_token_loss = -(policy_loss - beta * per_token_kl)\n",
        "\n",
        "    # Only compute loss over the output tokens\n",
        "    loss = (per_token_loss * completion_mask).sum() / completion_mask.sum()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzsyeVJ3f9NL",
        "outputId": "bfe355ec-cb51-43d3-9da8-2638ef9d834f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Define the range of Δ (log-probability difference between\n",
        "# model and reference)\n",
        "delta = np.linspace(-6, 6, 500)\n",
        "\n",
        "# Compute the per-token reverse KL divergence: KL(π_ref || π)\n",
        "kl_divergence = np.exp(-delta) + delta - 1\n",
        "\n",
        "# Plot the KL divergence\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(delta, kl_divergence, label=r'$KL(\\pi_{\\mathrm{ref}} || \\pi) = e^{-\\Delta} + \\Delta - 1$')\n",
        "plt.axhline(0, color='gray', linestyle='--', linewidth=0.5)\n",
        "plt.axvline(0, color='gray', linestyle='--', linewidth=0.5)\n",
        "plt.fill_between(delta, kl_divergence, where=(delta > 0), color='red', alpha=0.3, label='Overconfident region (Δ > 0)')\n",
        "plt.fill_between(delta, kl_divergence, where=(delta < 0), color='green', alpha=0.3, label='Conservative region (Δ < 0)')\n",
        "plt.title(\"KL Divergence as 'Gravitational Pull' Toward Reference Policy\")\n",
        "plt.xlabel(r'$\\Delta = \\log \\pi - \\log \\pi_{\\mathrm{ref}}$')\n",
        "plt.ylabel('KL Penalty')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsLxIWX5gB4g",
        "outputId": "672c7323-174d-42de-cfcf-6e6388dbdb51"
      },
      "outputs": [],
      "source": [
        "for beta in [0, 0.1, 0.5]:\n",
        "    loss = grpo_loss_with_kl(\n",
        "        model,\n",
        "        ref_model,\n",
        "        prompt,\n",
        "        \"fence and\",\n",
        "        advantage=2.0,\n",
        "        epsilon=0.2,\n",
        "        beta=beta\n",
        "    )\n",
        "    print(f\"beta={beta}\")\n",
        "    print(f\"loss={loss.item():.3f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgZBS37XjimV"
      },
      "source": [
        "# Wordle RL (work in progress)\n",
        "In the following, we apply what we learned on a small model to have it play wordle. This is adapted from [DeepLearning course GRPO](https://www.google.com/url?q=https%3A%2F%2Flearn.deeplearning.ai%2Fcourses%2Freinforcement-fine-tuning-llms-grpo) but the training was abstracted by predibase so I thought we should write our own to learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrsgUrmajUu4"
      },
      "source": [
        "## Start with base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mlx-lm in ./venv/lib/python3.13/site-packages (0.26.0)\n",
            "Requirement already satisfied: mlx>=0.25.0 in ./venv/lib/python3.13/site-packages (from mlx-lm) (0.26.3)\n",
            "Requirement already satisfied: numpy in ./venv/lib/python3.13/site-packages (from mlx-lm) (2.3.1)\n",
            "Requirement already satisfied: transformers>=4.39.3 in ./venv/lib/python3.13/site-packages (from mlx-lm) (4.53.1)\n",
            "Requirement already satisfied: protobuf in ./venv/lib/python3.13/site-packages (from mlx-lm) (6.31.1)\n",
            "Requirement already satisfied: pyyaml in ./venv/lib/python3.13/site-packages (from mlx-lm) (6.0.2)\n",
            "Requirement already satisfied: jinja2 in ./venv/lib/python3.13/site-packages (from mlx-lm) (3.1.6)\n",
            "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (0.33.2)\n",
            "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (2024.11.6)\n",
            "Requirement already satisfied: requests in ./venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in ./venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.13/site-packages (from transformers>=4.39.3->mlx-lm) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.39.3->mlx-lm) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.39.3->mlx-lm) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.39.3->mlx-lm) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.13/site-packages (from jinja2->mlx-lm) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests->transformers>=4.39.3->mlx-lm) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.13/site-packages (from requests->transformers>=4.39.3->mlx-lm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests->transformers>=4.39.3->mlx-lm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.13/site-packages (from requests->transformers>=4.39.3->mlx-lm) (2025.7.9)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: matplotlib in ./venv/lib/python3.13/site-packages (3.10.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.13/site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.13/site-packages (from matplotlib) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.13/site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in ./venv/lib/python3.13/site-packages (from matplotlib) (2.3.1)\n",
            "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.13/site-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.13/site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: torch in ./venv/lib/python3.13/site-packages (2.7.1)\n",
            "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.13/site-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: setuptools in ./venv/lib/python3.13/site-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.13/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in ./venv/lib/python3.13/site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in ./venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in ./venv/lib/python3.13/site-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install mlx-lm\n",
        "%pip install matplotlib\n",
        "%pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check for MPS and set the device\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(f\"Using device: {device}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(f\"MPS not available, using CPU. Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "efed8287ef094ebb80f6c5e3ad414201",
            "4ce285969691429aa3d9aaca171909df",
            "0ced5eefafca4f22a7782a43218f4973",
            "d204612862634a1a82b146269bfbbfc0",
            "fde49001fb57479ba34610192d175a62",
            "161a74daffd341c1a30ee4216e802aa0",
            "9caac590a10946058e19156009fbd95a",
            "0557f1f45f0b4907af863a8a72835681",
            "44543b607c02463a9141ce91fca56f42",
            "9fa176cec32647efbde9f55db23cdb05",
            "86aacef10471405abd0141c3519a10e5",
            "07be4a333b4b4a2b9b8b4bb5d50959b8",
            "805af9e5f5954cb48d416f9c6d6071cb",
            "7165a31c15ce4cf0a3b4fcf3a5647208",
            "92b13518080d40b3bef2fc5d2407c642",
            "83b1205ce14f4b07a1cf31a0f0246920",
            "1eefccc6b30a4e369a69e97abe45ca39"
          ]
        },
        "id": "2okVKHdMqvh_",
        "outputId": "1d08c5ae-57c8-4c61-9f48-2dbe258a4005"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/charbelk/dev/wordle-rl/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "The `notebook_login` function can only be used in a notebook (Jupyter or Colab) and you need the `ipywidgets` module: `pip install ipywidgets`.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dev/wordle-rl/venv/lib/python3.13/site-packages/huggingface_hub/_login.py:340\u001b[39m, in \u001b[36mnotebook_login\u001b[39m\u001b[34m(new_session, write_permission)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mipywidgets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwidgets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwidgets\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ipywidgets'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m notebook_login\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mnotebook_login\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dev/wordle-rl/venv/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[39m, in \u001b[36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     99\u001b[39m         message += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + custom_message\n\u001b[32m    100\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dev/wordle-rl/venv/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:31\u001b[39m, in \u001b[36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     29\u001b[39m extra_args = \u001b[38;5;28mlen\u001b[39m(args) - \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extra_args <= \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[32m     33\u001b[39m args_msg = [\n\u001b[32m     34\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[-extra_args:])\n\u001b[32m     36\u001b[39m ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dev/wordle-rl/venv/lib/python3.13/site-packages/huggingface_hub/_login.py:343\u001b[39m, in \u001b[36mnotebook_login\u001b[39m\u001b[34m(new_session, write_permission)\u001b[39m\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    344\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `notebook_login` function can only be used in a notebook (Jupyter or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    345\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m Colab) and you need the `ipywidgets` module: `pip install ipywidgets`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    346\u001b[39m     )\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_session \u001b[38;5;129;01mand\u001b[39;00m get_token() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mUser is already logged in.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mImportError\u001b[39m: The `notebook_login` function can only be used in a notebook (Jupyter or Colab) and you need the `ipywidgets` module: `pip install ipywidgets`."
          ]
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### To Download from MLX\n",
        "\n",
        "* pip install llm\n",
        "* llm install llm-mlx\n",
        "* llm mlx download-model mlx-community/gemma-3-1b-it-qat-8bit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iwooxLTq7hwM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 8 files: 100%|██████████| 8/8 [00:00<00:00, 137518.16it/s]\n"
          ]
        }
      ],
      "source": [
        "from mlx_lm import load, generate, stream_generate\n",
        "import torch\n",
        "# use this only for inference\n",
        "\n",
        "# model_id = \"mlx-community/Qwen3-0.6B-4bit-DWQ-053125\"\n",
        "model_id = \"mlx-community/gemma-3-1b-it-qat-8bit\"\n",
        "\n",
        "# model_id = \"mlx-community/gemma-3-1b-it-qat-6bit\"\n",
        "# model_id = \"mlx-community/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "# model_id = \"mlx-community/gemma-3-1b-it-qat-6bit\"\n",
        "# model_id = \"mlx-community/gemma-3-1b-it-4bit\"\n",
        "# model_id = \"mlx-community/gemma-3-1b-it-bf16\"\n",
        "torch_dtype = torch.bfloat16\n",
        "# model_config = dict(attn_implementation='eager', torch_dtype=torch_dtype,device_map=\"auto\", device=device)\n",
        "gemma_model, gemma_tokenizer = load(path_or_hf_repo=model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(gemma_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mlx_lm.sample_utils import make_sampler\n",
        "make_sampler(temp=0.7)\n",
        "\n",
        "def llm_generate_gemma(model, tokenizer, messages, max_tokens=512):\n",
        "    \"\"\"Generate text using LLM\"\"\"\n",
        "    try:\n",
        "        tokenizer.eos_token_id = tokenizer.get_vocab()[\"<end_of_turn>\"]\n",
        "    except KeyError:\n",
        "        # Fallback for other models or tokenizer configs\n",
        "         print(\"Warning: '<end_of_turn>' token not found. Using default eos_token.\")\n",
        "         pass\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages, \n",
        "        add_generation_prompt=True,\n",
        "        # Keep tokenize=True (the default) to get token IDs directly.\n",
        "        # This is more efficient than creating a string and re-tokenizing.\n",
        "        tokenize=True \n",
        "    )\n",
        "\n",
        "    # Pass the eos_token_id to the generate function\n",
        "    return generate(\n",
        "        model, \n",
        "        tokenizer, \n",
        "        prompt=prompt, \n",
        "        # verbose=False, \n",
        "        max_tokens=max_tokens,\n",
        "        sampler=make_sampler(temp=0.7),\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUWclVs4fd29"
      },
      "source": [
        "take it for a test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you are running locally, you might want to use a smaller model\n",
        "\n",
        "### Prepare your prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dWOTZk_d7zrT"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a world-class Wordle-solving AI. Your goal is to guess the secret 5-letter word.\n",
        "\n",
        "### Instructions\n",
        "1.  **Analyze the feedback:** After each guess, you get clues.\n",
        "    *   `✓` = Correct letter in the correct spot.\n",
        "    *   `-` = Correct letter in the wrong spot.\n",
        "    *   `x` = Letter is not in the word.\n",
        "\n",
        "2.  **State your reasoning:** Inside `<think>` tags, briefly explain your logic.\n",
        "    *   What clues do you have so far? (Correct letters, misplaced letters, wrong letters).\n",
        "    *   What is your plan for the next guess?\n",
        "    *   Why is your chosen word a good strategic move?\n",
        "\n",
        "3.  **Make your guess:** After your reasoning, provide your 5-letter guess inside `<guess>` tags.\n",
        "\n",
        "**IMPORTANT:** You MUST always follow the `<think>` and `<guess>` format exactly like the examples below.\n",
        "\n",
        "---\n",
        "### EXAMPLES\n",
        "\n",
        "**Example 1: First Guess**\n",
        "\n",
        "<think>\n",
        "This is the first guess. There are no clues yet. A good starting word uses common and unique letters to get the most information. 'RAISE' is a great choice as it tests three common vowels (A, I, E) and two common consonants (R, S).\n",
        "</think>\n",
        "<guess>RAISE</guess>\n",
        "\n",
        "**Example 2: A Mid-Game Guess**\n",
        "\n",
        "**Clues so far:**\n",
        "*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\n",
        "*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\n",
        "\n",
        "<think>\n",
        "Based on the clues, I know the word starts with 'BR' (B✓, R✓). I also know the letter 'S' is in the word, but not in the first position. The letters T, O, M, A, V, E are wrong and must be avoided. My strategy is to find a word that fits the pattern `BR__S` or `BR_S_`. The word 'BRISK' fits all these rules perfectly. It starts with 'BR', contains 'S', and uses no incorrect letters.\n",
        "</think>\n",
        "<guess>BRISK</guess>\n",
        "\n",
        "---\n",
        "End of examples. The new puzzle starts now.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "We9d7Hf274Ha"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<think>\n",
            "Okay, let's start with a guess that incorporates the letters 'BR' and 'S'. I’ll focus on a word with a relatively common vowel and consonant combination. Given the feedback, I’m going to try to find a word that starts with 'BR' and has a 'S' in it.  I'll need to avoid common letters like 'E' and 'A'.\n",
            "\n",
            "<guess>BRIS</guess>\n"
          ]
        }
      ],
      "source": [
        "gemma_messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": SYSTEM_PROMPT\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"This is your first guess. What is the 5-letter word?\"\n",
        "        }\n",
        "    ]\n",
        "text = \"\"\n",
        "logits = []\n",
        "generated_tokens = []\n",
        "for response in llm_generate_gemma(gemma_model, gemma_tokenizer, gemma_messages):\n",
        "    text += response\n",
        "    # logits.append(response.logprobs)\n",
        "    # generated_tokens.append(response.token)\n",
        "print(text)\n",
        "# print(\"Logits shape:\", logits)\n",
        "# print(\"Generated tokens:\", generated_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "S3XzLNLAgKtq"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class LetterFeedback(Enum):\n",
        "    CORRECT = \"✓\"\n",
        "    WRONG_POS = \"-\"\n",
        "    WRONG_LETTER = \"x\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GuessWithFeedback:\n",
        "    guess: str\n",
        "    feedback: List[LetterFeedback]\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        \"\"\"Returns a readable string showing the guess alongside\n",
        "        its letter-by-letter feedback.\"\"\"\n",
        "        feedback_str = \" \".join(f\"{letter}({fb.value})\" for letter, fb in zip(self.guess, self.feedback))\n",
        "        return f\"{self.guess} → Feedback: {feedback_str}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fyAxDBrShVUt"
      },
      "outputs": [],
      "source": [
        "def render_user_prompt(past_guesses: List[GuessWithFeedback]) -> str:\n",
        "    \"\"\"Creates a user-facing prompt that includes past guesses\n",
        "    and their feedback.\"\"\"\n",
        "    prompt = \"Make a new 5-letter word guess.\"\n",
        "    if past_guesses:\n",
        "        prompt += \"\\n\\nHere is some previous feedback:\"\n",
        "        for i, guess in enumerate(past_guesses):\n",
        "            prompt += f\"\\nGuess {i+1}: {guess}\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "soZCEvYQhlDJ"
      },
      "outputs": [],
      "source": [
        "def render_prompt(past_guesses: List[GuessWithFeedback]):\n",
        "    \"\"\"Formats a full chat prompt using a system message, user\n",
        "    prompt, and assistant preamble to start the model's\n",
        "    step-by-step reasoning.\"\"\"\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": SYSTEM_PROMPT\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": render_user_prompt(past_guesses)\n",
        "        }\n",
        "    ]\n",
        "    return messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "JQ-YY1hRhtVZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gemma response :  <think>\n",
            "Okay, the feedback suggests a word that starts with 'C' and has a few letters missing. I'm going to try a word with a common vowel and a consonant, focusing on the 'C' starting point. Given the previous feedback, I'll try to avoid words with 'S' or 'H'. I'll focus on a word with 'R' and 'A'.\n",
            "</think>\n",
            "<guess>CARROT</guess>\n"
          ]
        }
      ],
      "source": [
        "secret_word = \"CRAFT\"\n",
        "\n",
        "past_guesses = [\n",
        "    GuessWithFeedback(\n",
        "        \"CRANE\", [\n",
        "            LetterFeedback.CORRECT,\n",
        "            LetterFeedback.CORRECT,\n",
        "            LetterFeedback.CORRECT,\n",
        "            LetterFeedback.WRONG_LETTER,\n",
        "            LetterFeedback.WRONG_LETTER,\n",
        "        ]),\n",
        "    GuessWithFeedback(\n",
        "        \"CRASH\", [\n",
        "            LetterFeedback.CORRECT,\n",
        "            LetterFeedback.CORRECT,\n",
        "            LetterFeedback.CORRECT,\n",
        "            LetterFeedback.WRONG_LETTER,\n",
        "            LetterFeedback.WRONG_LETTER,\n",
        "        ]),\n",
        "]\n",
        "\n",
        "messages = render_prompt(past_guesses)\n",
        "response = llm_generate_gemma(gemma_model, gemma_tokenizer, messages)\n",
        "print(\"Gemma response : \", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "UgUW7plxiIFK"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def get_feedback(guess: str, secret_word: str) -> List[LetterFeedback]:\n",
        "    valid_letters = set(secret_word)\n",
        "    feedback = []\n",
        "    for letter, secret_letter in zip(guess, secret_word):\n",
        "        if letter == secret_letter:\n",
        "            feedback.append(LetterFeedback.CORRECT)\n",
        "        elif letter in valid_letters:\n",
        "            feedback.append(LetterFeedback.WRONG_POS)\n",
        "        else:\n",
        "            feedback.append(LetterFeedback.WRONG_LETTER)\n",
        "    return feedback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "J5O_-KidiPRh"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def next_turn_gemma(\n",
        "        gemma_model,\n",
        "        gemma_tokenizer,\n",
        "    past_guesses: List[GuessWithFeedback],\n",
        "    secret_word: str,\n",
        "    number_guesses: int\n",
        "):\n",
        "\n",
        "    messages = render_prompt(past_guesses)\n",
        "    print(\"Messages: \", messages)\n",
        "    response = llm_generate_gemma(gemma_model, gemma_tokenizer, messages)\n",
        "    match = re.search(\n",
        "        r\"<guess>\\s*(.*?)\\s*</guess>\", response, re.DOTALL\n",
        "    )\n",
        "    if not match:\n",
        "        print(\"invalid guess\")\n",
        "        return\n",
        "    print(\"match: \", match)\n",
        "    guess = match.group(1).upper()\n",
        "    print('The LLM guessed: ', guess)\n",
        "    feedback = get_feedback(guess, secret_word)\n",
        "    past_guesses.append(GuessWithFeedback(guess, feedback))\n",
        "    print(\"\\n\\n\")\n",
        "    print((\"-\" * 100) + \"\\n\")\n",
        "    for past_guess in past_guesses:\n",
        "        print(past_guess)\n",
        "\n",
        "    if guess == secret_word:\n",
        "        print(\"🎉 SUCCESS 🎉\")\n",
        "    elif len(past_guesses) >= number_guesses:\n",
        "        print(\"❌ better luck next time... ❌\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "BMDgn7pNiY18"
      },
      "outputs": [],
      "source": [
        "secret_word = \"AUDIO\"\n",
        "past_guesses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "sUgoBiGWicRb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a world-class Wordle-solving AI. Your goal is to guess the secret 5-letter word.\\n\\n### Instructions\\n1.  **Analyze the feedback:** After each guess, you get clues.\\n    *   `✓` = Correct letter in the correct spot.\\n    *   `-` = Correct letter in the wrong spot.\\n    *   `x` = Letter is not in the word.\\n\\n2.  **State your reasoning:** Inside `<think>` tags, briefly explain your logic.\\n    *   What clues do you have so far? (Correct letters, misplaced letters, wrong letters).\\n    *   What is your plan for the next guess?\\n    *   Why is your chosen word a good strategic move?\\n\\n3.  **Make your guess:** After your reasoning, provide your 5-letter guess inside `<guess>` tags.\\n\\n**IMPORTANT:** You MUST always follow the `<think>` and `<guess>` format exactly like the examples below.\\n\\n---\\n### EXAMPLES\\n\\n**Example 1: First Guess**\\n\\n<think>\\nThis is the first guess. There are no clues yet. A good starting word uses common and unique letters to get the most information. 'RAISE' is a great choice as it tests three common vowels (A, I, E) and two common consonants (R, S).\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: A Mid-Game Guess**\\n\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n\\n<think>\\nBased on the clues, I know the word starts with 'BR' (B✓, R✓). I also know the letter 'S' is in the word, but not in the first position. The letters T, O, M, A, V, E are wrong and must be avoided. My strategy is to find a word that fits the pattern `BR__S` or `BR_S_`. The word 'BRISK' fits all these rules perfectly. It starts with 'BR', contains 'S', and uses no incorrect letters.\\n</think>\\n<guess>BRISK</guess>\\n\\n---\\nEnd of examples. The new puzzle starts now.\\n\"}, {'role': 'user', 'content': 'Make a new 5-letter word guess.'}]\n",
            "match:  <re.Match object; span=(396, 416), match='<guess>SLAVE</guess>'>\n",
            "The LLM guessed:  SLAVE\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "SLAVE → Feedback: S(x) L(x) A(-) V(x) E(x)\n",
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a world-class Wordle-solving AI. Your goal is to guess the secret 5-letter word.\\n\\n### Instructions\\n1.  **Analyze the feedback:** After each guess, you get clues.\\n    *   `✓` = Correct letter in the correct spot.\\n    *   `-` = Correct letter in the wrong spot.\\n    *   `x` = Letter is not in the word.\\n\\n2.  **State your reasoning:** Inside `<think>` tags, briefly explain your logic.\\n    *   What clues do you have so far? (Correct letters, misplaced letters, wrong letters).\\n    *   What is your plan for the next guess?\\n    *   Why is your chosen word a good strategic move?\\n\\n3.  **Make your guess:** After your reasoning, provide your 5-letter guess inside `<guess>` tags.\\n\\n**IMPORTANT:** You MUST always follow the `<think>` and `<guess>` format exactly like the examples below.\\n\\n---\\n### EXAMPLES\\n\\n**Example 1: First Guess**\\n\\n<think>\\nThis is the first guess. There are no clues yet. A good starting word uses common and unique letters to get the most information. 'RAISE' is a great choice as it tests three common vowels (A, I, E) and two common consonants (R, S).\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: A Mid-Game Guess**\\n\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n\\n<think>\\nBased on the clues, I know the word starts with 'BR' (B✓, R✓). I also know the letter 'S' is in the word, but not in the first position. The letters T, O, M, A, V, E are wrong and must be avoided. My strategy is to find a word that fits the pattern `BR__S` or `BR_S_`. The word 'BRISK' fits all these rules perfectly. It starts with 'BR', contains 'S', and uses no incorrect letters.\\n</think>\\n<guess>BRISK</guess>\\n\\n---\\nEnd of examples. The new puzzle starts now.\\n\"}, {'role': 'user', 'content': 'Make a new 5-letter word guess.\\n\\nHere is some previous feedback:\\nGuess 1: SLAVE → Feedback: S(x) L(x) A(-) V(x) E(x)'}]\n",
            "match:  <re.Match object; span=(390, 409), match='<guess>SEAS</guess>'>\n",
            "The LLM guessed:  SEAS\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "SLAVE → Feedback: S(x) L(x) A(-) V(x) E(x)\n",
            "SEAS → Feedback: S(x) E(x) A(-) S(x)\n",
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a world-class Wordle-solving AI. Your goal is to guess the secret 5-letter word.\\n\\n### Instructions\\n1.  **Analyze the feedback:** After each guess, you get clues.\\n    *   `✓` = Correct letter in the correct spot.\\n    *   `-` = Correct letter in the wrong spot.\\n    *   `x` = Letter is not in the word.\\n\\n2.  **State your reasoning:** Inside `<think>` tags, briefly explain your logic.\\n    *   What clues do you have so far? (Correct letters, misplaced letters, wrong letters).\\n    *   What is your plan for the next guess?\\n    *   Why is your chosen word a good strategic move?\\n\\n3.  **Make your guess:** After your reasoning, provide your 5-letter guess inside `<guess>` tags.\\n\\n**IMPORTANT:** You MUST always follow the `<think>` and `<guess>` format exactly like the examples below.\\n\\n---\\n### EXAMPLES\\n\\n**Example 1: First Guess**\\n\\n<think>\\nThis is the first guess. There are no clues yet. A good starting word uses common and unique letters to get the most information. 'RAISE' is a great choice as it tests three common vowels (A, I, E) and two common consonants (R, S).\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: A Mid-Game Guess**\\n\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n\\n<think>\\nBased on the clues, I know the word starts with 'BR' (B✓, R✓). I also know the letter 'S' is in the word, but not in the first position. The letters T, O, M, A, V, E are wrong and must be avoided. My strategy is to find a word that fits the pattern `BR__S` or `BR_S_`. The word 'BRISK' fits all these rules perfectly. It starts with 'BR', contains 'S', and uses no incorrect letters.\\n</think>\\n<guess>BRISK</guess>\\n\\n---\\nEnd of examples. The new puzzle starts now.\\n\"}, {'role': 'user', 'content': 'Make a new 5-letter word guess.\\n\\nHere is some previous feedback:\\nGuess 1: SLAVE → Feedback: S(x) L(x) A(-) V(x) E(x)\\nGuess 2: SEAS → Feedback: S(x) E(x) A(-) S(x)'}]\n",
            "match:  <re.Match object; span=(308, 328), match='<guess>SLEEP</guess>'>\n",
            "The LLM guessed:  SLEEP\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "SLAVE → Feedback: S(x) L(x) A(-) V(x) E(x)\n",
            "SEAS → Feedback: S(x) E(x) A(-) S(x)\n",
            "SLEEP → Feedback: S(x) L(x) E(x) E(x) P(x)\n",
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a world-class Wordle-solving AI. Your goal is to guess the secret 5-letter word.\\n\\n### Instructions\\n1.  **Analyze the feedback:** After each guess, you get clues.\\n    *   `✓` = Correct letter in the correct spot.\\n    *   `-` = Correct letter in the wrong spot.\\n    *   `x` = Letter is not in the word.\\n\\n2.  **State your reasoning:** Inside `<think>` tags, briefly explain your logic.\\n    *   What clues do you have so far? (Correct letters, misplaced letters, wrong letters).\\n    *   What is your plan for the next guess?\\n    *   Why is your chosen word a good strategic move?\\n\\n3.  **Make your guess:** After your reasoning, provide your 5-letter guess inside `<guess>` tags.\\n\\n**IMPORTANT:** You MUST always follow the `<think>` and `<guess>` format exactly like the examples below.\\n\\n---\\n### EXAMPLES\\n\\n**Example 1: First Guess**\\n\\n<think>\\nThis is the first guess. There are no clues yet. A good starting word uses common and unique letters to get the most information. 'RAISE' is a great choice as it tests three common vowels (A, I, E) and two common consonants (R, S).\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: A Mid-Game Guess**\\n\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n\\n<think>\\nBased on the clues, I know the word starts with 'BR' (B✓, R✓). I also know the letter 'S' is in the word, but not in the first position. The letters T, O, M, A, V, E are wrong and must be avoided. My strategy is to find a word that fits the pattern `BR__S` or `BR_S_`. The word 'BRISK' fits all these rules perfectly. It starts with 'BR', contains 'S', and uses no incorrect letters.\\n</think>\\n<guess>BRISK</guess>\\n\\n---\\nEnd of examples. The new puzzle starts now.\\n\"}, {'role': 'user', 'content': 'Make a new 5-letter word guess.\\n\\nHere is some previous feedback:\\nGuess 1: SLAVE → Feedback: S(x) L(x) A(-) V(x) E(x)\\nGuess 2: SEAS → Feedback: S(x) E(x) A(-) S(x)\\nGuess 3: SLEEP → Feedback: S(x) L(x) E(x) E(x) P(x)'}]\n",
            "match:  <re.Match object; span=(413, 432), match='<guess>SIRS</guess>'>\n",
            "The LLM guessed:  SIRS\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "SLAVE → Feedback: S(x) L(x) A(-) V(x) E(x)\n",
            "SEAS → Feedback: S(x) E(x) A(-) S(x)\n",
            "SLEEP → Feedback: S(x) L(x) E(x) E(x) P(x)\n",
            "SIRS → Feedback: S(x) I(-) R(x) S(x)\n",
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a world-class Wordle-solving AI. Your goal is to guess the secret 5-letter word.\\n\\n### Instructions\\n1.  **Analyze the feedback:** After each guess, you get clues.\\n    *   `✓` = Correct letter in the correct spot.\\n    *   `-` = Correct letter in the wrong spot.\\n    *   `x` = Letter is not in the word.\\n\\n2.  **State your reasoning:** Inside `<think>` tags, briefly explain your logic.\\n    *   What clues do you have so far? (Correct letters, misplaced letters, wrong letters).\\n    *   What is your plan for the next guess?\\n    *   Why is your chosen word a good strategic move?\\n\\n3.  **Make your guess:** After your reasoning, provide your 5-letter guess inside `<guess>` tags.\\n\\n**IMPORTANT:** You MUST always follow the `<think>` and `<guess>` format exactly like the examples below.\\n\\n---\\n### EXAMPLES\\n\\n**Example 1: First Guess**\\n\\n<think>\\nThis is the first guess. There are no clues yet. A good starting word uses common and unique letters to get the most information. 'RAISE' is a great choice as it tests three common vowels (A, I, E) and two common consonants (R, S).\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: A Mid-Game Guess**\\n\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n\\n<think>\\nBased on the clues, I know the word starts with 'BR' (B✓, R✓). I also know the letter 'S' is in the word, but not in the first position. The letters T, O, M, A, V, E are wrong and must be avoided. My strategy is to find a word that fits the pattern `BR__S` or `BR_S_`. The word 'BRISK' fits all these rules perfectly. It starts with 'BR', contains 'S', and uses no incorrect letters.\\n</think>\\n<guess>BRISK</guess>\\n\\n---\\nEnd of examples. The new puzzle starts now.\\n\"}, {'role': 'user', 'content': 'Make a new 5-letter word guess.\\n\\nHere is some previous feedback:\\nGuess 1: SLAVE → Feedback: S(x) L(x) A(-) V(x) E(x)\\nGuess 2: SEAS → Feedback: S(x) E(x) A(-) S(x)\\nGuess 3: SLEEP → Feedback: S(x) L(x) E(x) E(x) P(x)\\nGuess 4: SIRS → Feedback: S(x) I(-) R(x) S(x)'}]\n",
            "match:  <re.Match object; span=(445, 464), match='<guess>SIRS</guess>'>\n",
            "The LLM guessed:  SIRS\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "SLAVE → Feedback: S(x) L(x) A(-) V(x) E(x)\n",
            "SEAS → Feedback: S(x) E(x) A(-) S(x)\n",
            "SLEEP → Feedback: S(x) L(x) E(x) E(x) P(x)\n",
            "SIRS → Feedback: S(x) I(-) R(x) S(x)\n",
            "SIRS → Feedback: S(x) I(-) R(x) S(x)\n",
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a world-class Wordle-solving AI. Your goal is to guess the secret 5-letter word.\\n\\n### Instructions\\n1.  **Analyze the feedback:** After each guess, you get clues.\\n    *   `✓` = Correct letter in the correct spot.\\n    *   `-` = Correct letter in the wrong spot.\\n    *   `x` = Letter is not in the word.\\n\\n2.  **State your reasoning:** Inside `<think>` tags, briefly explain your logic.\\n    *   What clues do you have so far? (Correct letters, misplaced letters, wrong letters).\\n    *   What is your plan for the next guess?\\n    *   Why is your chosen word a good strategic move?\\n\\n3.  **Make your guess:** After your reasoning, provide your 5-letter guess inside `<guess>` tags.\\n\\n**IMPORTANT:** You MUST always follow the `<think>` and `<guess>` format exactly like the examples below.\\n\\n---\\n### EXAMPLES\\n\\n**Example 1: First Guess**\\n\\n<think>\\nThis is the first guess. There are no clues yet. A good starting word uses common and unique letters to get the most information. 'RAISE' is a great choice as it tests three common vowels (A, I, E) and two common consonants (R, S).\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: A Mid-Game Guess**\\n\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n\\n<think>\\nBased on the clues, I know the word starts with 'BR' (B✓, R✓). I also know the letter 'S' is in the word, but not in the first position. The letters T, O, M, A, V, E are wrong and must be avoided. My strategy is to find a word that fits the pattern `BR__S` or `BR_S_`. The word 'BRISK' fits all these rules perfectly. It starts with 'BR', contains 'S', and uses no incorrect letters.\\n</think>\\n<guess>BRISK</guess>\\n\\n---\\nEnd of examples. The new puzzle starts now.\\n\"}, {'role': 'user', 'content': 'Make a new 5-letter word guess.\\n\\nHere is some previous feedback:\\nGuess 1: SLAVE → Feedback: S(x) L(x) A(-) V(x) E(x)\\nGuess 2: SEAS → Feedback: S(x) E(x) A(-) S(x)\\nGuess 3: SLEEP → Feedback: S(x) L(x) E(x) E(x) P(x)\\nGuess 4: SIRS → Feedback: S(x) I(-) R(x) S(x)\\nGuess 5: SIRS → Feedback: S(x) I(-) R(x) S(x)'}]\n",
            "match:  <re.Match object; span=(306, 327), match='<guess>SECRET</guess>'>\n",
            "The LLM guessed:  SECRET\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "SLAVE → Feedback: S(x) L(x) A(-) V(x) E(x)\n",
            "SEAS → Feedback: S(x) E(x) A(-) S(x)\n",
            "SLEEP → Feedback: S(x) L(x) E(x) E(x) P(x)\n",
            "SIRS → Feedback: S(x) I(-) R(x) S(x)\n",
            "SIRS → Feedback: S(x) I(-) R(x) S(x)\n",
            "SECRET → Feedback: S(x) E(x) C(x) R(x) E(x)\n",
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a world-class Wordle-solving AI. Your goal is to guess the secret 5-letter word.\\n\\n### Instructions\\n1.  **Analyze the feedback:** After each guess, you get clues.\\n    *   `✓` = Correct letter in the correct spot.\\n    *   `-` = Correct letter in the wrong spot.\\n    *   `x` = Letter is not in the word.\\n\\n2.  **State your reasoning:** Inside `<think>` tags, briefly explain your logic.\\n    *   What clues do you have so far? (Correct letters, misplaced letters, wrong letters).\\n    *   What is your plan for the next guess?\\n    *   Why is your chosen word a good strategic move?\\n\\n3.  **Make your guess:** After your reasoning, provide your 5-letter guess inside `<guess>` tags.\\n\\n**IMPORTANT:** You MUST always follow the `<think>` and `<guess>` format exactly like the examples below.\\n\\n---\\n### EXAMPLES\\n\\n**Example 1: First Guess**\\n\\n<think>\\nThis is the first guess. There are no clues yet. A good starting word uses common and unique letters to get the most information. 'RAISE' is a great choice as it tests three common vowels (A, I, E) and two common consonants (R, S).\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: A Mid-Game Guess**\\n\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n\\n<think>\\nBased on the clues, I know the word starts with 'BR' (B✓, R✓). I also know the letter 'S' is in the word, but not in the first position. The letters T, O, M, A, V, E are wrong and must be avoided. My strategy is to find a word that fits the pattern `BR__S` or `BR_S_`. The word 'BRISK' fits all these rules perfectly. It starts with 'BR', contains 'S', and uses no incorrect letters.\\n</think>\\n<guess>BRISK</guess>\\n\\n---\\nEnd of examples. The new puzzle starts now.\\n\"}, {'role': 'user', 'content': 'Make a new 5-letter word guess.\\n\\nHere is some previous feedback:\\nGuess 1: SLAVE → Feedback: S(x) L(x) A(-) V(x) E(x)\\nGuess 2: SEAS → Feedback: S(x) E(x) A(-) S(x)\\nGuess 3: SLEEP → Feedback: S(x) L(x) E(x) E(x) P(x)\\nGuess 4: SIRS → Feedback: S(x) I(-) R(x) S(x)\\nGuess 5: SIRS → Feedback: S(x) I(-) R(x) S(x)\\nGuess 6: SECRET → Feedback: S(x) E(x) C(x) R(x) E(x)'}]\n",
            "match:  <re.Match object; span=(285, 304), match='<guess>SURE</guess>'>\n",
            "The LLM guessed:  SURE\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "SLAVE → Feedback: S(x) L(x) A(-) V(x) E(x)\n",
            "SEAS → Feedback: S(x) E(x) A(-) S(x)\n",
            "SLEEP → Feedback: S(x) L(x) E(x) E(x) P(x)\n",
            "SIRS → Feedback: S(x) I(-) R(x) S(x)\n",
            "SIRS → Feedback: S(x) I(-) R(x) S(x)\n",
            "SECRET → Feedback: S(x) E(x) C(x) R(x) E(x)\n",
            "SURE → Feedback: S(x) U(✓) R(x) E(x)\n",
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a world-class Wordle-solving AI. Your goal is to guess the secret 5-letter word.\\n\\n### Instructions\\n1.  **Analyze the feedback:** After each guess, you get clues.\\n    *   `✓` = Correct letter in the correct spot.\\n    *   `-` = Correct letter in the wrong spot.\\n    *   `x` = Letter is not in the word.\\n\\n2.  **State your reasoning:** Inside `<think>` tags, briefly explain your logic.\\n    *   What clues do you have so far? (Correct letters, misplaced letters, wrong letters).\\n    *   What is your plan for the next guess?\\n    *   Why is your chosen word a good strategic move?\\n\\n3.  **Make your guess:** After your reasoning, provide your 5-letter guess inside `<guess>` tags.\\n\\n**IMPORTANT:** You MUST always follow the `<think>` and `<guess>` format exactly like the examples below.\\n\\n---\\n### EXAMPLES\\n\\n**Example 1: First Guess**\\n\\n<think>\\nThis is the first guess. There are no clues yet. A good starting word uses common and unique letters to get the most information. 'RAISE' is a great choice as it tests three common vowels (A, I, E) and two common consonants (R, S).\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: A Mid-Game Guess**\\n\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n\\n<think>\\nBased on the clues, I know the word starts with 'BR' (B✓, R✓). I also know the letter 'S' is in the word, but not in the first position. The letters T, O, M, A, V, E are wrong and must be avoided. My strategy is to find a word that fits the pattern `BR__S` or `BR_S_`. The word 'BRISK' fits all these rules perfectly. It starts with 'BR', contains 'S', and uses no incorrect letters.\\n</think>\\n<guess>BRISK</guess>\\n\\n---\\nEnd of examples. The new puzzle starts now.\\n\"}, {'role': 'user', 'content': 'Make a new 5-letter word guess.\\n\\nHere is some previous feedback:\\nGuess 1: SLAVE → Feedback: S(x) L(x) A(-) V(x) E(x)\\nGuess 2: SEAS → Feedback: S(x) E(x) A(-) S(x)\\nGuess 3: SLEEP → Feedback: S(x) L(x) E(x) E(x) P(x)\\nGuess 4: SIRS → Feedback: S(x) I(-) R(x) S(x)\\nGuess 5: SIRS → Feedback: S(x) I(-) R(x) S(x)\\nGuess 6: SECRET → Feedback: S(x) E(x) C(x) R(x) E(x)\\nGuess 7: SURE → Feedback: S(x) U(✓) R(x) E(x)'}]\n",
            "match:  <re.Match object; span=(525, 544), match='<guess>SEES</guess>'>\n",
            "The LLM guessed:  SEES\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "SLAVE → Feedback: S(x) L(x) A(-) V(x) E(x)\n",
            "SEAS → Feedback: S(x) E(x) A(-) S(x)\n",
            "SLEEP → Feedback: S(x) L(x) E(x) E(x) P(x)\n",
            "SIRS → Feedback: S(x) I(-) R(x) S(x)\n",
            "SIRS → Feedback: S(x) I(-) R(x) S(x)\n",
            "SECRET → Feedback: S(x) E(x) C(x) R(x) E(x)\n",
            "SURE → Feedback: S(x) U(✓) R(x) E(x)\n",
            "SEES → Feedback: S(x) E(x) E(x) S(x)\n",
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a world-class Wordle-solving AI. Your goal is to guess the secret 5-letter word.\\n\\n### Instructions\\n1.  **Analyze the feedback:** After each guess, you get clues.\\n    *   `✓` = Correct letter in the correct spot.\\n    *   `-` = Correct letter in the wrong spot.\\n    *   `x` = Letter is not in the word.\\n\\n2.  **State your reasoning:** Inside `<think>` tags, briefly explain your logic.\\n    *   What clues do you have so far? (Correct letters, misplaced letters, wrong letters).\\n    *   What is your plan for the next guess?\\n    *   Why is your chosen word a good strategic move?\\n\\n3.  **Make your guess:** After your reasoning, provide your 5-letter guess inside `<guess>` tags.\\n\\n**IMPORTANT:** You MUST always follow the `<think>` and `<guess>` format exactly like the examples below.\\n\\n---\\n### EXAMPLES\\n\\n**Example 1: First Guess**\\n\\n<think>\\nThis is the first guess. There are no clues yet. A good starting word uses common and unique letters to get the most information. 'RAISE' is a great choice as it tests three common vowels (A, I, E) and two common consonants (R, S).\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: A Mid-Game Guess**\\n\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n\\n<think>\\nBased on the clues, I know the word starts with 'BR' (B✓, R✓). I also know the letter 'S' is in the word, but not in the first position. The letters T, O, M, A, V, E are wrong and must be avoided. My strategy is to find a word that fits the pattern `BR__S` or `BR_S_`. The word 'BRISK' fits all these rules perfectly. It starts with 'BR', contains 'S', and uses no incorrect letters.\\n</think>\\n<guess>BRISK</guess>\\n\\n---\\nEnd of examples. The new puzzle starts now.\\n\"}, {'role': 'user', 'content': 'Make a new 5-letter word guess.\\n\\nHere is some previous feedback:\\nGuess 1: SLAVE → Feedback: S(x) L(x) A(-) V(x) E(x)\\nGuess 2: SEAS → Feedback: S(x) E(x) A(-) S(x)\\nGuess 3: SLEEP → Feedback: S(x) L(x) E(x) E(x) P(x)\\nGuess 4: SIRS → Feedback: S(x) I(-) R(x) S(x)\\nGuess 5: SIRS → Feedback: S(x) I(-) R(x) S(x)\\nGuess 6: SECRET → Feedback: S(x) E(x) C(x) R(x) E(x)\\nGuess 7: SURE → Feedback: S(x) U(✓) R(x) E(x)\\nGuess 8: SEES → Feedback: S(x) E(x) E(x) S(x)'}]\n",
            "match:  <re.Match object; span=(414, 433), match='<guess>SISS</guess>'>\n",
            "The LLM guessed:  SISS\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "SLAVE → Feedback: S(x) L(x) A(-) V(x) E(x)\n",
            "SEAS → Feedback: S(x) E(x) A(-) S(x)\n",
            "SLEEP → Feedback: S(x) L(x) E(x) E(x) P(x)\n",
            "SIRS → Feedback: S(x) I(-) R(x) S(x)\n",
            "SIRS → Feedback: S(x) I(-) R(x) S(x)\n",
            "SECRET → Feedback: S(x) E(x) C(x) R(x) E(x)\n",
            "SURE → Feedback: S(x) U(✓) R(x) E(x)\n",
            "SEES → Feedback: S(x) E(x) E(x) S(x)\n",
            "SISS → Feedback: S(x) I(-) S(x) S(x)\n",
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a world-class Wordle-solving AI. Your goal is to guess the secret 5-letter word.\\n\\n### Instructions\\n1.  **Analyze the feedback:** After each guess, you get clues.\\n    *   `✓` = Correct letter in the correct spot.\\n    *   `-` = Correct letter in the wrong spot.\\n    *   `x` = Letter is not in the word.\\n\\n2.  **State your reasoning:** Inside `<think>` tags, briefly explain your logic.\\n    *   What clues do you have so far? (Correct letters, misplaced letters, wrong letters).\\n    *   What is your plan for the next guess?\\n    *   Why is your chosen word a good strategic move?\\n\\n3.  **Make your guess:** After your reasoning, provide your 5-letter guess inside `<guess>` tags.\\n\\n**IMPORTANT:** You MUST always follow the `<think>` and `<guess>` format exactly like the examples below.\\n\\n---\\n### EXAMPLES\\n\\n**Example 1: First Guess**\\n\\n<think>\\nThis is the first guess. There are no clues yet. A good starting word uses common and unique letters to get the most information. 'RAISE' is a great choice as it tests three common vowels (A, I, E) and two common consonants (R, S).\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: A Mid-Game Guess**\\n\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n\\n<think>\\nBased on the clues, I know the word starts with 'BR' (B✓, R✓). I also know the letter 'S' is in the word, but not in the first position. The letters T, O, M, A, V, E are wrong and must be avoided. My strategy is to find a word that fits the pattern `BR__S` or `BR_S_`. The word 'BRISK' fits all these rules perfectly. It starts with 'BR', contains 'S', and uses no incorrect letters.\\n</think>\\n<guess>BRISK</guess>\\n\\n---\\nEnd of examples. The new puzzle starts now.\\n\"}, {'role': 'user', 'content': 'Make a new 5-letter word guess.\\n\\nHere is some previous feedback:\\nGuess 1: SLAVE → Feedback: S(x) L(x) A(-) V(x) E(x)\\nGuess 2: SEAS → Feedback: S(x) E(x) A(-) S(x)\\nGuess 3: SLEEP → Feedback: S(x) L(x) E(x) E(x) P(x)\\nGuess 4: SIRS → Feedback: S(x) I(-) R(x) S(x)\\nGuess 5: SIRS → Feedback: S(x) I(-) R(x) S(x)\\nGuess 6: SECRET → Feedback: S(x) E(x) C(x) R(x) E(x)\\nGuess 7: SURE → Feedback: S(x) U(✓) R(x) E(x)\\nGuess 8: SEES → Feedback: S(x) E(x) E(x) S(x)\\nGuess 9: SISS → Feedback: S(x) I(-) S(x) S(x)'}]\n",
            "match:  <re.Match object; span=(424, 443), match='<guess>SISS</guess>'>\n",
            "The LLM guessed:  SISS\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "SLAVE → Feedback: S(x) L(x) A(-) V(x) E(x)\n",
            "SEAS → Feedback: S(x) E(x) A(-) S(x)\n",
            "SLEEP → Feedback: S(x) L(x) E(x) E(x) P(x)\n",
            "SIRS → Feedback: S(x) I(-) R(x) S(x)\n",
            "SIRS → Feedback: S(x) I(-) R(x) S(x)\n",
            "SECRET → Feedback: S(x) E(x) C(x) R(x) E(x)\n",
            "SURE → Feedback: S(x) U(✓) R(x) E(x)\n",
            "SEES → Feedback: S(x) E(x) E(x) S(x)\n",
            "SISS → Feedback: S(x) I(-) S(x) S(x)\n",
            "SISS → Feedback: S(x) I(-) S(x) S(x)\n",
            "❌ better luck next time... ❌\n"
          ]
        }
      ],
      "source": [
        "n = 10\n",
        "for i in range(n):\n",
        "  next_turn_gemma(gemma_model, gemma_tokenizer, past_guesses, secret_word, number_guesses=n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVN4nt8TgIyx"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNNEajFvhv1C",
        "outputId": "f3724a03-4732-409e-ad6c-3d2629a4b925"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/charbelk/dev/wordle/wordle-rl/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['prompt', 'word_list', 'past_guess_history', 'secret'],\n",
            "    num_rows: 72\n",
            "})\n",
            "Dataset({\n",
            "    features: ['prompt', 'word_list', 'past_guess_history', 'secret'],\n",
            "    num_rows: 4\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Load dataset from HuggingFace\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Login using e.g. `huggingface-cli login` to access this dataset, the dataset has only train\n",
        "train_dataset, test_dataset = load_dataset(\"predibase/wordle-grpo\", split=['train[:95%]', 'train[:5%]'])\n",
        "print(train_dataset)\n",
        "print(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'prompt': '<|im_start|>system\\n\\nYou are playing Wordle, a word-guessing game.\\n\\n### Game Rules:\\n- You have **6 tries** to guess a secret **5-letter** word.\\n- Each guess must be a valid **5-letter English word**.\\n- After each guess, you will receive feedback indicating how close your guess was.\\n\\n### Feedback Format:\\nEach letter in your guess will receive one of three symbols:\\n1. ✓ : The letter is in the word and in the CORRECT position.\\n2. - : The letter is in the word but in the WRONG position.\\n3. x : The letter is NOT in the word.\\n\\n### Example:\\nSecret Word: BRISK\\n\\nGuess 1: STORM → Feedback: S(-) T(x) O(x) R(-) M(x)\\nGuess 2: BRAVE → Feedback: B(✓) R(✓) A(x) V(x) E(x)\\nGuess 3: BRISK → Feedback: B(✓) R(✓) I(✓) S(✓) K(✓)\\n\\n### Response Format:\\nThink through the problem and feedback step by step. Make sure to first add your step by step thought process within <think> </think> tags. Then, return your guessed word in the following format: <guess> guessed-word </guess>.\\n<|im_end|>\\n<|im_start|>user\\nMake a new 5-letter word guess.\\n\\n Here is some previous feedback:\\nGuess 1: CRANE -> Feedback: C(x) R(x) A(-) N(x) E(-)\\nGuess 2: SWEAT -> Feedback: S(x) W(x) E(-) A(-) T(x)<|im_end|>\\n<|im_start|>assistant\\nLet me solve this step by step.\\n<think>',\n",
              " 'word_list': 'https://raw.githubusercontent.com/arnavgarg1/arnavgarg1/refs/heads/main/five_letter_words.csv',\n",
              " 'past_guess_history': \"[['CRANE', 'C(x) R(x) A(-) N(x) E(-)'], ['SWEAT', 'S(x) W(x) E(-) A(-) T(x)']]\",\n",
              " 'secret': 'ALLEY'}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYWMVB6m-KwP"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.remove_columns(['word_list', 'prompt'])\n",
        "test_dataset = test_dataset.remove_columns(['word_list', 'prompt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def get_word_list_string_from_url(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Fetches a CSV file of words from a URL, parses it, and returns\n",
        "    a string representation of a Python list of those words.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the raw CSV file.\n",
        "\n",
        "    Returns:\n",
        "        str: A string that can be used as a Python list literal.\n",
        "             Returns an empty list '[]' on failure.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Fetch the data from the URL\n",
        "        print(f\"Fetching data from {url}...\")\n",
        "        response = requests.get(url)\n",
        "        # Raise an exception for bad status codes (like 404, 500)\n",
        "        response.raise_for_status()\n",
        "        print(\"Data fetched successfully.\")\n",
        "\n",
        "        # 2. Parse the CSV content\n",
        "        # The content is a single string with words separated by newlines\n",
        "        csv_text = response.text\n",
        "        lines = csv_text.splitlines()\n",
        "\n",
        "        # 3. Create a clean list of words\n",
        "        # We skip the first line (header: 'word') and filter out any empty lines\n",
        "        word_list = [line.strip() for line in lines[1:] if line.strip()]\n",
        "        \n",
        "        print(f\"Found {len(word_list)} five-letter words.\")\n",
        "\n",
        "        # 4. Convert the Python list to its string representation\n",
        "        return str(word_list)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error: Could not fetch data from the URL. {e}\")\n",
        "        return \"[]\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return \"[]\"\n",
        "\n",
        "\n",
        "# The URL you provided\n",
        "WORD_LIST_URL = \"https://raw.githubusercontent.com/arnavgarg1/arnavgarg1/refs/heads/main/five_letter_words.csv\"\n",
        "\n",
        "# Get the string\n",
        "five_letter_words_string = get_word_list_string_from_url(WORD_LIST_URL)\n",
        "\n",
        "# Print the final result (output is truncated for readability)\n",
        "print(\"\\nHere is the string representing the Python list:\")\n",
        "print(five_letter_words_string[:500] + \"...\") # Print the first 500 characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ_ye3EIpt8I"
      },
      "source": [
        "## Prepare reward functions\n",
        "\n",
        "in each reward function we will have the following args:\n",
        "\n",
        "- completions: what the model returned\n",
        "- prompts: the list of prompts\n",
        "- completion_ids : list of list of tokens\n",
        "- past_guess_history: list of guesses, example: [\"[['CRANE', 'C(x) R(x) A(x) N(x) E(-)']], ...]\n",
        "- secret: list of secrets, example: ['BEFIT', 'BEFIT',  ...]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjPJmt00xp0C"
      },
      "source": [
        "## Training\n",
        "\n",
        "Input:\n",
        "- initial_policy: Starting model to be trained\n",
        "- reward_function: Function that evaluates outputs\n",
        "- training_prompts: Set of training examples\n",
        "- group_size: Number of outputs per prompt (typically 4-16)\n",
        "\n",
        "Algorithm GRPO:\n",
        "\n",
        "For each training iteration:\n",
        "\n",
        "   a. Set reference_policy = initial_policy (snapshot current policy)\n",
        "\n",
        "   b. For each prompt in batch:\n",
        "\n",
        "      - Generate group_size different outputs using initial_policy\n",
        "\n",
        "      - Compute rewards for each output using reward_function\n",
        "\n",
        "      - Normalize rewards within group:\n",
        "\n",
        "           normalized_advantage = (reward - mean(rewards)) / std(rewards)\n",
        "\n",
        "      - Update policy by maximizing the clipped ratio:\n",
        "          min(prob_ratio * normalized_advantage, clip(prob_ratio, 1-epsilon, 1+epsilon) * normalized_advantage) - kl_weight * KL(initial_policy || reference_policy)\n",
        "\n",
        "          where prob_ratio is current_prob / reference_prob\n",
        "\n",
        "Output: Optimized policy model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GRPO A La Magistral but with an update\n",
        "\n",
        "As outlined above in the section: Magistral RL changes, the KL divergence was dropped entirely.\n",
        "\n",
        "\n",
        "1.  **Role #1: Calculating KL Divergence (Very Expensive)**\n",
        "    To calculate the full KL Divergence between two policies (`D_KL(π_current || π_ref)`), you have to compare the **entire probability distribution over the whole vocabulary** for every single token. This is computationally very expensive and is the part the Magistral authors decided was \"unjustified\" and **removed**.\n",
        "\n",
        "2.  **Role #2: Calculating the Policy Ratio (Relatively Cheap)**\n",
        "    The PPO/GRPO loss function is built on the ratio `r = π_current / π_ref`. To calculate this ratio, you only need the probability of the **single token that was actually generated**. You don't need the other 30,000+ probabilities in the distribution. This is much, much cheaper to compute.\n",
        "\n",
        "Let's look at the paper's PPO/GRPO formula again on Page 2. It needs `π_θ_old` (an old/reference policy) to compute the ratio. The entire clipping mechanism `clip(ratio, ...)` depends on it.\n",
        "\n",
        "So, when the authors say on Page 3 that they are removing the reference model, they specifically mean they are removing it from the costly **KL Divergence calculation**. They *keep* a reference policy to perform the much cheaper **ratio calculation** which is essential for the clipping mechanism.\n",
        "\n",
        " The `ref_model` is a snapshot of the initial model, and it **never changes**.\n",
        "\n",
        "\n",
        "*   **What it does:** The ratio `π_current / π_ref` would measure the change from the **immediately preceding step**. This makes the algorithm very \"on-policy,\" as you're always comparing against the policy that existed just a moment ago.\n",
        "*   **Trade-offs:** This is the most responsive to recent changes, but it can also be less stable. If the model takes a \"bad\" step, the reference point immediately shifts to that bad location, potentially making it harder to recover.\n",
        "\n",
        "### Recommended Approach: Periodic Updates (The Best of Both Worlds)\n",
        "\n",
        "The most common and robust strategy in practice is to update the reference model **periodically**, for example, every 10, 50, or 100 steps.\n",
        "\n",
        "*   **What it does:** This mimics the classic PPO workflow of:\n",
        "    1.  Collect a batch of data using the current policy `π_k`.\n",
        "    2.  For the next `N` steps, update the model, always comparing it against the fixed `π_k`.\n",
        "    3.  After `N` steps, update the reference policy: `π_k` becomes the newly improved model `π_{k+1}`.\n",
        "*   **Why it's good:** This provides a stable reference point for a block of updates, but also allows the reference to \"catch up\" to the learning progress, preventing the ratio from becoming too large or small over time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/charbelk/dev/local_train/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLX is using default device: Device(gpu, 0)\n",
            "Loading base model: mlx-community/gemma-3-1b-it-qat-8bit\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 8 files: 100%|██████████| 8/8 [00:00<00:00, 130561.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying LoRA to the model...\n",
            "LoRA trainable parameters: 0.459M | Total model parameters: 366.708M | Percentage: 0.1251%\n",
            "Creating reference model...\n",
            "Loading dataset 'predibase/wordle-grpo'...\n",
            "\n",
            "Starting GRPO training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GRPO Training Steps:   2%|▎         | 5/200 [01:36<1:04:19, 19.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 0005 | Train Loss: 0.7932 | Best Guess: 'CRANE' on 'PRANK' (R: 13.10)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GRPO Training Steps:   5%|▌         | 10/200 [03:01<53:49, 17.00s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 0010 | Train Loss: 0.3468 | Best Guess: 'BRAVO' on 'LOYAL' (R: 4.60)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GRPO Training Steps:   8%|▊         | 15/200 [04:27<54:10, 17.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 0015 | Train Loss: 0.0420 | Best Guess: 'BRISK' on 'BATTY' (R: 6.75)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GRPO Training Steps:  10%|█         | 20/200 [05:49<49:35, 16.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 0020 | Train Loss: 0.1557 | Best Guess: 'BRISK' on 'STUNK' (R: 5.60)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GRPO Training Steps:  12%|█▎        | 25/200 [07:16<50:38, 17.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 0025 | Train Loss: 0.0713 | Best Guess: 'REASE' on 'ALLEY' (R: 5.10)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GRPO Training Steps:  15%|█▌        | 30/200 [08:44<50:24, 17.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 0030 | Train Loss: 0.4247 | Best Guess: 'SRINE' on 'WINCH' (R: 4.60)\n",
            "\n",
            "--- Step 30: Updating reference model ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GRPO Training Steps:  18%|█▊        | 35/200 [10:15<50:32, 18.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 0035 | Train Loss: -0.1340 | Best Guess: 'BRASS' on 'REARM' (R: 6.25)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GRPO Training Steps:  20%|██        | 40/200 [11:40<45:17, 16.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 0040 | Train Loss: -0.0191 | Best Guess: 'CRANE' on 'MERGE' (R: 5.60)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GRPO Training Steps:  22%|██▎       | 45/200 [13:01<41:40, 16.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 0045 | Train Loss: -0.2268 | Best Guess: 'BRISK' on 'NINTH' (R: 4.75)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GRPO Training Steps:  25%|██▌       | 50/200 [14:29<45:13, 18.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 0050 | Train Loss: -0.1883 | Best Guess: 'BRAVE' on 'STONE' (R: 6.20)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 4/4 [00:36<00:00,  9.05s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 0050 | Eval Loss:  0.0000\n",
            "\n",
            "--- Checkpoint saved to adapters_step_50.npz ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GRPO Training Steps:  28%|██▊       | 55/200 [16:36<49:02, 20.30s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 0055 | Train Loss: 0.1864 | Best Guess: 'BRISK' on 'SHELF' (R: 4.10)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GRPO Training Steps:  30%|███       | 60/200 [18:19<48:36, 20.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 0060 | Train Loss: 0.2517 | Best Guess: 'RELIQ' on 'PERCH' (R: 5.60)\n",
            "\n",
            "--- Step 60: Updating reference model ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GRPO Training Steps:  32%|███▎      | 65/200 [19:43<39:18, 17.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 0065 | Train Loss: -0.0061 | Best Guess: 'CRANE' on 'CLOTH' (R: 5.10)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GRPO Training Steps:  35%|███▌      | 70/200 [21:28<44:53, 20.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 0070 | Train Loss: 0.0287 | Best Guess: 'RISER' on 'PITCH' (R: 5.10)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GRPO Training Steps:  38%|███▊      | 75/200 [23:27<49:32, 23.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 0075 | Train Loss: 0.8775 | Best Guess: 'CRANE' on 'DRONE' (R: 13.10)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GRPO Training Steps:  40%|████      | 80/200 [25:10<43:36, 21.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 0080 | Train Loss: 0.2830 | Best Guess: 'SINE' on 'SCENE' (R: 5.75)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GRPO Training Steps:  42%|████▎     | 85/200 [26:52<37:57, 19.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 0085 | Train Loss: 0.1528 | Best Guess: 'CREAM' on 'REEDY' (R: 5.60)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GRPO Training Steps:  44%|████▎     | 87/200 [27:27<35:09, 18.67s/it]"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a world-class Wordle-solving AI. Your goal is to guess the secret 5-letter word.\n",
        "\n",
        "### Instructions\n",
        "1.  **Analyze the feedback:** After each guess, you get clues.\n",
        "    *   `✓` = Correct letter in the correct spot.\n",
        "    *   `-` = Correct letter in the wrong spot.\n",
        "    *   `x` = Letter is not in the word.\n",
        "\n",
        "2.  **State your reasoning:** Inside `<think>` tags, briefly explain your logic.\n",
        "    *   What clues do you have so far? (Correct letters, misplaced letters, wrong letters).\n",
        "    *   What is your plan for the next guess?\n",
        "    *   Why is your chosen word a good strategic move?\n",
        "\n",
        "3.  **Make your guess:** After your reasoning, provide your 5-letter guess inside `<guess>` tags.\n",
        "\n",
        "**IMPORTANT:** You MUST always follow the `<think>` and `<guess>` format exactly like the examples below.\n",
        "\n",
        "---\n",
        "### EXAMPLES\n",
        "\n",
        "**Example 1: First Guess**\n",
        "\n",
        "<think>\n",
        "This is the first guess. There are no clues yet. A good starting word uses common and unique letters to get the most information. 'RAISE' is a great choice as it tests three common vowels (A, I, E) and two common consonants (R, S).\n",
        "</think>\n",
        "<guess>RAISE</guess>\n",
        "\n",
        "**Example 2: A Mid-Game Guess**\n",
        "\n",
        "**Clues so far:**\n",
        "*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\n",
        "*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\n",
        "\n",
        "<think>\n",
        "Based on the clues, I know the word starts with 'BR' (B✓, R✓). I also know the letter 'S' is in the word, but not in the first position. The letters T, O, M, A, V, E are wrong and must be avoided. My strategy is to find a word that fits the pattern `BR__S` or `BR_S_`. The word 'BRISK' fits all these rules perfectly. It starts with 'BR', contains 'S', and uses no incorrect letters.\n",
        "</think>\n",
        "<guess>BRISK</guess>\n",
        "\n",
        "---\n",
        "End of examples. The new puzzle starts now.\n",
        "\"\"\"\n",
        "# ==============================================================================\n",
        "# train_final.py\n",
        "#\n",
        "# A complete, self-contained script for training a language model on a Wordle\n",
        "# task using a Group Relative Policy Optimization (GRPO) style loss.\n",
        "#\n",
        "# This script was iteratively developed and incorporates several advanced\n",
        "# reinforcement learning concepts inspired by state-of-the-art methods like\n",
        "# those discussed in the Mistral AI \"Magistral\" paper, including:\n",
        "#   - A PPO-style clipped surrogate objective for stable updates.\n",
        "#   - Critic-free advantage calculation and normalization.\n",
        "#   - Per-token loss normalization to prevent length bias.\n",
        "#   - A periodically updated reference policy for the PPO ratio.\n",
        "#   - Detailed prompt engineering to guide model behavior.\n",
        "#   - Evaluation and plotting of loss curves.\n",
        "#\n",
        "# ==============================================================================\n",
        "import itertools\n",
        "import copy\n",
        "import re\n",
        "import ast\n",
        "import math\n",
        "import mlx.core as mx\n",
        "import mlx.nn as nn\n",
        "import mlx.optimizers as optim\n",
        "from mlx.utils import tree_flatten\n",
        "from mlx_lm import load, generate\n",
        "from mlx_lm.sample_utils import make_sampler\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==============================================================================\n",
        "# --- 1. CONFIGURATION ---\n",
        "# ==============================================================================\n",
        "# Model and training parameters\n",
        "# MODEL_NAME = \"mlx-community/gemma-3-1b-it-qat-4bit\"\n",
        "MODEL_NAME = \"mlx-community/gemma-3-1b-it-qat-8bit\"\n",
        "# MODEL_NAME = \"mlx-community/Qwen3-0.6B-4bit-DWQ-053125\"\n",
        "ADAPTER_FILE = \"wordle_gemma3_grpo_adapters.npz\"\n",
        "TRAIN_ITERATIONS = 200\n",
        "LEARNING_RATE = 2e-6\n",
        "LOGGING_STEPS = 5\n",
        "CHECKPOINT_STEPS = 50\n",
        "EVAL_STEPS = 50       # How often to run evaluation\n",
        "EVAL_SAMPLES = 4     # Number of samples to use for evaluation\n",
        "\n",
        "# LoRA parameters\n",
        "LORA_RANK = 8\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.1\n",
        "LORA_LAYERS_TO_TUNE = 16\n",
        "\n",
        "# RL / Generation parameters\n",
        "NUM_GENERATIONS = 4   # Number of responses to generate per prompt for the RL batch\n",
        "MAX_COMPLETION_LENGTH = 256\n",
        "\n",
        "# GRPO / PPO specific parameters\n",
        "PPO_CLIP_EPSILON = 0.2  # The clipping threshold for the PPO objective\n",
        "REF_UPDATE_STEPS = 30   # How often to update the reference model to the current model's state\n",
        "SAMPLING_TEMPERATURE=0.7\n",
        "# A detailed system prompt using prompt engineering techniques (e.g., role-playing,\n",
        "# structured instructions, few-shot examples) to guide the model's behavior. This\n",
        "# aligns the model's task with our reward function and encourages structured\n",
        "# chain-of-thought reasoning.\n",
        "\n",
        "# ==============================================================================\n",
        "# --- 2. LoRA Code (Copied directly from mlx-examples) ---\n",
        "# ==============================================================================\n",
        "# We define the LoRALinear class here to avoid importing it from mlx-examples,\n",
        "# making the script fully self-contained.\n",
        "class LoRALinear(nn.Module):\n",
        "    @staticmethod\n",
        "    def from_linear(linear: nn.Linear, rank: int = 8):\n",
        "        output_dims, input_dims = linear.weight.shape\n",
        "        if isinstance(linear, nn.QuantizedLinear):\n",
        "            input_dims *= 32 // linear.bits\n",
        "        lora_lin = LoRALinear(input_dims, output_dims, rank)\n",
        "        lora_lin.linear = linear\n",
        "        return lora_lin\n",
        "    def to_linear(self):\n",
        "        linear = self.linear\n",
        "        bias = \"bias\" in linear\n",
        "        weight = linear.weight\n",
        "        is_quantized = isinstance(linear, nn.QuantizedLinear)\n",
        "        dtype = weight.dtype\n",
        "        if is_quantized:\n",
        "            dtype = mx.float16\n",
        "            weight = mx.dequantize(\n",
        "                weight,\n",
        "                linear.scales,\n",
        "                linear.biases,\n",
        "                linear.group_size,\n",
        "                linear.bits,\n",
        "            )\n",
        "        output_dims, input_dims = weight.shape\n",
        "        fused_linear = nn.Linear(input_dims, output_dims, bias=bias)\n",
        "        lora_b = (self.scale * self.lora_b.T).astype(dtype)\n",
        "        lora_a = self.lora_a.T.astype(dtype)\n",
        "        fused_linear.weight = weight + lora_b @ lora_a\n",
        "        if bias:\n",
        "            fused_linear.bias = linear.bias\n",
        "        if is_quantized:\n",
        "            fused_linear = nn.QuantizedLinear.from_linear(\n",
        "                fused_linear,\n",
        "                linear.group_size,\n",
        "                linear.bits,\n",
        "            )\n",
        "        return fused_linear\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dims: int,\n",
        "        output_dims: int,\n",
        "        lora_rank: int = 8,\n",
        "        bias: bool = False,\n",
        "        scale: float = 20.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_dims, output_dims, bias=bias)\n",
        "        self.scale = scale\n",
        "        scale = 1 / math.sqrt(input_dims)\n",
        "        self.lora_a = mx.random.uniform(\n",
        "            low=-scale,\n",
        "            high=scale,\n",
        "            shape=(input_dims, lora_rank),\n",
        "        )\n",
        "        self.lora_b = mx.zeros(shape=(lora_rank, output_dims))\n",
        "    def __call__(self, x):\n",
        "        dtype = self.linear.weight.dtype\n",
        "        if isinstance(self.linear, nn.QuantizedLinear):\n",
        "            dtype = self.linear.scales.dtype\n",
        "        y = self.linear(x.astype(dtype))\n",
        "        z = (x @ self.lora_a) @ self.lora_b\n",
        "        return y + self.scale * z\n",
        "\n",
        "# ==============================================================================\n",
        "# --- 3. HELPER & REWARD FUNCTIONS ---\n",
        "# ==============================================================================\n",
        "# These functions help parse the data and calculate a reward score for each\n",
        "# generated response. The total reward guides the reinforcement learning process.\n",
        "import ast\n",
        "\n",
        "def format_prompt_from_dataset(sample):\n",
        "    \"\"\"\n",
        "    Formats the user prompt to match the markdown style in the system prompt's examples.\n",
        "\n",
        "    For the first turn, it provides a simple instruction. For subsequent turns,\n",
        "    it formats the history as a markdown list under a \"Clues so far\" heading.\n",
        "    \"\"\"\n",
        "    history_str = sample['past_guess_history']\n",
        "    \n",
        "    try:\n",
        "        # Safely evaluate the string representation of the list\n",
        "        history_list = ast.literal_eval(history_str)\n",
        "    except (ValueError, SyntaxError):\n",
        "        history_list = []\n",
        "\n",
        "    # --- Case 1: First guess of the game ---\n",
        "    if not history_list:\n",
        "        return \"This is the first turn. Please provide your best starting word.\"\n",
        "\n",
        "    # --- Case 2: Mid-game with a history of previous guesses ---\n",
        "    prompt_parts = [\"**Clues so far:**\"]\n",
        "    for i, (guess, feedback) in enumerate(history_list):\n",
        "        # Format each line to match the markdown list in the system prompt, e.g.:\n",
        "        # \"*   Guess 1: RAISE → R(x) A(✓) I(x) S(-) E(x)\"\n",
        "        prompt_parts.append(f\"*   Guess {i+1}: {guess} → {feedback}\")\n",
        "    \n",
        "    # Join all parts into a single string separated by newlines\n",
        "    return \"\\n\".join(prompt_parts)\n",
        "\n",
        "def parse_guess(response: str) -> str:\n",
        "    \"\"\"\n",
        "    Parses the guessed word from the model's response.\n",
        "    This function robustly extracts all letters from within the <guess>\n",
        "    tag and returns them as a clean string, regardless of length.\n",
        "    \"\"\"\n",
        "    match = re.search(r\"<guess>(.*?)</guess>\", response, re.DOTALL | re.IGNORECASE)\n",
        "    if not match:\n",
        "        return None\n",
        "\n",
        "    # Clean the extracted content: find all alphabetic characters and join them.\n",
        "    letters = re.findall(r\"[A-Z]\", match.group(1), re.IGNORECASE)\n",
        "    \n",
        "    if not letters:\n",
        "        return None\n",
        "\n",
        "    return \"\".join(letters).upper()\n",
        "\n",
        "\n",
        "def reward_for_format(completion: str) -> float:\n",
        "    \"\"\"\n",
        "    Rewards the model for correctly using the <think> and <guess> tags.\n",
        "    It provides a base reward for a valid guess format and a large bonus\n",
        "    if the guess is the correct length (5 letters).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # A basic check for the presence of all required tags\n",
        "        if not (\"<think>\" in completion and \"</think>\" in completion and \n",
        "                \"<guess>\" in completion and \"</guess>\" in completion):\n",
        "            return 0.0\n",
        "\n",
        "        guess = parse_guess(completion)\n",
        "        \n",
        "        # If no guess could be parsed at all, reward is 0.\n",
        "        if not guess:\n",
        "            return 0.0\n",
        "\n",
        "        # --- Reward Shaping ---\n",
        "        # 1. Give a small base reward for successfully producing a parsable guess.\n",
        "        #    This encourages the model to learn the tag format.\n",
        "        reward = 1.0\n",
        "        \n",
        "        # 2. Give a large additional reward only if the guess is the correct length.\n",
        "        #    This specifically encourages the model to learn the 5-letter constraint.\n",
        "        if len(guess) == 5:\n",
        "            reward += 2.5 # Bonus for getting the length right\n",
        "            \n",
        "        return reward\n",
        "    except Exception: \n",
        "        return 0.0\n",
        "\n",
        "def reward_for_feedback_use(completion: str, history_str: str) -> float:\n",
        "    try:\n",
        "        guess = parse_guess(completion)\n",
        "        if not guess: return 0.0\n",
        "        history_list = ast.literal_eval(history_str) if history_str else []\n",
        "        \n",
        "        if not history_list: \n",
        "            # return 1 for the guess we dont want to penalize on the first guess\n",
        "            # avoid reward hacking?\n",
        "            return 1\n",
        "        correct, valid, wrong = {}, {}, {}\n",
        "        for _, feedback_str in history_list:\n",
        "            for i, fb in enumerate(feedback_str.split(\" \")):\n",
        "                letter, status = fb[0], fb[2]\n",
        "                if status == '✓': correct.setdefault(letter, set()).add(i)\n",
        "                elif status == '-': valid.setdefault(letter, set()).add(i)\n",
        "                else: wrong.setdefault(letter, set()).add(i)\n",
        "        reward = 0.0\n",
        "        for idx, letter in enumerate(guess):\n",
        "            if (letter in correct and idx in correct[letter]): reward += 2\n",
        "            elif (letter in valid and idx not in valid[letter]): reward += 1\n",
        "            elif (letter in valid and idx in valid[letter]): reward -= 1.5\n",
        "            elif letter in wrong: reward -= 0.5\n",
        "            else: reward += 0.05\n",
        "        return max(0, reward)\n",
        "    except Exception: return 0.0\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def reward_partial_credit(guess: str, secret_word: str) -> float:\n",
        "    \"\"\"\n",
        "    Calculates a partial credit reward for a guess against a secret word,\n",
        "    correctly handling duplicate letters.\n",
        "    \"\"\"\n",
        "    # Ensure consistent case for comparison\n",
        "    guess = guess.upper()\n",
        "    secret_word = secret_word.upper()\n",
        "    \n",
        "    # Handle cases where guess and secret word might not be the same length\n",
        "    # This can happen if the model generates a non-5-letter word\n",
        "    min_len = min(len(guess), len(secret_word))\n",
        "    \n",
        "    reward = 0.1  # Base reward for making a guess\n",
        "    \n",
        "    # Use a Counter to track the available letters for scoring \"yellows\"\n",
        "    secret_counts = Counter(secret_word)\n",
        "    \n",
        "    # Use a list to track which letters in the guess have been scored\n",
        "    # to prevent scoring them twice (as both green and yellow)\n",
        "    scored = [False] * len(guess)\n",
        "\n",
        "    # --- Pass 1: Score \"Green\" letters (exact matches) ---\n",
        "    for i in range(min_len):\n",
        "        if guess[i] == secret_word[i]:\n",
        "            reward += 1.5\n",
        "            secret_counts[guess[i]] -= 1  # Decrement the count for this matched letter\n",
        "            scored[i] = True\n",
        "\n",
        "    # --- Pass 2: Score \"Yellow\" letters (misplaced matches) ---\n",
        "    for i in range(len(guess)):\n",
        "        # Only check letters that were NOT a perfect match (not green)\n",
        "        if not scored[i]:\n",
        "            # If the letter exists in the secret word AND we still have some left to match\n",
        "            if guess[i] in secret_counts and secret_counts[guess[i]] > 0:\n",
        "                reward += 0.5\n",
        "                secret_counts[guess[i]] -= 0.5  # Decrement the count, \"using up\" this yellow match\n",
        "    \n",
        "    return reward\n",
        "\n",
        "def calculate_total_reward(completion: str, sample: dict) -> float:\n",
        "    secret_word = sample['secret']\n",
        "    history_str = sample['past_guess_history']\n",
        "    format_r = reward_for_format(completion)\n",
        "    if format_r == 0.0: \n",
        "        return 0.0\n",
        "    feedback_r = reward_for_feedback_use(completion, history_str)\n",
        "    guess = parse_guess(completion)\n",
        "    solution_r = 0.1\n",
        "    if guess:\n",
        "        if guess.upper() == secret_word.upper():\n",
        "            solution_r = 10.0\n",
        "        else:\n",
        "            solution_r = reward_partial_credit(guess, secret_word)\n",
        "    return format_r + feedback_r + solution_r\n",
        "\n",
        "def get_log_probs(model, input_ids, output_ids):\n",
        "    full_sequence = mx.concatenate([input_ids, output_ids], axis=1)\n",
        "    logits = model(full_sequence)\n",
        "    # take only the output logits\n",
        "    # [: take all the batches\n",
        "    # the index where we start taking the slice: input_ids.shape[1]-1\n",
        "    # the cat sat on -> 4 - 1 : we slice at 3\n",
        "    # :-1 (the end of the slice)\n",
        "    # :]: this is the vocabulary dimension, we need it for the cross-entropy calculation.\n",
        "    output_logits = logits[:, input_ids.shape[1]-1:-1, :]\n",
        "    negative_log_probs = nn.losses.cross_entropy(output_logits, output_ids, reduction=\"none\")\n",
        "    return -mx.sum(negative_log_probs)\n",
        "\n",
        "def grpo_loss_and_grad(model, ref_model, prompt_tok, gen_toks_list, rewards):\n",
        "    \"\"\"Calculates the GRPO loss using a PPO-style clipped surrogate objective.\n",
        "\n",
        "    This function implements the core reinforcement learning update rule. It is\n",
        "    heavily inspired by the practical implementation of GRPO in the Mistral AI\n",
        "    \"Magistral\" paper, which adapts the PPO objective for training LLMs.\n",
        "\n",
        "    The key steps are:\n",
        "    1.  Calculate a critic-free advantage for each generation by baselining\n",
        "        the reward against the mean reward of the group (r - μ).\n",
        "    2.  Normalize the advantages to have a mean of 0 and std of 1 for stability.\n",
        "    3.  Compute the policy probability ratio (π_current / π_reference) for each\n",
        "        generation. The reference model is crucial here but is NOT used for the\n",
        "        computationally expensive KL divergence, which is omitted.\n",
        "    4.  Apply the PPO clipping mechanism to the ratio and advantage to prevent\n",
        "        destabilizingly large policy updates.\n",
        "    5.  Normalize the final loss on a per-token basis to avoid length bias,\n",
        "        ensuring each token contributes equally to the gradient.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The main, trainable policy model whose parameters are\n",
        "            being updated.\n",
        "        ref_model (nn.Module): A frozen, periodically updated copy of the policy\n",
        "            model that serves as the reference for the ratio calculation.\n",
        "        prompt_tok (mx.array): A 1D array of tokens for the input prompt.\n",
        "        gen_toks_list (list[mx.array]): A list where each element is a 1D array\n",
        "            of tokens for a single generated response.\n",
        "        rewards (mx.array): A 1D array of scalar reward values, one for each\n",
        "            generation in `gen_toks_list`.\n",
        "\n",
        "    Returns:\n",
        "        mx.array: A single scalar (0-dimensional) tensor representing the final,\n",
        "        normalized loss for the batch, ready for backpropagation.\n",
        "    \"\"\"\n",
        "    # 1. Advantage Calculation (Critic-Free)\n",
        "    # As proposed in GRPO, we create a baseline by subtracting the mean reward\n",
        "    # of the group. This avoids needing a separate, complex \"critic\" model.\n",
        "    advantages = rewards - mx.mean(rewards)\n",
        "\n",
        "    # 2. Advantage Normalization\n",
        "    # We normalize the advantages to have a mean of 0 and std of 1. This is a\n",
        "    # standard RL technique to stabilize training when rewards might vary wildly.\n",
        "    advantages = (advantages - mx.mean(advantages)) / (mx.std(advantages) + 1e-8)\n",
        "\n",
        "    # This ensures that a single lucky/unlucky generation doesn't create a huge advantage.\n",
        "    advantages = mx.clip(advantages, -5.0, 5.0)\n",
        "\n",
        "    # 3. Policy Ratio Calculation\n",
        "    # We get log-probabilities from both the current (training) model and the\n",
        "    # frozen reference model to compute the ratio r = π_current / π_ref.\n",
        "    log_probs_current = mx.array([get_log_probs(model, prompt_tok, gen_tok) for gen_tok in gen_toks_list])\n",
        "    \n",
        "    # The reference model is used for the cheap ratio calculation, NOT for the\n",
        "    # expensive KL divergence, which we have omitted as per the paper.\n",
        "    log_probs_ref = mx.array([get_log_probs(ref_model, prompt_tok, gen_tok) for gen_tok in gen_toks_list])\n",
        "\n",
        "   # We clip the log-ratio to a reasonable range like [-15, 15] before exponentiating.\n",
        "    log_ratios = log_probs_current - log_probs_ref\n",
        "    log_ratios = mx.clip(log_ratios, -4.0, 4.0)\n",
        "    ratios = mx.exp(log_ratios)\n",
        "\n",
        "    # 4. PPO Clipped Objective\n",
        "    # This is the core of PPO's stability. It prevents the model from taking\n",
        "    # excessively large update steps by clipping the objective.\n",
        "    unclipped_objective = ratios * advantages\n",
        "    clipped_objective = mx.clip(ratios, 1 - PPO_CLIP_EPSILON, 1 + PPO_CLIP_EPSILON) * advantages\n",
        "    ppo_objective = mx.minimum(unclipped_objective, clipped_objective)\n",
        "\n",
        "    # 5. Loss Normalization\n",
        "    # We calculate a per-token loss by weighting each sequence's objective\n",
        "    # by its length. This prevents longer sequences from dominating the gradient.\n",
        "    gen_lengths = mx.array([t.shape[1] for t in gen_toks_list])\n",
        "    weighted_objective = ppo_objective * gen_lengths\n",
        "    \n",
        "    # We minimize the negative of the objective.\n",
        "    loss = -mx.sum(weighted_objective) / mx.sum(gen_lengths)\n",
        "    return loss\n",
        "\n",
        "# ==============================================================================\n",
        "# --- 4. EVALUATION FUNCTION ---\n",
        "# ==============================================================================\n",
        "def evaluate(model, ref_model, tokenizer, dataset, num_samples):\n",
        "    \"\"\"\n",
        "    Periodically evaluates the model on a subset of the test data.\n",
        "    This runs without computing gradients to save memory and time.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    # Use greedy sampling (temp=0.0) for deterministic and repeatable evaluation\n",
        "    sampler = make_sampler(temp=0.0)\n",
        "\n",
        "    # Ensure we don't request more samples than are in the dataset\n",
        "    num_to_select = min(num_samples, len(dataset))\n",
        "    if num_to_select == 0:\n",
        "        print(\"Warning: Evaluation dataset is empty. Skipping evaluation.\")\n",
        "        return 0.0\n",
        "    eval_subset = dataset.shuffle().select(range(num_to_select))\n",
        "\n",
        "    for sample in tqdm(eval_subset, desc=\"Evaluating\"):\n",
        "        # 1. Prepare the prompt for the current sample\n",
        "        user_prompt_text = format_prompt_from_dataset(sample)\n",
        "        full_prompt_text = tokenizer.apply_chat_template(\n",
        "            [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": user_prompt_text}],\n",
        "            add_generation_prompt=True, tokenize=False\n",
        "        )\n",
        "        prompt_tokens = mx.array(tokenizer.encode(full_prompt_text)).reshape(1, -1)\n",
        "\n",
        "        # 2. Generate responses\n",
        "        generations = []\n",
        "        # With temp=0.0, the model will produce the same output every time.\n",
        "        # We still generate multiple times to fit the GRPO loss structure.\n",
        "        for _ in range(NUM_GENERATIONS):\n",
        "            response_text = generate(\n",
        "                model, tokenizer, prompt=full_prompt_text + \"<think>\",\n",
        "                max_tokens=MAX_COMPLETION_LENGTH, sampler=sampler, verbose=False\n",
        "            )\n",
        "            full_completion = \"<think>\" + response_text\n",
        "            generations.append({\n",
        "                \"text\": full_completion,\n",
        "                \"tokens\": mx.array(tokenizer.encode(full_completion)).reshape(1, -1)\n",
        "            })\n",
        "\n",
        "        # 3. Calculate rewards for each generated response\n",
        "        rewards = mx.array([calculate_total_reward(gen[\"text\"], sample) for gen in generations])\n",
        "\n",
        "        # We must skip any evaluation sample that results in zero advantage. This is\n",
        "        # guaranteed to happen here because deterministic sampling (temp=0.0) produces\n",
        "        # identical generations, leading to identical rewards. Without this check,\n",
        "        # the calculated loss would be a misleading 0.0.\n",
        "        if mx.all(rewards == rewards[0]):\n",
        "            continue\n",
        "\n",
        "        # 4. If we pass the check, calculate the loss for this sample\n",
        "        all_gen_tokens = [gen[\"tokens\"] for gen in generations]\n",
        "        \n",
        "        # Calculate loss directly, without using the gradient function.\n",
        "        # This is the MLX equivalent of a \"no_grad\" block.\n",
        "        loss = grpo_loss_and_grad(model, ref_model, prompt_tokens, all_gen_tokens, rewards)\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    model.train()  # Set the model back to training mode\n",
        "    \n",
        "    # Return the average loss, or 0.0 if all evaluation batches were skipped\n",
        "    return (total_loss / num_batches) if num_batches > 0 else 0.0\n",
        "\n",
        "# ==============================================================================\n",
        "# --- 5. MAIN TRAINING SCRIPT ---\n",
        "# ==============================================================================\n",
        "def main():\n",
        "    print(f\"MLX is using default device: {mx.default_device()}\")\n",
        "    print(f\"Loading base model: {MODEL_NAME}\")\n",
        "    model, tokenizer = load(MODEL_NAME)\n",
        "\n",
        "    print(\"Applying LoRA to the model...\")\n",
        "    model.freeze()\n",
        "    for l in model.model.layers[len(model.model.layers) - LORA_LAYERS_TO_TUNE :]:\n",
        "        l.self_attn.q_proj = LoRALinear.from_linear(l.self_attn.q_proj)\n",
        "        l.self_attn.v_proj = LoRALinear.from_linear(l.self_attn.v_proj)\n",
        "        if hasattr(l, \"block_sparse_moe\"):\n",
        "            l.block_sparse_moe.gate = LoRALinear.from_linear(l.block_sparse_moe.gate)\n",
        "    # Calculate the number of trainable parameters (LoRA adapters)\n",
        "    trainable_params = sum(v.size for _, v in tree_flatten(model.trainable_parameters()))\n",
        "\n",
        "    # Calculate the total number of parameters in the model\n",
        "    total_params = sum(v.size for _, v in tree_flatten(model.parameters()))\n",
        "    print(\n",
        "        f\"LoRA trainable parameters: {trainable_params / 1e6:.3f}M | \"\n",
        "        f\"Total model parameters: {total_params / 1e6:.3f}M | \"\n",
        "        f\"Percentage: {(trainable_params / total_params) * 100:.4f}%\"\n",
        "    )\n",
        "    \n",
        "    # Create the reference model for the PPO ratio calculation. It starts as a\n",
        "    # copy of the initial model and is updated periodically.\n",
        "    print(\"Creating reference model...\")\n",
        "    ref_model = copy.deepcopy(model)\n",
        "    ref_model.freeze()\n",
        "\n",
        "    model.train()\n",
        "    optimizer = optim.AdamW(learning_rate=LEARNING_RATE)\n",
        "    try:\n",
        "        tokenizer.eos_token_id = tokenizer.get_vocab()[\"<end_of_turn>\"]\n",
        "    except KeyError:\n",
        "        print(\"Warning: '<end_of_turn>' token not found. Using default EOS.\")\n",
        "\n",
        "    print(\"Loading dataset 'predibase/wordle-grpo'...\")\n",
        "    train_dataset, test_dataset = load_dataset(\"predibase/wordle-grpo\", split=['train[:95%]', 'train[:5%]'])\n",
        "    train_dataset = train_dataset.shuffle(seed=42).remove_columns(['word_list', 'prompt'])\n",
        "    test_dataset = test_dataset.shuffle(seed=42).remove_columns(['word_list', 'prompt'])\n",
        "\n",
        "    # Create the function that computes the loss and gradients.\n",
        "    # It's configured to only compute gradients with respect to `model`.\n",
        "    grad_fn = nn.value_and_grad(model, grpo_loss_and_grad)\n",
        "\n",
        "    # Lists to store data for plotting loss curves\n",
        "    train_steps, train_losses = [], []\n",
        "    eval_steps, eval_losses = [], []\n",
        "\n",
        "    print(\"\\nStarting GRPO training...\")\n",
        "    step_counter = 0\n",
        "    sampler = make_sampler(temp=SAMPLING_TEMPERATURE)\n",
        "    skipped_batches = 0\n",
        "    data_iterator = iter(itertools.cycle(train_dataset))\n",
        "    pbar = tqdm(total=TRAIN_ITERATIONS, desc=\"GRPO Training Steps\")\n",
        "\n",
        "    # Main training loop\n",
        "    while step_counter < TRAIN_ITERATIONS:\n",
        "        sample = next(data_iterator)\n",
        "\n",
        "        # 1. Prepare the prompt\n",
        "        user_prompt_text = format_prompt_from_dataset(sample)\n",
        "        full_prompt_text = tokenizer.apply_chat_template(\n",
        "            [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": user_prompt_text}],\n",
        "            add_generation_prompt=True, tokenize=False\n",
        "        )\n",
        "        prompt_tokens = mx.array(tokenizer.encode(full_prompt_text)).reshape(1, -1)\n",
        "\n",
        "        # 2. Generate a batch of responses from the current policy\n",
        "        generations = []\n",
        "        for _ in range(NUM_GENERATIONS):\n",
        "            response_text = generate(\n",
        "                model, tokenizer, prompt=full_prompt_text + \"<think>\",\n",
        "                max_tokens=MAX_COMPLETION_LENGTH, sampler=sampler, verbose=False\n",
        "            )\n",
        "            full_completion = \"<think>\" + response_text\n",
        "            generations.append({\"text\": full_completion, \"tokens\": mx.array(tokenizer.encode(full_completion)).reshape(1, -1)})\n",
        "\n",
        "        # 3. Score all responses to get rewards\n",
        "        rewards = mx.array([calculate_total_reward(gen[\"text\"], sample) for gen in generations])\n",
        "        \n",
        "        # Add a \"quality gate\" to skip batches with no valid responses.\n",
        "        # If the max reward is 0, it means every generation failed completely.\n",
        "        # These \"dead\" batches will cause 'nan' and provide no learning signal, so we skip them.\n",
        "        if mx.max(rewards) <= 0.1: # Using <= 0.1 to also catch cases of only malformed guesses\n",
        "            # This optional print helps confirm the gate is working\n",
        "            print(f\"Skipping batch with max reward {mx.max(rewards).item():.2f} on the following generations:\")\n",
        "            for gen in generations:\n",
        "                print(\"-- \", gen[\"text\"])\n",
        "            skipped_batches += 1\n",
        "            pbar.set_postfix(skipped=skipped_batches)\n",
        "            continue # Go to the next iteration of the while-loop\n",
        "\n",
        "        # Add a tiny amount of noise to break reward ties\n",
        "        rewards = rewards + mx.random.normal(rewards.shape) * 1e-6\n",
        "        if mx.all(rewards == rewards[0]): # Skip if all rewards are the same (zero advantage)\n",
        "            skipped_batches += 1\n",
        "            pbar.set_postfix(skipped=skipped_batches)\n",
        "            continue\n",
        "\n",
        "        all_gen_tokens = [gen[\"tokens\"] for gen in generations]\n",
        "\n",
        "        # 4. Compute loss and gradients, then update the model\n",
        "        loss, grads = grad_fn(model, ref_model, prompt_tokens, all_gen_tokens, rewards)\n",
        "        grads, _ = optim.clip_grad_norm(grads, 1.0) # Clip the gradients to a max norm of 1.0\n",
        "        optimizer.update(model, grads)\n",
        "        mx.eval(model.parameters(), optimizer.state)\n",
        "\n",
        "        # === Periodic Logging, Evaluation, and Checkpointing ===\n",
        "        # 6. A successful step is complete. Increment counter and update progress bar.\n",
        "        step_counter += 1\n",
        "        pbar.update(1)\n",
        "\n",
        "        if step_counter % LOGGING_STEPS == 0:\n",
        "            best_idx = mx.argmax(rewards).item()\n",
        "            best_guess = parse_guess(generations[best_idx]['text']) or \"N/A\"\n",
        "            print(\n",
        "                f\"\\nStep {step_counter:04d} | Train Loss: {loss.item():.4f} | \"\n",
        "                f\"Best Guess: '{best_guess}' on '{sample['secret']}' (R: {rewards[best_idx].item():.2f})\"\n",
        "            )\n",
        "            train_steps.append(step_counter)\n",
        "            train_losses.append(loss.item())\n",
        "        \n",
        "        if step_counter % EVAL_STEPS == 0:\n",
        "            eval_loss = evaluate(model, ref_model, tokenizer, test_dataset, EVAL_SAMPLES)\n",
        "            print(f\"\\nStep {step_counter:04d} | Eval Loss:  {eval_loss:.4f}\")\n",
        "            eval_steps.append(step_counter)\n",
        "            eval_losses.append(eval_loss)\n",
        "\n",
        "        if step_counter % REF_UPDATE_STEPS == 0:\n",
        "            print(f\"\\n--- Step {step_counter}: Updating reference model ---\")\n",
        "            ref_model.update(model.parameters())\n",
        "\n",
        "        if step_counter % CHECKPOINT_STEPS == 0:\n",
        "            checkpoint_file = f\"adapters_step_{step_counter}.npz\"\n",
        "            mx.savez(checkpoint_file, **dict(tree_flatten(model.trainable_parameters())))\n",
        "            print(f\"\\n--- Checkpoint saved to {checkpoint_file} ---\")\n",
        "\n",
        "\n",
        "\n",
        "    # --- Final Actions After Training ---\n",
        "    print(\"\\n--- Training Finished ---\")\n",
        "    print(f\"Saving final adapter weights to {ADAPTER_FILE}\")\n",
        "    mx.savez(ADAPTER_FILE, **dict(tree_flatten(model.trainable_parameters())))\n",
        "    print(\"Done.\")\n",
        "\n",
        "    # Generate and save the loss curve plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(train_steps, train_losses, label=\"Training Loss\")\n",
        "    plt.plot(eval_steps, eval_losses, label=\"Evaluation Loss\", marker='o', linestyle='--')\n",
        "    plt.xlabel(\"Training Steps\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training and Evaluation Loss Curves\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plot_filename = \"loss_curves.png\"\n",
        "    plt.savefig(plot_filename)\n",
        "    print(f\"Loss curve plot saved to {plot_filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate with the trained adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from mlx.utils import tree_unflatten\n",
        "from mlx_lm import load, generate\n",
        "import mlx.nn as nn\n",
        "import math\n",
        "import mlx.core as mx\n",
        "import ast\n",
        "# ==============================================================================\n",
        "# TODO make a config file for these parameters\n",
        "LORA_LAYERS_TO_TUNE = 16\n",
        "LORA_RANK = 8\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.1\n",
        "LORA_LAYERS_TO_TUNE = 16\n",
        "MAX_COMPLETION_LENGTH = 256\n",
        "\n",
        "def format_prompt_from_dataset(sample):\n",
        "    \"\"\"\n",
        "    Formats the user prompt to match the markdown style in the system prompt's examples.\n",
        "\n",
        "    For the first turn, it provides a simple instruction. For subsequent turns,\n",
        "    it formats the history as a markdown list under a \"Clues so far\" heading.\n",
        "    \"\"\"\n",
        "    history_str = sample['past_guess_history']\n",
        "    \n",
        "    try:\n",
        "        # Safely evaluate the string representation of the list\n",
        "        history_list = ast.literal_eval(history_str)\n",
        "    except (ValueError, SyntaxError):\n",
        "        history_list = []\n",
        "\n",
        "    # --- Case 1: First guess of the game ---\n",
        "    if not history_list:\n",
        "        return \"This is the first turn. Please provide your best starting word.\"\n",
        "\n",
        "    # --- Case 2: Mid-game with a history of previous guesses ---\n",
        "    prompt_parts = [\"**Clues so far:**\"]\n",
        "    for i, (guess, feedback) in enumerate(history_list):\n",
        "        # Format each line to match the markdown list in the system prompt, e.g.:\n",
        "        # \"*   Guess 1: RAISE → R(x) A(✓) I(x) S(-) E(x)\"\n",
        "        prompt_parts.append(f\"*   Guess {i+1}: {guess} → {feedback}\")\n",
        "    \n",
        "    # Join all parts into a single string separated by newlines\n",
        "    return \"\\n\".join(prompt_parts)\n",
        "\n",
        "class LoRALinear(nn.Module):\n",
        "    @staticmethod\n",
        "    def from_linear(linear: nn.Linear, rank: int = 8):\n",
        "        output_dims, input_dims = linear.weight.shape\n",
        "        if isinstance(linear, nn.QuantizedLinear):\n",
        "            input_dims *= 32 // linear.bits\n",
        "        lora_lin = LoRALinear(input_dims, output_dims, rank)\n",
        "        lora_lin.linear = linear\n",
        "        return lora_lin\n",
        "\n",
        "    def to_linear(self):\n",
        "        linear = self.linear\n",
        "        \n",
        "        # check for the bias\n",
        "        has_bias = hasattr(linear, \"bias\") and linear.bias is not None\n",
        "\n",
        "        weight = linear.weight\n",
        "        is_quantized = isinstance(linear, nn.QuantizedLinear)\n",
        "        original_dtype = weight.dtype\n",
        "\n",
        "        if is_quantized:\n",
        "            # Dequantize the weight to a high-precision format for the fusion\n",
        "            weight = mx.dequantize(\n",
        "                weight,\n",
        "                linear.scales,\n",
        "                linear.biases,\n",
        "                linear.group_size,\n",
        "                linear.bits,\n",
        "            )\n",
        "\n",
        "        # The LoRA update is W' = W + scale * (A @ B).T\n",
        "        # which is equivalent to W' = W + scale * (B.T @ A.T)\n",
        "        lora_a_t = self.lora_a.T\n",
        "        lora_b_t = self.lora_b.T\n",
        "\n",
        "        # Correct matrix multiplication: (out, rank) @ (rank, in) -> (out, in)\n",
        "        delta_w = (self.scale * lora_b_t) @ lora_a_t\n",
        "                \n",
        "        # Ensure all components have a compatible dtype for the addition\n",
        "        fused_weight = weight.astype(delta_w.dtype) + delta_w\n",
        "\n",
        "        # Create the new, non-quantized linear layer\n",
        "        output_dims, input_dims = fused_weight.shape\n",
        "        fused_linear = nn.Linear(input_dims, output_dims, bias=has_bias)\n",
        "        \n",
        "        # Assign the fused weights and original bias\n",
        "        fused_linear.weight = fused_weight.astype(original_dtype)\n",
        "        if has_bias:\n",
        "            fused_linear.bias = linear.bias.astype(original_dtype)\n",
        "\n",
        "        # Re-quantize the fused layer if the original was quantized\n",
        "        if is_quantized:\n",
        "            print(f\"Note: Skipping re-quantization for layer to avoid Metal kernel error.\")\n",
        "   \n",
        "            # fused_linear = nn.QuantizedLinear.from_linear(\n",
        "            #     fused_linear,\n",
        "            #     linear.group_size,\n",
        "            #     linear.bits,\n",
        "            # )\n",
        "            \n",
        "        return fused_linear\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dims: int,\n",
        "        output_dims: int,\n",
        "        lora_rank: int = 8,\n",
        "        bias: bool = False,\n",
        "        scale: float = 20.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_dims, output_dims, bias=bias)\n",
        "        self.scale = scale\n",
        "        scale = 1 / math.sqrt(input_dims)\n",
        "        self.lora_a = mx.random.uniform(\n",
        "            low=-scale,\n",
        "            high=scale,\n",
        "            shape=(input_dims, lora_rank),\n",
        "        )\n",
        "        self.lora_b = mx.zeros(shape=(lora_rank, output_dims))\n",
        "    def __call__(self, x):\n",
        "        dtype = self.linear.weight.dtype\n",
        "        if isinstance(self.linear, nn.QuantizedLinear):\n",
        "            dtype = self.linear.scales.dtype\n",
        "        y = self.linear(x.astype(dtype))\n",
        "        z = (x @ self.lora_a) @ self.lora_b\n",
        "        return y + self.scale * z\n",
        "\n",
        "\n",
        "\n",
        "from mlx_lm import load\n",
        "from mlx.utils import tree_unflatten\n",
        "import mlx.core as mx\n",
        "import os\n",
        "\n",
        "# Assume LoRALinear, format_prompt_from_dataset, etc., are defined elsewhere\n",
        "# in your script as they were in your training code.\n",
        "\n",
        "def fuse_lora_model(\n",
        "    base_model_id: str,\n",
        "    adapter_file: str,\n",
        "    lora_layers: int,\n",
        "    lora_rank: int\n",
        "):\n",
        "    \"\"\"\n",
        "    Loads a base model, applies LoRA layers, loads trained adapter weights,\n",
        "    and fuses them into a standard model for fast inference.\n",
        "\n",
        "    Args:\n",
        "        base_model_id (str): The Hugging Face ID of the base model.\n",
        "        adapter_file (str): Path to the trained LoRA adapter weights (.npz file).\n",
        "        lora_layers (int): The number of layers from the end of the model to apply LoRA to.\n",
        "        lora_rank (int): The rank used during LoRA training.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the fused MLX model and its tokenizer.\n",
        "    \"\"\"\n",
        "    print(\"--- Creating Fused Model ---\")\n",
        "\n",
        "    # 1. Check if the adapter file exists\n",
        "    if not os.path.exists(adapter_file):\n",
        "        raise FileNotFoundError(f\"Adapter file not found at '{adapter_file}'\")\n",
        "\n",
        "    # 2. Load the base model and tokenizer\n",
        "    print(f\"Loading base model: {base_model_id}\")\n",
        "    model, tokenizer = load(base_model_id)\n",
        "\n",
        "    # 3. Apply the LoRA architecture scaffolding to the model\n",
        "    print(\"Applying LoRA layer structure...\")\n",
        "    model.freeze()\n",
        "    for l in model.model.layers[len(model.model.layers) - lora_layers:]:\n",
        "        l.self_attn.q_proj = LoRALinear.from_linear(l.self_attn.q_proj, rank=lora_rank)\n",
        "        l.self_attn.v_proj = LoRALinear.from_linear(l.self_attn.v_proj, rank=lora_rank)\n",
        "        if hasattr(l, \"block_sparse_moe\"):\n",
        "            l.block_sparse_moe.gate = LoRALinear.from_linear(l.block_sparse_moe.gate, rank=lora_rank)\n",
        "\n",
        "    # 4. Load the trained adapter weights onto the LoRA layers\n",
        "    print(f\"Loading adapter weights from: {adapter_file}\")\n",
        "    model.load_weights(adapter_file, strict=False)\n",
        "\n",
        "    # DIAGNOSTIC STEP (as before)\n",
        "    print(\"\\n--- Running Diagnostic Check on LoRA Weights ---\")\n",
        "    try:\n",
        "        sample_lora_layer = model.model.layers[-1].self_attn.v_proj\n",
        "        if isinstance(sample_lora_layer, LoRALinear):\n",
        "            lora_b_norm = mx.linalg.norm(sample_lora_layer.lora_b).item()\n",
        "            print(f\"  > Norm of a sample lora_b matrix: {lora_b_norm:.4f}\")\n",
        "            if lora_b_norm == 0.0: print(\"\\n[!] WARNING: LoRA weights were likely not loaded correctly.\\n\")\n",
        "            else: print(\"\\n[✓] SUCCESS: LoRA weights appear to be loaded correctly.\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"[!] ERROR during diagnostic check: {e}\")\n",
        "\n",
        "    # 5. Fuse the LoRA weights back into the linear layers\n",
        "    print(\"Fusing LoRA layers into the base model...\")\n",
        "    for layer in model.model.layers:\n",
        "        if hasattr(layer.self_attn, \"q_proj\") and isinstance(layer.self_attn.q_proj, LoRALinear):\n",
        "            layer.self_attn.q_proj = layer.self_attn.q_proj.to_linear()\n",
        "        if hasattr(layer.self_attn, \"v_proj\") and isinstance(layer.self_attn.v_proj, LoRALinear):\n",
        "            layer.self_attn.v_proj = layer.self_attn.v_proj.to_linear()\n",
        "        if hasattr(layer, \"block_sparse_moe\") and isinstance(layer.block_sparse_moe.gate, LoRALinear):\n",
        "            layer.block_sparse_moe.gate = layer.block_sparse_moe.gate.to_linear()\n",
        "\n",
        "    # Ensure all changes are materialized\n",
        "    mx.eval(model.parameters())\n",
        "\n",
        "    print(\"\\n[✓] SUCCESS: Model fused and ready for inference.\\n\")\n",
        "\n",
        "    # DIAGNOSTIC STEP\n",
        "    print(\"\\n--- Running Diagnostic Check on LoRA Weights ---\")\n",
        "    try:\n",
        "        # Get one of the LoRA layers you know you tuned\n",
        "        sample_lora_layer = model.model.layers[-1].self_attn.v_proj \n",
        "        if isinstance(sample_lora_layer, LoRALinear):\n",
        "            lora_b_norm = mx.linalg.norm(sample_lora_layer.lora_b).item()\n",
        "            print(f\"  > Norm of a sample lora_b matrix: {lora_b_norm:.4f}\")\n",
        "            if lora_b_norm < 1e-6: # Check if it's effectively zero\n",
        "                print(\"\\n[!] CRITICAL: LoRA `b` weights are all zero. Training had no effect.\\n\")\n",
        "            else:\n",
        "                print(\"\\n[✓] SUCCESS: LoRA `b` weights have been trained.\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"[!] ERROR during diagnostic check: {e}\")\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def run_generation_with_adapter(base_model_id: str, adapter_file: str, sample: dict):\n",
        "    \"\"\"\n",
        "    Loads a base model, applies a LoRA adapter, and generates a response.\n",
        "\n",
        "    Args:\n",
        "        base_model_id: the identifier for the base model to load.\n",
        "        adapter_file (str): The path to the .npz file containing the adapter weights.\n",
        "        sample (dict): A data sample dictionary from the dataset, expected to have\n",
        "                       'past_guess_history' and 'secret' keys.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Generation with Fine-Tuned Adapter ---\")\n",
        "\n",
        "    # 1. Check if the adapter file exists\n",
        "    if not os.path.exists(adapter_file):\n",
        "        print(f\"Error: Adapter file not found at '{adapter_file}'.\")\n",
        "        print(\"Please run the training function first to generate the adapter file.\")\n",
        "        return\n",
        "\n",
        "    # 2. Load the base model and tokenizer\n",
        "    print(f\"Loading base model: {base_model_id}\")\n",
        "    # Note: This loads the model every time. For multiple runs, you might want\n",
        "    # to load the model once outside this function.\n",
        "    model, tokenizer = load(base_model_id)\n",
        "\n",
        "    # 3. Re-apply the LoRA architecture to the model\n",
        "    print(\"Applying LoRA layer structure to the model...\")\n",
        "    model.freeze()\n",
        "    for l in model.model.layers[len(model.model.layers) - LORA_LAYERS_TO_TUNE :]:\n",
        "        l.self_attn.q_proj = LoRALinear.from_linear(l.self_attn.q_proj, rank=LORA_RANK)\n",
        "        l.self_attn.v_proj = LoRALinear.from_linear(l.self_attn.v_proj, rank=LORA_RANK)\n",
        "        if hasattr(l, \"block_sparse_moe\"):\n",
        "            l.block_sparse_moe.gate = LoRALinear.from_linear(l.block_sparse_moe.gate, rank=LORA_RANK)\n",
        "\n",
        "    # 4. Load the adapter weights and apply them to the model\n",
        "    print(f\"Loading adapter weights from: {adapter_file}\")\n",
        "    weights = mx.load(adapter_file)\n",
        "    model.update(tree_unflatten(list(weights.items())))\n",
        "    mx.eval(model.parameters())\n",
        "\n",
        "    # 5. Manually fuse the LoRA layers\n",
        "    print(\"Fusing LoRA layers for faster inference...\")\n",
        "    for layer in model.model.layers:\n",
        "        if isinstance(layer.self_attn.q_proj, LoRALinear):\n",
        "            layer.self_attn.q_proj = layer.self_attn.q_proj.to_linear()\n",
        "        if isinstance(layer.self_attn.v_proj, LoRALinear):\n",
        "            layer.self_attn.v_proj = layer.self_attn.v_proj.to_linear()\n",
        "        if hasattr(layer, \"block_sparse_moe\") and isinstance(layer.block_sparse_moe.gate, LoRALinear):\n",
        "            layer.block_sparse_moe.gate = layer.block_sparse_moe.gate.to_linear()\n",
        "    mx.eval(model.parameters())\n",
        "\n",
        "    # 6. Prepare the prompt using the provided sample\n",
        "    user_prompt_text = format_prompt_from_dataset(sample)\n",
        "    full_prompt_text = tokenizer.apply_chat_template(\n",
        "        [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": user_prompt_text}],\n",
        "        add_generation_prompt=True, tokenize=False\n",
        "    )\n",
        "\n",
        "    # 7. Generate text\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"--- Generating with Fine-Tuned Model ---\")\n",
        "    print(f\"Secret Word (for validation): {sample.get('secret', 'N/A')}\")\n",
        "    print(f\"User Prompt:\\n{user_prompt_text}\")\n",
        "    print(\"\\nModel's Generated Response:\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    response = generate(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        prompt=full_prompt_text,\n",
        "        max_tokens=MAX_COMPLETION_LENGTH,\n",
        "        verbose=True,\n",
        "    )\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Creating Fused Model ---\n",
            "Loading base model: mlx-community/gemma-3-1b-it-qat-8bit\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 8 files: 100%|██████████| 8/8 [00:00<00:00, 24600.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying LoRA layer structure...\n",
            "Loading adapter weights from: /Users/charbelk/dev/wordle/wordle-rl/wordle_gemma3_grpo_adapters_step400_20250621-082256.npz\n",
            "\n",
            "--- Running Diagnostic Check on LoRA Weights ---\n",
            "  > Norm of a sample lora_b matrix: 0.0004\n",
            "\n",
            "[✓] SUCCESS: LoRA weights appear to be loaded correctly.\n",
            "\n",
            "Fusing LoRA layers into the base model...\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "Note: Skipping re-quantization for layer to avoid Metal kernel error.\n",
            "\n",
            "[✓] SUCCESS: Model fused and ready for inference.\n",
            "\n",
            "\n",
            "--- Running Diagnostic Check on LoRA Weights ---\n"
          ]
        }
      ],
      "source": [
        "# adapter = \"/Users/charbelk/dev/wordle/wordle-rl/wordle_gemma3_grpo_adapters.npz\"\n",
        "# adapter = \"/Users/charbelk/dev/wordle/wordle-rl/wordle_gemma3_grpo_adapters_final_step_20250620-194821.npz\"\n",
        "# adapter = \"/Users/charbelk/dev/wordle/wordle-rl/wordle_gemma3_grpo_adapters_step250_20250620-221337.npz\"\n",
        "adapter = \"/Users/charbelk/dev/wordle/wordle-rl/wordle_gemma3_grpo_adapters_step400_20250621-082256.npz\"\n",
        "model_id = \"mlx-community/gemma-3-1b-it-qat-8bit\"\n",
        "merged_model, merged_tokenizer = fuse_lora_model(\n",
        "    base_model_id=model_id,\n",
        "    adapter_file=adapter,\n",
        "    lora_layers=LORA_LAYERS_TO_TUNE,\n",
        "    lora_rank=LORA_RANK\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT_SIMPLIFIED = \"\"\"\n",
        "You are a Wordle AI. Your goal is to guess the secret 5-letter word.\n",
        "\n",
        "### Rules\n",
        "1.  **Analyze Clues:** `✓` = correct letter & spot. `-` = correct letter, wrong spot. `x` = wrong letter.\n",
        "2.  **Explain Your Logic:** First, write your reasoning inside `<think>` and `</think>` tags.\n",
        "3.  **Make Your Guess:** Then, provide your 5-letter guess inside `<guess>` and `</guess>` tags.\n",
        "4.  **Format is Mandatory:** You must always use the `<think>...</think><guess>...</guess>` structure.\n",
        "\n",
        "### Examples\n",
        "\n",
        "**Example 1: First Guess**\n",
        "<think>\n",
        "This is the first guess. A good starting word has common, unique letters. 'RAISE' tests A, I, E, R, S.\n",
        "</think>\n",
        "<guess>RAISE</guess>\n",
        "\n",
        "**Example 2: Mid-Game Guess**\n",
        "**Clues so far:**\n",
        "*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\n",
        "*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\n",
        "<think>\n",
        "Clues show the word starts with 'BR' and contains 'S'. T,O,M,A,V,E are wrong. 'BRISK' fits the `BR__S` or `BR_S_` pattern and uses no wrong letters.\n",
        "</think>\n",
        "<guess>BRISK</guess>\n",
        "\"\"\"\n",
        "\n",
        "def format_prompt_from_dataset(history_str):\n",
        "    \"\"\"\n",
        "    Formats the user prompt to match the markdown style in the system prompt's examples.\n",
        "\n",
        "    For the first turn, it provides a simple instruction. For subsequent turns,\n",
        "    it formats the history as a markdown list under a \"Clues so far\" heading.\n",
        "    \"\"\"    \n",
        "    try:\n",
        "        # Safely evaluate the string representation of the list\n",
        "        history_list = ast.literal_eval(history_str)\n",
        "    except (ValueError, SyntaxError):\n",
        "        history_list = []\n",
        "\n",
        "    # --- Case 1: First guess of the game ---\n",
        "    if not history_list:\n",
        "        return \"This is the first turn. Please provide your best starting word.\"\n",
        "\n",
        "    # --- Case 2: Mid-game with a history of previous guesses ---\n",
        "    prompt_parts = [\"**Clues so far:**\"]\n",
        "    for i, (guess, feedback) in enumerate(history_list):\n",
        "        # Format each line to match the markdown list in the system prompt, e.g.:\n",
        "        # \"*   Guess 1: RAISE → R(x) A(✓) I(x) S(-) E(x)\"\n",
        "        prompt_parts.append(f\"*   Guess {i+1}: {guess} → {feedback}\")\n",
        "    \n",
        "    # Join all parts into a single string separated by newlines\n",
        "    return \"\\n\".join(prompt_parts)\n",
        "\n",
        "import re\n",
        "def next_turn_gemma_2(\n",
        "        gemma_model,\n",
        "        gemma_tokenizer,\n",
        "    past_guesses: List[GuessWithFeedback],\n",
        "    secret_word: str,\n",
        "    number_guesses: int\n",
        "):\n",
        "\n",
        "    raw_message = format_prompt_from_dataset(past_guesses)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": SYSTEM_PROMPT_SIMPLIFIED\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": raw_message\n",
        "        }\n",
        "    ]\n",
        "    print(\"Messages: \", messages)\n",
        "    response = llm_generate_gemma(gemma_model, gemma_tokenizer, messages)\n",
        "    match = re.search(\n",
        "        r\"<guess>\\s*(.*?)\\s*</guess>\", response, re.DOTALL\n",
        "    )\n",
        "    if not match:\n",
        "        print(\"invalid guess\")\n",
        "        return\n",
        "    print(\"match: \", match)\n",
        "    guess = match.group(1).upper()\n",
        "    print('The LLM guessed: ', guess)\n",
        "    feedback = get_feedback(guess, secret_word)\n",
        "    print(\"Feedback: \", feedback)\n",
        "    past_guesses.append(feedback)\n",
        "    print(\"\\n\\n\")\n",
        "    print((\"-\" * 100) + \"\\n\")\n",
        "    for past_guess in past_guesses:\n",
        "        print(past_guess)\n",
        "\n",
        "    if guess == secret_word:\n",
        "        print(\"🎉 SUCCESS 🎉\")\n",
        "    elif len(past_guesses) >= number_guesses:\n",
        "        print(\"❌ better luck next time... ❌\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a Wordle AI. Your goal is to guess the secret 5-letter word.\\n\\n### Rules\\n1.  **Analyze Clues:** `✓` = correct letter & spot. `-` = correct letter, wrong spot. `x` = wrong letter.\\n2.  **Explain Your Logic:** First, write your reasoning inside `<think>` and `</think>` tags.\\n3.  **Make Your Guess:** Then, provide your 5-letter guess inside `<guess>` and `</guess>` tags.\\n4.  **Format is Mandatory:** You must always use the `<think>...</think><guess>...</guess>` structure.\\n\\n### Examples\\n\\n**Example 1: First Guess**\\n<think>\\nThis is the first guess. A good starting word has common, unique letters. 'RAISE' tests A, I, E, R, S.\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: Mid-Game Guess**\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n<think>\\nClues show the word starts with 'BR' and contains 'S'. T,O,M,A,V,E are wrong. 'BRISK' fits the `BR__S` or `BR_S_` pattern and uses no wrong letters.\\n</think>\\n<guess>BRISK</guess>\\n\"}, {'role': 'user', 'content': 'This is the first turn. Please provide your best starting word.'}]\n",
            "match:  <re.Match object; span=(273, 291), match='<guess>SAD</guess>'>\n",
            "The LLM guessed:  SAD\n",
            "Feedback:  ('Guess must be 5 letters', False)\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "('Guess must be 5 letters', False)\n",
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a Wordle AI. Your goal is to guess the secret 5-letter word.\\n\\n### Rules\\n1.  **Analyze Clues:** `✓` = correct letter & spot. `-` = correct letter, wrong spot. `x` = wrong letter.\\n2.  **Explain Your Logic:** First, write your reasoning inside `<think>` and `</think>` tags.\\n3.  **Make Your Guess:** Then, provide your 5-letter guess inside `<guess>` and `</guess>` tags.\\n4.  **Format is Mandatory:** You must always use the `<think>...</think><guess>...</guess>` structure.\\n\\n### Examples\\n\\n**Example 1: First Guess**\\n<think>\\nThis is the first guess. A good starting word has common, unique letters. 'RAISE' tests A, I, E, R, S.\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: Mid-Game Guess**\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n<think>\\nClues show the word starts with 'BR' and contains 'S'. T,O,M,A,V,E are wrong. 'BRISK' fits the `BR__S` or `BR_S_` pattern and uses no wrong letters.\\n</think>\\n<guess>BRISK</guess>\\n\"}, {'role': 'user', 'content': 'This is the first turn. Please provide your best starting word.'}]\n",
            "match:  <re.Match object; span=(273, 291), match='<guess>SAD</guess>'>\n",
            "The LLM guessed:  SAD\n",
            "Feedback:  ('Guess must be 5 letters', False)\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a Wordle AI. Your goal is to guess the secret 5-letter word.\\n\\n### Rules\\n1.  **Analyze Clues:** `✓` = correct letter & spot. `-` = correct letter, wrong spot. `x` = wrong letter.\\n2.  **Explain Your Logic:** First, write your reasoning inside `<think>` and `</think>` tags.\\n3.  **Make Your Guess:** Then, provide your 5-letter guess inside `<guess>` and `</guess>` tags.\\n4.  **Format is Mandatory:** You must always use the `<think>...</think><guess>...</guess>` structure.\\n\\n### Examples\\n\\n**Example 1: First Guess**\\n<think>\\nThis is the first guess. A good starting word has common, unique letters. 'RAISE' tests A, I, E, R, S.\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: Mid-Game Guess**\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n<think>\\nClues show the word starts with 'BR' and contains 'S'. T,O,M,A,V,E are wrong. 'BRISK' fits the `BR__S` or `BR_S_` pattern and uses no wrong letters.\\n</think>\\n<guess>BRISK</guess>\\n\"}, {'role': 'user', 'content': 'This is the first turn. Please provide your best starting word.'}]\n",
            "match:  <re.Match object; span=(273, 291), match='<guess>SAD</guess>'>\n",
            "The LLM guessed:  SAD\n",
            "Feedback:  ('Guess must be 5 letters', False)\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a Wordle AI. Your goal is to guess the secret 5-letter word.\\n\\n### Rules\\n1.  **Analyze Clues:** `✓` = correct letter & spot. `-` = correct letter, wrong spot. `x` = wrong letter.\\n2.  **Explain Your Logic:** First, write your reasoning inside `<think>` and `</think>` tags.\\n3.  **Make Your Guess:** Then, provide your 5-letter guess inside `<guess>` and `</guess>` tags.\\n4.  **Format is Mandatory:** You must always use the `<think>...</think><guess>...</guess>` structure.\\n\\n### Examples\\n\\n**Example 1: First Guess**\\n<think>\\nThis is the first guess. A good starting word has common, unique letters. 'RAISE' tests A, I, E, R, S.\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: Mid-Game Guess**\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n<think>\\nClues show the word starts with 'BR' and contains 'S'. T,O,M,A,V,E are wrong. 'BRISK' fits the `BR__S` or `BR_S_` pattern and uses no wrong letters.\\n</think>\\n<guess>BRISK</guess>\\n\"}, {'role': 'user', 'content': 'This is the first turn. Please provide your best starting word.'}]\n",
            "match:  <re.Match object; span=(273, 291), match='<guess>SAD</guess>'>\n",
            "The LLM guessed:  SAD\n",
            "Feedback:  ('Guess must be 5 letters', False)\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a Wordle AI. Your goal is to guess the secret 5-letter word.\\n\\n### Rules\\n1.  **Analyze Clues:** `✓` = correct letter & spot. `-` = correct letter, wrong spot. `x` = wrong letter.\\n2.  **Explain Your Logic:** First, write your reasoning inside `<think>` and `</think>` tags.\\n3.  **Make Your Guess:** Then, provide your 5-letter guess inside `<guess>` and `</guess>` tags.\\n4.  **Format is Mandatory:** You must always use the `<think>...</think><guess>...</guess>` structure.\\n\\n### Examples\\n\\n**Example 1: First Guess**\\n<think>\\nThis is the first guess. A good starting word has common, unique letters. 'RAISE' tests A, I, E, R, S.\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: Mid-Game Guess**\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n<think>\\nClues show the word starts with 'BR' and contains 'S'. T,O,M,A,V,E are wrong. 'BRISK' fits the `BR__S` or `BR_S_` pattern and uses no wrong letters.\\n</think>\\n<guess>BRISK</guess>\\n\"}, {'role': 'user', 'content': 'This is the first turn. Please provide your best starting word.'}]\n",
            "match:  <re.Match object; span=(273, 291), match='<guess>SAD</guess>'>\n",
            "The LLM guessed:  SAD\n",
            "Feedback:  ('Guess must be 5 letters', False)\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a Wordle AI. Your goal is to guess the secret 5-letter word.\\n\\n### Rules\\n1.  **Analyze Clues:** `✓` = correct letter & spot. `-` = correct letter, wrong spot. `x` = wrong letter.\\n2.  **Explain Your Logic:** First, write your reasoning inside `<think>` and `</think>` tags.\\n3.  **Make Your Guess:** Then, provide your 5-letter guess inside `<guess>` and `</guess>` tags.\\n4.  **Format is Mandatory:** You must always use the `<think>...</think><guess>...</guess>` structure.\\n\\n### Examples\\n\\n**Example 1: First Guess**\\n<think>\\nThis is the first guess. A good starting word has common, unique letters. 'RAISE' tests A, I, E, R, S.\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: Mid-Game Guess**\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n<think>\\nClues show the word starts with 'BR' and contains 'S'. T,O,M,A,V,E are wrong. 'BRISK' fits the `BR__S` or `BR_S_` pattern and uses no wrong letters.\\n</think>\\n<guess>BRISK</guess>\\n\"}, {'role': 'user', 'content': 'This is the first turn. Please provide your best starting word.'}]\n",
            "match:  <re.Match object; span=(273, 291), match='<guess>SAD</guess>'>\n",
            "The LLM guessed:  SAD\n",
            "Feedback:  ('Guess must be 5 letters', False)\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a Wordle AI. Your goal is to guess the secret 5-letter word.\\n\\n### Rules\\n1.  **Analyze Clues:** `✓` = correct letter & spot. `-` = correct letter, wrong spot. `x` = wrong letter.\\n2.  **Explain Your Logic:** First, write your reasoning inside `<think>` and `</think>` tags.\\n3.  **Make Your Guess:** Then, provide your 5-letter guess inside `<guess>` and `</guess>` tags.\\n4.  **Format is Mandatory:** You must always use the `<think>...</think><guess>...</guess>` structure.\\n\\n### Examples\\n\\n**Example 1: First Guess**\\n<think>\\nThis is the first guess. A good starting word has common, unique letters. 'RAISE' tests A, I, E, R, S.\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: Mid-Game Guess**\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n<think>\\nClues show the word starts with 'BR' and contains 'S'. T,O,M,A,V,E are wrong. 'BRISK' fits the `BR__S` or `BR_S_` pattern and uses no wrong letters.\\n</think>\\n<guess>BRISK</guess>\\n\"}, {'role': 'user', 'content': 'This is the first turn. Please provide your best starting word.'}]\n",
            "match:  <re.Match object; span=(273, 291), match='<guess>SAD</guess>'>\n",
            "The LLM guessed:  SAD\n",
            "Feedback:  ('Guess must be 5 letters', False)\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a Wordle AI. Your goal is to guess the secret 5-letter word.\\n\\n### Rules\\n1.  **Analyze Clues:** `✓` = correct letter & spot. `-` = correct letter, wrong spot. `x` = wrong letter.\\n2.  **Explain Your Logic:** First, write your reasoning inside `<think>` and `</think>` tags.\\n3.  **Make Your Guess:** Then, provide your 5-letter guess inside `<guess>` and `</guess>` tags.\\n4.  **Format is Mandatory:** You must always use the `<think>...</think><guess>...</guess>` structure.\\n\\n### Examples\\n\\n**Example 1: First Guess**\\n<think>\\nThis is the first guess. A good starting word has common, unique letters. 'RAISE' tests A, I, E, R, S.\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: Mid-Game Guess**\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n<think>\\nClues show the word starts with 'BR' and contains 'S'. T,O,M,A,V,E are wrong. 'BRISK' fits the `BR__S` or `BR_S_` pattern and uses no wrong letters.\\n</think>\\n<guess>BRISK</guess>\\n\"}, {'role': 'user', 'content': 'This is the first turn. Please provide your best starting word.'}]\n",
            "match:  <re.Match object; span=(273, 291), match='<guess>SAD</guess>'>\n",
            "The LLM guessed:  SAD\n",
            "Feedback:  ('Guess must be 5 letters', False)\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a Wordle AI. Your goal is to guess the secret 5-letter word.\\n\\n### Rules\\n1.  **Analyze Clues:** `✓` = correct letter & spot. `-` = correct letter, wrong spot. `x` = wrong letter.\\n2.  **Explain Your Logic:** First, write your reasoning inside `<think>` and `</think>` tags.\\n3.  **Make Your Guess:** Then, provide your 5-letter guess inside `<guess>` and `</guess>` tags.\\n4.  **Format is Mandatory:** You must always use the `<think>...</think><guess>...</guess>` structure.\\n\\n### Examples\\n\\n**Example 1: First Guess**\\n<think>\\nThis is the first guess. A good starting word has common, unique letters. 'RAISE' tests A, I, E, R, S.\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: Mid-Game Guess**\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n<think>\\nClues show the word starts with 'BR' and contains 'S'. T,O,M,A,V,E are wrong. 'BRISK' fits the `BR__S` or `BR_S_` pattern and uses no wrong letters.\\n</think>\\n<guess>BRISK</guess>\\n\"}, {'role': 'user', 'content': 'This is the first turn. Please provide your best starting word.'}]\n",
            "match:  <re.Match object; span=(273, 291), match='<guess>SAD</guess>'>\n",
            "The LLM guessed:  SAD\n",
            "Feedback:  ('Guess must be 5 letters', False)\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a Wordle AI. Your goal is to guess the secret 5-letter word.\\n\\n### Rules\\n1.  **Analyze Clues:** `✓` = correct letter & spot. `-` = correct letter, wrong spot. `x` = wrong letter.\\n2.  **Explain Your Logic:** First, write your reasoning inside `<think>` and `</think>` tags.\\n3.  **Make Your Guess:** Then, provide your 5-letter guess inside `<guess>` and `</guess>` tags.\\n4.  **Format is Mandatory:** You must always use the `<think>...</think><guess>...</guess>` structure.\\n\\n### Examples\\n\\n**Example 1: First Guess**\\n<think>\\nThis is the first guess. A good starting word has common, unique letters. 'RAISE' tests A, I, E, R, S.\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: Mid-Game Guess**\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n<think>\\nClues show the word starts with 'BR' and contains 'S'. T,O,M,A,V,E are wrong. 'BRISK' fits the `BR__S` or `BR_S_` pattern and uses no wrong letters.\\n</think>\\n<guess>BRISK</guess>\\n\"}, {'role': 'user', 'content': 'This is the first turn. Please provide your best starting word.'}]\n",
            "match:  <re.Match object; span=(273, 291), match='<guess>SAD</guess>'>\n",
            "The LLM guessed:  SAD\n",
            "Feedback:  ('Guess must be 5 letters', False)\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a Wordle AI. Your goal is to guess the secret 5-letter word.\\n\\n### Rules\\n1.  **Analyze Clues:** `✓` = correct letter & spot. `-` = correct letter, wrong spot. `x` = wrong letter.\\n2.  **Explain Your Logic:** First, write your reasoning inside `<think>` and `</think>` tags.\\n3.  **Make Your Guess:** Then, provide your 5-letter guess inside `<guess>` and `</guess>` tags.\\n4.  **Format is Mandatory:** You must always use the `<think>...</think><guess>...</guess>` structure.\\n\\n### Examples\\n\\n**Example 1: First Guess**\\n<think>\\nThis is the first guess. A good starting word has common, unique letters. 'RAISE' tests A, I, E, R, S.\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: Mid-Game Guess**\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n<think>\\nClues show the word starts with 'BR' and contains 'S'. T,O,M,A,V,E are wrong. 'BRISK' fits the `BR__S` or `BR_S_` pattern and uses no wrong letters.\\n</think>\\n<guess>BRISK</guess>\\n\"}, {'role': 'user', 'content': 'This is the first turn. Please provide your best starting word.'}]\n",
            "match:  <re.Match object; span=(273, 291), match='<guess>SAD</guess>'>\n",
            "The LLM guessed:  SAD\n",
            "Feedback:  ('Guess must be 5 letters', False)\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a Wordle AI. Your goal is to guess the secret 5-letter word.\\n\\n### Rules\\n1.  **Analyze Clues:** `✓` = correct letter & spot. `-` = correct letter, wrong spot. `x` = wrong letter.\\n2.  **Explain Your Logic:** First, write your reasoning inside `<think>` and `</think>` tags.\\n3.  **Make Your Guess:** Then, provide your 5-letter guess inside `<guess>` and `</guess>` tags.\\n4.  **Format is Mandatory:** You must always use the `<think>...</think><guess>...</guess>` structure.\\n\\n### Examples\\n\\n**Example 1: First Guess**\\n<think>\\nThis is the first guess. A good starting word has common, unique letters. 'RAISE' tests A, I, E, R, S.\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: Mid-Game Guess**\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n<think>\\nClues show the word starts with 'BR' and contains 'S'. T,O,M,A,V,E are wrong. 'BRISK' fits the `BR__S` or `BR_S_` pattern and uses no wrong letters.\\n</think>\\n<guess>BRISK</guess>\\n\"}, {'role': 'user', 'content': 'This is the first turn. Please provide your best starting word.'}]\n",
            "match:  <re.Match object; span=(273, 291), match='<guess>SAD</guess>'>\n",
            "The LLM guessed:  SAD\n",
            "Feedback:  ('Guess must be 5 letters', False)\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a Wordle AI. Your goal is to guess the secret 5-letter word.\\n\\n### Rules\\n1.  **Analyze Clues:** `✓` = correct letter & spot. `-` = correct letter, wrong spot. `x` = wrong letter.\\n2.  **Explain Your Logic:** First, write your reasoning inside `<think>` and `</think>` tags.\\n3.  **Make Your Guess:** Then, provide your 5-letter guess inside `<guess>` and `</guess>` tags.\\n4.  **Format is Mandatory:** You must always use the `<think>...</think><guess>...</guess>` structure.\\n\\n### Examples\\n\\n**Example 1: First Guess**\\n<think>\\nThis is the first guess. A good starting word has common, unique letters. 'RAISE' tests A, I, E, R, S.\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: Mid-Game Guess**\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n<think>\\nClues show the word starts with 'BR' and contains 'S'. T,O,M,A,V,E are wrong. 'BRISK' fits the `BR__S` or `BR_S_` pattern and uses no wrong letters.\\n</think>\\n<guess>BRISK</guess>\\n\"}, {'role': 'user', 'content': 'This is the first turn. Please provide your best starting word.'}]\n",
            "match:  <re.Match object; span=(273, 291), match='<guess>SAD</guess>'>\n",
            "The LLM guessed:  SAD\n",
            "Feedback:  ('Guess must be 5 letters', False)\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a Wordle AI. Your goal is to guess the secret 5-letter word.\\n\\n### Rules\\n1.  **Analyze Clues:** `✓` = correct letter & spot. `-` = correct letter, wrong spot. `x` = wrong letter.\\n2.  **Explain Your Logic:** First, write your reasoning inside `<think>` and `</think>` tags.\\n3.  **Make Your Guess:** Then, provide your 5-letter guess inside `<guess>` and `</guess>` tags.\\n4.  **Format is Mandatory:** You must always use the `<think>...</think><guess>...</guess>` structure.\\n\\n### Examples\\n\\n**Example 1: First Guess**\\n<think>\\nThis is the first guess. A good starting word has common, unique letters. 'RAISE' tests A, I, E, R, S.\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: Mid-Game Guess**\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n<think>\\nClues show the word starts with 'BR' and contains 'S'. T,O,M,A,V,E are wrong. 'BRISK' fits the `BR__S` or `BR_S_` pattern and uses no wrong letters.\\n</think>\\n<guess>BRISK</guess>\\n\"}, {'role': 'user', 'content': 'This is the first turn. Please provide your best starting word.'}]\n",
            "match:  <re.Match object; span=(273, 291), match='<guess>SAD</guess>'>\n",
            "The LLM guessed:  SAD\n",
            "Feedback:  ('Guess must be 5 letters', False)\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "Messages:  [{'role': 'system', 'content': \"\\nYou are a Wordle AI. Your goal is to guess the secret 5-letter word.\\n\\n### Rules\\n1.  **Analyze Clues:** `✓` = correct letter & spot. `-` = correct letter, wrong spot. `x` = wrong letter.\\n2.  **Explain Your Logic:** First, write your reasoning inside `<think>` and `</think>` tags.\\n3.  **Make Your Guess:** Then, provide your 5-letter guess inside `<guess>` and `</guess>` tags.\\n4.  **Format is Mandatory:** You must always use the `<think>...</think><guess>...</guess>` structure.\\n\\n### Examples\\n\\n**Example 1: First Guess**\\n<think>\\nThis is the first guess. A good starting word has common, unique letters. 'RAISE' tests A, I, E, R, S.\\n</think>\\n<guess>RAISE</guess>\\n\\n**Example 2: Mid-Game Guess**\\n**Clues so far:**\\n*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\\n*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\\n<think>\\nClues show the word starts with 'BR' and contains 'S'. T,O,M,A,V,E are wrong. 'BRISK' fits the `BR__S` or `BR_S_` pattern and uses no wrong letters.\\n</think>\\n<guess>BRISK</guess>\\n\"}, {'role': 'user', 'content': 'This is the first turn. Please provide your best starting word.'}]\n",
            "match:  <re.Match object; span=(273, 291), match='<guess>SAD</guess>'>\n",
            "The LLM guessed:  SAD\n",
            "Feedback:  ('Guess must be 5 letters', False)\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "('Guess must be 5 letters', False)\n",
            "❌ better luck next time... ❌\n"
          ]
        }
      ],
      "source": [
        "secret_word = \"AUDIO\"\n",
        "parst_guess = []\n",
        "n = 15\n",
        "for i in range(n):\n",
        "  next_turn_gemma_2(gemma_model=merged_model, gemma_tokenizer=merged_tokenizer, past_guesses=parst_guess, secret_word=secret_word, number_guesses=n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "==================================================\n",
            "STARTING NEW WORDLE GAME! Secret is 'APPLE'\n",
            "==================================================\n",
            "\n",
            "--- Sending Prompt to Model ---\n",
            "User Content:\n",
            "This is the first turn. Please provide your best starting word.\n",
            "\n",
            "--- Full Model Response ---\n",
            "<think>\n",
            "Known Letters: None.\n",
            "Eliminated Letters: None.\n",
            "Plan: I need to start with a common letter and a vowel. Let's try 'I'. I need to place 'I' in a new position, and it must not contain any of the eliminated letters. I will start with 'I'.\n",
            "</think>\n",
            "<guess>I</guess>\n",
            "-------------------------\n",
            "\n",
            "✅ Parsed Guess: 'I'\n",
            "Feedback: Guess must be 5 letters\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Sending Prompt to Model ---\n",
            "User Content:\n",
            "**Clues so far:**\n",
            "*   Guess 1: I → Guess must be 5 letters\n",
            "\n",
            "--- Full Model Response ---\n",
            "Okay, let's analyze the situation.\n",
            "\n",
            "**Think:**\n",
            "\n",
            "*   **Known Letters:** `✓` - B, R, S\n",
            "*   **Eliminated Letters:** `x` - T, O, M, A, V, E\n",
            "*   **Plan:**  We need a 5-letter word starting with 'BR'.  The letter 'S' must be placed in a new position.  We'll eliminate 'T', 'O', 'M', 'A', 'V', and 'E'.  Let's try 'BRIS' – this fits the criteria.\n",
            "\n",
            "**Guess:** BRIS\n",
            "\n",
            "```\n",
            "<guess>BRIS</guess>\n",
            "```\n",
            "\n",
            "Let's check the results:\n",
            "\n",
            "*   **Known Letters:** `✓` - B, R, S\n",
            "*   **Eliminated Letters:** `x` - T, O, M, A, V, E\n",
            "*   **Your Plan:**  The word starts with 'BR', and 'S' is placed in a new position.  We've eliminated 'T', 'O', 'M', 'A', 'V', and 'E'.  The word 'BRIS' fits the criteria.\n",
            "\n",
            "-------------------------\n",
            "\n",
            "✅ Parsed Guess: 'BRIS'\n",
            "Feedback: Guess must be 5 letters\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Sending Prompt to Model ---\n",
            "User Content:\n",
            "**Clues so far:**\n",
            "*   Guess 1: I → Guess must be 5 letters\n",
            "*   Guess 2: BRIS → Guess must be 5 letters\n",
            "\n",
            "--- Full Model Response ---\n",
            "Okay, let's analyze the situation.\n",
            "\n",
            "**Think:**\n",
            "\n",
            "*   **Known Letters:** `✓` - B, R, S\n",
            "*   **Eliminated Letters:** `x` - T, O, M, A, V, E\n",
            "*   **Plan:**  We need a 5-letter word starting with 'BR'.  The letter 'S' must be placed in a new position.  We'll eliminate 'T', 'O', 'M', 'A', 'V', and 'E'.  Let's try 'BRIS'. This is a strong contender, and it's a good starting point to test the remaining letters.\n",
            "\n",
            "**Guess:** BRIS\n",
            "\n",
            "```\n",
            "<guess>BRIS</guess>\n",
            "```\n",
            "\n",
            "\n",
            "-------------------------\n",
            "\n",
            "✅ Parsed Guess: 'BRIS'\n",
            "Feedback: Guess must be 5 letters\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Sending Prompt to Model ---\n",
            "User Content:\n",
            "**Clues so far:**\n",
            "*   Guess 1: I → Guess must be 5 letters\n",
            "*   Guess 2: BRIS → Guess must be 5 letters\n",
            "*   Guess 3: BRIS → Guess must be 5 letters\n",
            "\n",
            "--- Full Model Response ---\n",
            "Okay, let's analyze the situation.\n",
            "\n",
            "**Think Logically:**\n",
            "\n",
            "*   **Known Letters:** `✓` - B, R, S\n",
            "*   **Eliminated Letters:** `x` - T, O, M, A, V, E\n",
            "*   **Plan:**  We need a 5-letter word starting with 'BR'.  The letter 'S' must be placed in a new position.  We'll eliminate 'T', 'O', 'M', 'A', 'V', and 'E'.  Let's try to build a word with 'BR' and 'S' in a new position.  I'm going to start with 'BRIS'.\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Think Logically:**\n",
            "\n",
            "*   **Known Letters:** `✓` - B, R, S\n",
            "*   **Eliminated Letters:** `x` - T, O, M, A, V, E\n",
            "*   **Your Plan:**  The word must start with 'BR'. The letter 'S' must be placed in a new position (not position 1).  I'm going to start with 'BRIS'.\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Gu\n",
            "-------------------------\n",
            "\n",
            "❌ ERROR: Model did not return a parsable guess in <guess> tags.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Sending Prompt to Model ---\n",
            "User Content:\n",
            "**Clues so far:**\n",
            "*   Guess 1: I → Guess must be 5 letters\n",
            "*   Guess 2: BRIS → Guess must be 5 letters\n",
            "*   Guess 3: BRIS → Guess must be 5 letters\n",
            "\n",
            "--- Full Model Response ---\n",
            "Okay, let's analyze the situation.\n",
            "\n",
            "**Think Logically:**\n",
            "\n",
            "*   **Known Letters:** `✓` - B, R, S\n",
            "*   **Eliminated Letters:** `x` - T, O, M, A, V, E\n",
            "*   **Plan:**  We need a 5-letter word starting with 'BR'.  The letter 'S' must be placed in a new position.  We'll eliminate 'T', 'O', 'M', 'A', 'V', and 'E'.  Let's try to build a word with 'BR' and 'S' in a new position.  I'm going to start with 'BRIS'.\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Think Logically:**\n",
            "\n",
            "*   **Known Letters:** `✓` - B, R, S\n",
            "*   **Eliminated Letters:** `x` - T, O, M, A, V, E\n",
            "*   **Your Plan:**  The word must start with 'BR'. The letter 'S' must be placed in a new position (not position 1).  I'm going to start with 'BRIS'.\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Gu\n",
            "-------------------------\n",
            "\n",
            "❌ ERROR: Model did not return a parsable guess in <guess> tags.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Sending Prompt to Model ---\n",
            "User Content:\n",
            "**Clues so far:**\n",
            "*   Guess 1: I → Guess must be 5 letters\n",
            "*   Guess 2: BRIS → Guess must be 5 letters\n",
            "*   Guess 3: BRIS → Guess must be 5 letters\n",
            "\n",
            "--- Full Model Response ---\n",
            "Okay, let's analyze the situation.\n",
            "\n",
            "**Think Logically:**\n",
            "\n",
            "*   **Known Letters:** `✓` - B, R, S\n",
            "*   **Eliminated Letters:** `x` - T, O, M, A, V, E\n",
            "*   **Plan:**  We need a 5-letter word starting with 'BR'.  The letter 'S' must be placed in a new position.  We'll eliminate 'T', 'O', 'M', 'A', 'V', and 'E'.  Let's try to build a word with 'BR' and 'S' in a new position.  I'm going to start with 'BRIS'.\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Think Logically:**\n",
            "\n",
            "*   **Known Letters:** `✓` - B, R, S\n",
            "*   **Eliminated Letters:** `x` - T, O, M, A, V, E\n",
            "*   **Your Plan:**  The word must start with 'BR'. The letter 'S' must be placed in a new position (not position 1).  I'm going to start with 'BRIS'.\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Gu\n",
            "-------------------------\n",
            "\n",
            "❌ ERROR: Model did not return a parsable guess in <guess> tags.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Sending Prompt to Model ---\n",
            "User Content:\n",
            "**Clues so far:**\n",
            "*   Guess 1: I → Guess must be 5 letters\n",
            "*   Guess 2: BRIS → Guess must be 5 letters\n",
            "*   Guess 3: BRIS → Guess must be 5 letters\n",
            "\n",
            "--- Full Model Response ---\n",
            "Okay, let's analyze the situation.\n",
            "\n",
            "**Think Logically:**\n",
            "\n",
            "*   **Known Letters:** `✓` - B, R, S\n",
            "*   **Eliminated Letters:** `x` - T, O, M, A, V, E\n",
            "*   **Plan:**  We need a 5-letter word starting with 'BR'.  The letter 'S' must be placed in a new position.  We'll eliminate 'T', 'O', 'M', 'A', 'V', and 'E'.  Let's try to build a word with 'BR' and 'S' in a new position.  I'm going to start with 'BRIS'.\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Think Logically:**\n",
            "\n",
            "*   **Known Letters:** `✓` - B, R, S\n",
            "*   **Eliminated Letters:** `x` - T, O, M, A, V, E\n",
            "*   **Your Plan:**  The word must start with 'BR'. The letter 'S' must be placed in a new position (not position 1).  I'm going to start with 'BRIS'.\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Guessed Word:** BRIS\n",
            "\n",
            "**Gu\n",
            "-------------------------\n",
            "\n",
            "❌ ERROR: Model did not return a parsable guess in <guess> tags.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Sending Prompt to Model ---\n",
            "User Content:\n",
            "**Clues so far:**\n",
            "*   Guess 1: I → Guess must be 5 letters\n",
            "*   Guess 2: BRIS → Guess must be 5 letters\n",
            "*   Guess 3: BRIS → Guess must be 5 letters\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[87], line 235\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_game_over:\n\u001b[0;32m--> 235\u001b[0m     is_game_over \u001b[38;5;241m=\u001b[39m \u001b[43mnext_turn_gemma_2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgemma_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerged_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgemma_tokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerged_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_guesses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgame_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43msecret_word\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSECRET_WORD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumber_guesses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_GUESSES\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[87], line 170\u001b[0m, in \u001b[0;36mnext_turn_gemma_2\u001b[0;34m(gemma_model, gemma_tokenizer, past_guesses, secret_word, number_guesses)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Sending Prompt to Model ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser Content:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00muser_prompt_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 170\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm_generate_gemma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgemma_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgemma_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Full Model Response ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
            "Cell \u001b[0;32mIn[68], line 19\u001b[0m, in \u001b[0;36mllm_generate_gemma\u001b[0;34m(model, tokenizer, messages, max_tokens)\u001b[0m\n\u001b[1;32m     10\u001b[0m prompt \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m     11\u001b[0m     messages, \n\u001b[1;32m     12\u001b[0m     add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Pass the eos_token_id to the generate function\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/dev/wordle/wordle-rl/venv/lib/python3.10/site-packages/mlx_lm/generate.py:720\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(model, tokenizer, prompt, verbose, formatter, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    719\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 720\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m stream_generate(model, tokenizer, prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mtext, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/dev/wordle/wordle-rl/venv/lib/python3.10/site-packages/mlx_lm/generate.py:653\u001b[0m, in \u001b[0;36mstream_generate\u001b[0;34m(model, tokenizer, prompt, draft_model, **kwargs)\u001b[0m\n\u001b[1;32m    651\u001b[0m detokenizer\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m    652\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n, (token, logprobs, from_draft) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(token_generator):\n\u001b[1;32m    654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    655\u001b[0m         prompt_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m tic\n",
            "File \u001b[0;32m~/dev/wordle/wordle-rl/venv/lib/python3.10/site-packages/mlx_lm/generate.py:642\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    640\u001b[0m     token_generator \u001b[38;5;241m=\u001b[39m generate_step(prompt, model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# from_draft always false for non-speculative generation\u001b[39;00m\n\u001b[0;32m--> 642\u001b[0m     token_generator \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    643\u001b[0m         (token, logprobs, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m token, logprobs \u001b[38;5;129;01min\u001b[39;00m token_generator\n\u001b[1;32m    644\u001b[0m     )\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    646\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_kv_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "File \u001b[0;32m~/dev/wordle/wordle-rl/venv/lib/python3.10/site-packages/mlx_lm/generate.py:409\u001b[0m, in \u001b[0;36mgenerate_step\u001b[0;34m(prompt, model, max_tokens, sampler, logits_processors, max_kv_size, prompt_cache, prefill_step_size, kv_bits, kv_group_size, quantized_kv_start, prompt_progress_callback, input_embeddings)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m!=\u001b[39m max_tokens:\n\u001b[1;32m    408\u001b[0m     next_y, next_logprobs \u001b[38;5;241m=\u001b[39m _step(y)\n\u001b[0;32m--> 409\u001b[0m     \u001b[43mmx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masync_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_logprobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    411\u001b[0m     mx\u001b[38;5;241m.\u001b[39meval(y)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import mlx.core as mx\n",
        "import mlx.nn as nn\n",
        "from mlx_lm import load, generate\n",
        "import re\n",
        "import ast\n",
        "from typing import List, Tuple\n",
        "\n",
        "# Define a type hint for clarity\n",
        "GuessWithFeedback = Tuple[str, str]\n",
        "\n",
        "SYSTEM_PROMPT_ENHANCED = \"\"\"\n",
        "You are a Wordle AI. Your goal is to guess the secret 5-letter word.\n",
        "\n",
        "### Rules\n",
        "1.  **Analyze Clues:** `✓` = correct letter & spot. `-` = correct letter, wrong spot. `x` = wrong letter.\n",
        "2.  **Explain Your Logic:** First, write your reasoning inside `<think>` and `</think>` tags.\n",
        "3.  **Make Your Guess:** Then, provide your 5-letter guess inside `<guess>` and `</guess>` tags.\n",
        "4.  **Format is Mandatory:** You must always use the `<think>...</think><guess>...</guess>` structure.\n",
        "5.  **Avoid Wrong Letters:** You MUST NOT use any letter marked with `(x)` in your next guess.\n",
        "6.  **Move Misplaced Letters:** If a letter is marked with `(-)`, you MUST place it in a new position.\n",
        "7.  **Be Creative:** Never guess the same word twice. Your goal is to test new letters and solve the puzzle.\n",
        "\n",
        "### Examples\n",
        "\n",
        "**Example 1: First Guess**\n",
        "<think>\n",
        "This is the first guess. A good starting word has common, unique letters. 'RAISE' tests A, I, E, R, S.\n",
        "</think>\n",
        "<guess>RAISE</guess>\n",
        "\n",
        "**Example 2: Mid-Game Guess**\n",
        "**Clues so far:**\n",
        "*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\n",
        "*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\n",
        "<think>\n",
        "Clues show the word starts with 'BR' and contains 'S'. I must avoid the eliminated letters T,O,M,A,V,E. I also must move 'S' to a new spot. 'BRISK' fits the `BR__S` or `BR_S_` pattern and follows all rules.\n",
        "</think>\n",
        "<guess>BRISK</guess>\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_PROMPT_STRATEGIC = \"\"\"\n",
        "You are a strategic Wordle AI. Your primary goal is to use logic to eliminate possibilities and efficiently find the secret 5-letter word.\n",
        "\n",
        "### Core Directives\n",
        "\n",
        "1.  **THINK LOGICALLY:** Inside the `<think>` block, you MUST explicitly list:\n",
        "    *   **Known Letters:** All letters confirmed to be in the word (`✓` or `-`).\n",
        "    *   **Eliminated Letters:** All letters confirmed to be NOT in the word (`x`).\n",
        "    *   **Your Plan:** How you will use this information for your next guess.\n",
        "\n",
        "2.  **GUESS STRATEGICALLY:** Inside the `<guess>` block, your 5-letter guess must follow these hard rules:\n",
        "    *   It MUST NOT contain any of your listed \"Eliminated Letters\".\n",
        "    *   It MUST attempt to place misplaced (`-`) letters in a new position.\n",
        "    *   It MUST NOT be a word you have guessed before.\n",
        "\n",
        "3.  **FORMAT IS MANDATORY:** The `<think>...</think><guess>...</guess>` structure is required.\n",
        "\n",
        "### Examples\n",
        "\n",
        "**Example 1: First Guess**\n",
        "<think>\n",
        "Known Letters: None.\n",
        "Eliminated Letters: None.\n",
        "Plan: Start with a word with common and unique letters to gather maximum information. 'RAISE' is an excellent choice.\n",
        "</think>\n",
        "<guess>RAISE</guess>\n",
        "\n",
        "**Example 2: Mid-Game Guess**\n",
        "**Clues so far:**\n",
        "*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\n",
        "*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\n",
        "<think>\n",
        "Known Letters: B, R, S.\n",
        "Eliminated Letters: T, O, M, A, V, E.\n",
        "Plan: The word must start with 'BR'. The letter 'S' must be in a new position (not position 1). My guess must not contain any eliminated letters. 'BRISK' satisfies all these conditions and tests a new vowel 'I' and consonant 'K'.\n",
        "</think>\n",
        "<guess>BRISK</guess>\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_PROMPT_FINAL = \"\"\"\n",
        "You are a strategic Wordle AI. Your primary goal is to use logic to eliminate possibilities and find the secret 5-letter word.\n",
        "\n",
        "### Core Directives\n",
        "\n",
        "1.  **THINK LOGICALLY:** Inside the `<think>` block, you MUST explicitly list:\n",
        "    *   Known Letters: All letters confirmed to be in the word (`✓` or `-`).\n",
        "    *   Eliminated Letters: All letters confirmed to be NOT in the word (`x`).\n",
        "    *   Your Plan: How you will use this information for your next guess.\n",
        "\n",
        "2.  **GUESS STRATEGICALLY:** Inside the `<guess>` block, your 5-letter guess must follow these hard rules:\n",
        "    *   It MUST NOT contain any of your listed \"Eliminated Letters\".\n",
        "    *   It MUST attempt to place misplaced (`-`) letters in a new position.\n",
        "\n",
        "3.  **DO NOT REPEAT:** Never repeat a guess you have already made.\n",
        "\n",
        "4.  **PRIORITIZE SOLVING (Most Important!):** Once you have enough clues to be confident in the final answer, your only goal is to guess that word. Do NOT make a \"strategic\" guess to test more letters if you can win.\n",
        "\n",
        "### Examples\n",
        "\n",
        "**Example 1: First Guess (Exploration)**\n",
        "<think>\n",
        "Known Letters: None.\n",
        "Eliminated Letters: None.\n",
        "Plan: Start with a word with common and unique letters to gather maximum information. 'RAISE' is an excellent choice.\n",
        "</think>\n",
        "<guess>RAISE</guess>\n",
        "\n",
        "**Example 2: Mid-Game Guess (Strategic Elimination)**\n",
        "**Clues so far:**\n",
        "*   Guess 1: STORM → S(-), T(x), O(x), R(-), M(x)\n",
        "*   Guess 2: BRAVE → B(✓), R(✓), A(x), V(x), E(x)\n",
        "<think>\n",
        "Known Letters: B, R, S.\n",
        "Eliminated Letters: T, O, M, A, V, E.\n",
        "Plan: The word must start with 'BR' and contain 'S' elsewhere. It cannot contain any eliminated letters. 'BRISK' satisfies all conditions and tests new letters.\n",
        "</think>\n",
        "<guess>BRISK</guess>\n",
        "\n",
        "**Example 3: The Final Answer (Exploitation)**\n",
        "**Clues so far:**\n",
        "*   Guess 1: PILLS → P(✓), I(x), L(✓), L(✓), S(x)\n",
        "*   Guess 2: PLANK → P(✓), L(✓), A(✓), N(x), K(x)\n",
        "<think>\n",
        "Known Letters: P, L, A.\n",
        "Eliminated Letters: I, S, N, K.\n",
        "Plan: The word has the pattern P L A _ _. The only remaining letters I've seen are P and L, but they are already placed. The word must be 'PLAZA' or 'PLAID'. Given the clues, 'PLAID' doesn't work because I haven't seen 'D'. Let's try to make a word out of the letters I have. Oh wait, I see 'Z' has not been tried. I'm confident the word is PLAZA. My priority is to solve.\n",
        "</think>\n",
        "<guess>PLAZA</guess>\n",
        "\"\"\"\n",
        "\n",
        "# 1. FIXED: This function now correctly accepts a List, not a string.\n",
        "def format_prompt_from_dataset(history_list: List[GuessWithFeedback]) -> str:\n",
        "    \"\"\"\n",
        "    Formats the user prompt from a list of past guesses.\n",
        "    \"\"\"\n",
        "    # No longer need ast.literal_eval, because we receive a list directly.\n",
        "    # The try/except block is also no longer necessary.\n",
        "\n",
        "    # --- Case 1: First guess of the game ---\n",
        "    if not history_list:\n",
        "        return \"This is the first turn. Please provide your best starting word.\"\n",
        "\n",
        "    # --- Case 2: Mid-game with a history of previous guesses ---\n",
        "    prompt_parts = [\"**Clues so far:**\"]\n",
        "    for i, (guess, feedback) in enumerate(history_list):\n",
        "        prompt_parts.append(f\"*   Guess {i+1}: {guess} → {feedback}\")\n",
        "    \n",
        "    return \"\\n\".join(prompt_parts)\n",
        "\n",
        "\n",
        "# 3. YOUR FUNCTION (largely unchanged, but now it works with the fixed functions)\n",
        "def next_turn_gemma_2(\n",
        "    gemma_model,\n",
        "    gemma_tokenizer,\n",
        "    past_guesses: List[GuessWithFeedback],\n",
        "    secret_word: str,\n",
        "    number_guesses: int\n",
        "):\n",
        "    # This call now works because the function expects a List\n",
        "    user_prompt_content = format_prompt_from_dataset(past_guesses)\n",
        "    \n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT_STRATEGIC},\n",
        "        {\"role\": \"user\", \"content\": user_prompt_content}\n",
        "    ]\n",
        "\n",
        "    print(\"--- Sending Prompt to Model ---\")\n",
        "    print(f\"User Content:\\n{user_prompt_content}\\n\")\n",
        "\n",
        "    response = llm_generate_gemma(gemma_model, gemma_tokenizer, messages)\n",
        "    \n",
        "    print(\"--- Full Model Response ---\")\n",
        "    print(response)\n",
        "    print(\"-------------------------\\n\")\n",
        "\n",
        "    # The regex part is good, it handles whitespace correctly.\n",
        "    match = re.search(r\"<guess>\\s*(.*?)\\s*</guess>\", response, re.DOTALL | re.IGNORECASE)\n",
        "    \n",
        "    if not match:\n",
        "        print(\"❌ ERROR: Model did not return a parsable guess in <guess> tags.\")\n",
        "        return False # Return a flag to indicate failure\n",
        "\n",
        "    guess = match.group(1).strip().upper()\n",
        "    print(f\"✅ Parsed Guess: '{guess}'\")\n",
        "    \n",
        "    # You would have your own get_feedback function here. This is a placeholder.\n",
        "    feedback_str, is_correct = get_feedback(guess, secret_word)\n",
        "    \n",
        "    print(f\"Feedback: {feedback_str}\")\n",
        "    \n",
        "    # Append the new guess and feedback to the history list\n",
        "    past_guesses.append((guess, feedback_str))\n",
        "\n",
        "    if is_correct:\n",
        "        print(\"\\n🎉 SUCCESS! The AI guessed the word correctly. 🎉\")\n",
        "        return True # Return a flag to indicate success\n",
        "    elif len(past_guesses) >= number_guesses:\n",
        "        print(f\"\\n❌ FAILURE! The AI ran out of guesses. The word was '{secret_word}'. ❌\")\n",
        "        return True # Return a flag to indicate the game is over\n",
        "\n",
        "    return False # Game is not over yet\n",
        "\n",
        "# --- Placeholder `get_feedback` for a complete example ---\n",
        "def get_feedback(guess: str, secret: str) -> Tuple[str, bool]:\n",
        "    # A simplified feedback generator for demonstration\n",
        "    if len(guess) != 5:\n",
        "        return \"Guess must be 5 letters\", False\n",
        "    \n",
        "    feedback_parts = []\n",
        "    for i, char in enumerate(guess):\n",
        "        if char == secret[i]:\n",
        "            feedback_parts.append(f\"{char}(✓)\")\n",
        "        elif char in secret:\n",
        "            feedback_parts.append(f\"{char}(-)\")\n",
        "        else:\n",
        "            feedback_parts.append(f\"{char}(x)\")\n",
        "    \n",
        "    is_correct = (guess == secret)\n",
        "    return \" \".join(feedback_parts), is_correct\n",
        "\n",
        "# --- Example of how to run the fixed code ---\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # --- Start a New Game ---\n",
        "    SECRET_WORD = \"APPLE\"\n",
        "    MAX_GUESSES = 8\n",
        "    game_history: List[GuessWithFeedback] = []\n",
        "    is_game_over = False\n",
        "\n",
        "    print(\"\\n\\n\" + \"=\"*50)\n",
        "    print(f\"STARTING NEW WORDLE GAME! Secret is '{SECRET_WORD}'\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    while not is_game_over:\n",
        "        is_game_over = next_turn_gemma_2(\n",
        "            gemma_model=merged_model,\n",
        "            gemma_tokenizer=merged_tokenizer,\n",
        "            past_guesses=game_history,\n",
        "            secret_word=SECRET_WORD,\n",
        "            number_guesses=MAX_GUESSES\n",
        "        )\n",
        "        print(\"\\n\" + (\"-\" * 50) + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Generation with Fine-Tuned Adapter ---\n",
            "Loading base model: mlx-community/gemma-3-1b-it-qat-8bit\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 8 files: 100%|██████████| 8/8 [00:00<00:00, 88768.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying LoRA layer structure to the model...\n",
            "Loading adapter weights from: /Users/charbelk/dev/wordle/wordle-rl/wordle_gemma3_grpo_adapters.npz\n",
            "\n",
            "--- Running Diagnostic Check on LoRA Weights ---\n",
            "  > Norm of a sample lora_b matrix: 0.0045\n",
            "\n",
            "[✓] SUCCESS: LoRA weights appear to be loaded correctly.\n",
            "\n",
            "Fusing LoRA layers for faster inference...\n",
            "\n",
            "==================================================\n",
            "--- Generating with Fine-Tuned Model ---\n",
            "Secret Word (for validation): ABHOR\n",
            "User Prompt:\n",
            "This is the first turn. Please provide your best starting word.\n",
            "\n",
            "Model's Generated Response:\n",
            "==================================================\n",
            "==========\n",
            "<think>\n",
            "Okay, let's start with a word that includes 'R' and 'S' and avoids common letters. I'll focus on a word with a simple vowel structure. Considering the feedback, I'll try to build a word that fits the pattern `BR__S`. Let's try 'BRIS'.\n",
            "</think>\n",
            "<guess>BRIS</guess>\n",
            "==========\n",
            "Prompt: 539 tokens, 321.344 tokens-per-sec\n",
            "Generation: 83 tokens, 63.153 tokens-per-sec\n",
            "Peak memory: 4.166 GB\n",
            "<think>\n",
            "Okay, let's start with a word that includes 'R' and 'S' and avoids common letters. I'll focus on a word with a simple vowel structure. Considering the feedback, I'll try to build a word that fits the pattern `BR__S`. Let's try 'BRIS'.\n",
            "</think>\n",
            "<guess>BRIS</guess>\n"
          ]
        }
      ],
      "source": [
        "adapter = \"/Users/charbelk/dev/wordle/wordle-rl/wordle_gemma3_grpo_adapters.npz\"\n",
        "model_id = \"mlx-community/gemma-3-1b-it-qat-8bit\"\n",
        "response = run_generation_with_adapter(model_id, adapter, test_dataset[0])\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import shutil\n",
        "from huggingface_hub import HfApi, create_repo\n",
        "\n",
        "def upload_fused_model_to_hub(hub_repo_id: str, adapter_file: str):\n",
        "    \"\"\"\n",
        "    Fuses a LoRA adapter with its base model and uploads the result to the\n",
        "    Hugging Face Hub.\n",
        "\n",
        "    Args:\n",
        "        hub_repo_id (str): The desired repository ID on the Hub (e.g., \"YourUsername/YourModelName\").\n",
        "        adapter_file (str): Path to the local .npz adapter file.\n",
        "    \"\"\"\n",
        "    print(f\"--- Starting upload process for repo: {hub_repo_id} ---\")\n",
        "\n",
        "    # Use a temporary local directory to stage files for upload\n",
        "    temp_upload_dir = \"hf_upload_temp\"\n",
        "    if os.path.exists(temp_upload_dir):\n",
        "        shutil.rmtree(temp_upload_dir)\n",
        "    os.makedirs(temp_upload_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        # === 1. Load base model and apply adapter (same logic as before) ===\n",
        "        print(f\"Loading base model: {MODEL_NAME}\")\n",
        "        model, tokenizer = load(MODEL_NAME)\n",
        "\n",
        "        print(\"Applying LoRA layer structure...\")\n",
        "        model.freeze()\n",
        "        for l in model.model.layers[len(model.model.layers) - LORA_LAYERS_TO_TUNE :]:\n",
        "            l.self_attn.q_proj = LoRALinear.from_linear(l.self_attn.q_proj, rank=LORA_RANK)\n",
        "            l.self_attn.v_proj = LoRALinear.from_linear(l.self_attn.v_proj, rank=LORA_RANK)\n",
        "            if hasattr(l, \"block_sparse_moe\"):\n",
        "                l.block_sparse_moe.gate = LoRALinear.from_linear(l.block_sparse_moe.gate, rank=LORA_RANK)\n",
        "\n",
        "        print(f\"Loading adapter weights from: {adapter_file}\")\n",
        "        weights = mx.load(adapter_file)\n",
        "        model.update(tree_unflatten(list(weights.items())))\n",
        "\n",
        "        print(\"Fusing LoRA layers...\")\n",
        "        for layer in model.model.layers:\n",
        "            if isinstance(layer.self_attn.q_proj, LoRALinear):\n",
        "                layer.self_attn.q_proj = layer.self_attn.q_proj.to_linear()\n",
        "            if isinstance(layer.self_attn.v_proj, LoRALinear):\n",
        "                layer.self_attn.v_proj = layer.self_attn.v_proj.to_linear()\n",
        "            if hasattr(layer, \"block_sparse_moe\") and isinstance(layer.block_sparse_moe.gate, LoRALinear):\n",
        "                layer.block_sparse_moe.gate = layer.block_sparse_moe.gate.to_linear()\n",
        "        mx.eval(model.parameters())\n",
        "        print(\"Model fusion complete.\")\n",
        "\n",
        "        # === 2. Save all necessary files to the temporary directory ===\n",
        "        print(f\"Saving fused model and tokenizer to '{temp_upload_dir}'...\")\n",
        "\n",
        "        # Save model weights as safetensors\n",
        "        weights_path = os.path.join(temp_upload_dir, \"weights.safetensors\")\n",
        "        mx.save_safetensors(weights_path, dict(tree_flatten(model.parameters())))\n",
        "\n",
        "        # Save tokenizer\n",
        "        tokenizer.save_pretrained(temp_upload_dir)\n",
        "\n",
        "        # Save model configuration\n",
        "        config_path = os.path.join(temp_upload_dir, \"config.json\")\n",
        "        with open(config_path, \"w\") as f:\n",
        "            # The config is often in model.model.config\n",
        "            if hasattr(model, \"model\") and hasattr(model.model, \"config\"):\n",
        "                json.dump(model.model.config, f, indent=4)\n",
        "            # Or at the top level\n",
        "            elif hasattr(model, \"config\"):\n",
        "                 json.dump(model.config, f, indent=4)\n",
        "            else:\n",
        "                print(\"[!] Warning: Could not find model config to save.\")\n",
        "\n",
        "        print(\"All files saved locally.\")\n",
        "\n",
        "        # === 3. Upload to Hugging Face Hub ===\n",
        "        print(f\"Uploading files to Hugging Face Hub repository: {hub_repo_id}\")\n",
        "        api = HfApi()\n",
        "        create_repo(hub_repo_id, exist_ok=True)\n",
        "        api.upload_folder(\n",
        "            folder_path=temp_upload_dir,\n",
        "            repo_id=hub_repo_id,\n",
        "            repo_type=\"model\",\n",
        "        )\n",
        "        print(f\"[✓] Successfully uploaded model to: https://huggingface.co/{hub_repo_id}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[X] An error occurred during the upload process: {e}\")\n",
        "    finally:\n",
        "        # Clean up the temporary directory\n",
        "        if os.path.exists(temp_upload_dir):\n",
        "            shutil.rmtree(temp_upload_dir)\n",
        "            print(f\"Cleaned up temporary directory: {temp_upload_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "HUB_REPO_ID = \"charbull/ppo-gemma-lora-v1\"\n",
        "adapter_to_upload = ADAPTER_FILE\n",
        "upload_fused_model_to_hub(hub_repo_id=HUB_REPO_ID, adapter_file=adapter_to_upload)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "\n",
        "def upload_adapter_only(hub_repo_id: str, adapter_file: str):\n",
        "    \"\"\"\n",
        "    Uploads only the LoRA adapter file and a helpful README to the Hub.\n",
        "    \"\"\"\n",
        "    print(f\"--- Uploading adapter to {hub_repo_id} ---\")\n",
        "    \n",
        "    # Create a helpful README.md for the adapter card\n",
        "    readme_content = f\"\"\"\n",
        "    ---\n",
        "    license: mit\n",
        "    base_model: {MODEL_NAME}\n",
        "    ---\n",
        "\n",
        "    # LoRA Adapter for `{MODEL_NAME}`\n",
        "\n",
        "    This repository contains a LoRA adapter trained to play Wordle.\n",
        "\n",
        "    ## Usage\n",
        "\n",
        "    This adapter should be used with the base model `{MODEL_NAME}`.\n",
        "    You will need to define the `LoRALinear` class provided in the original MLX examples.\n",
        "\n",
        "    ```python\n",
        "    import mlx.core as mx\n",
        "    from mlx_lm import load\n",
        "    from huggingface_hub import hf_hub_download\n",
        "\n",
        "    # You must have the LoRALinear class defined in your script.\n",
        "    # (Insert the LoRALinear class definition here)\n",
        "\n",
        "    # Load the base model\n",
        "    model, tokenizer = load(\"{MODEL_NAME}\")\n",
        "\n",
        "    # Apply the LoRA layers to the model\n",
        "    # This must be done *before* loading the adapter weights\n",
        "    model.freeze()\n",
        "    for l in model.model.layers[-{LORA_LAYERS_TO_TUNE}:]:\n",
        "        l.self_attn.q_proj = LoRALinear.from_linear(l.self_attn.q_proj, rank={LORA_RANK})\n",
        "        l.self_attn.v_proj = LoRALinear.from_linear(l.self_attn.v_proj, rank={LORA_RANK})\n",
        "        if hasattr(l, \"block_sparse_moe\"):\n",
        "            l.block_sparse_moe.gate = LoRALinear.from_linear(l.block_sparse_moe.gate, rank={LORA_RANK})\n",
        "\n",
        "    # Download and load the adapter weights\n",
        "    adapter_path = hf_hub_download(repo_id=\"{hub_repo_id}\", filename=\"{os.path.basename(adapter_file)}\")\n",
        "    weights = mx.load(adapter_path)\n",
        "    model.update(weights) # Note: mlx-lm > 0.1.1 has model.update_from_safetensors\n",
        "\n",
        "    print(\"Adapter loaded successfully.\")\n",
        "    # The model is now ready for generation or fusion.\n",
        "    \"\"\"\n",
        "    \n",
        "    api = HfApi()\n",
        "    print(f\"Creating repository {hub_repo_id} on the Hub...\")\n",
        "    api.create_repo(hub_repo_id, exist_ok=True)\n",
        "\n",
        "    print(f\"Uploading adapter file: {adapter_file}\")\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=adapter_file,\n",
        "        path_in_repo=os.path.basename(adapter_file),\n",
        "        repo_id=hub_repo_id,\n",
        "    )\n",
        "\n",
        "    print(\"Uploading README.md\")\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=readme_content.encode(),\n",
        "        path_in_repo=\"README.md\",\n",
        "        repo_id=hub_repo_id,\n",
        "    )\n",
        "    print(f\"[✓] Successfully uploaded adapter to: https://huggingface.co/{hub_repo_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "HUB_REPO_ID = \"charbull/ppo-gemma-lora-only-v1\"\n",
        "adapter_to_upload = ADAPTER_FILE\n",
        "upload_adapter_only(hub_repo_id=HUB_REPO_ID, adapter_file=adapter_to_upload)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "attachments": {
        "training_curves_480_20250625-023809.png": {
          "image/png": "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"
        }
      },
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Information Theory to the rescue\n",
        "\n",
        "After reading many blogs about RL, we know that there is a step of SFT/LoRA that comes before the RL GRPO training that allow to finetune the model on a given format <think></think> and  <guess></guess>.\n",
        "\n",
        "However, since now I can run locally on M1, I ran it for 15 hours to see what happens\n",
        "![training_curves_480_20250625-023809.png](attachment:training_curves_480_20250625-023809.png)\n",
        "\n",
        "It trained for around 480 steps and then my process got an error so it stopped. I should have done the plotting a bit nice here.\n",
        "\n",
        "\n",
        "Why did I do that? I wanted to learn RL and implemented things from scratch and see how things behave before following a working receipe\n",
        "\n",
        "### Do we need RL?\n",
        "I was looking online if there is a brut force solution to solve wordle without any fintenuning, just with software 1.0 no AI just good old plain algorithm.\n",
        "\n",
        "I landed on [https://youtu.be/v68zYyaEmEA?si=D2HJCcVa-b6uhD1i](3blue1brow-wordle) video where he walks us through how he used information theory in order to guess the word based on the feedback.\n",
        "\n",
        "The main idea is which word should the algorith provides in order to maximize the amount of information it will recieve from the feedback.\n",
        "Please go watch the video if we want to know more details about it. Lets look at this example where the secret word is \"STARS\":\n",
        "\n",
        "```\n",
        "Played 'TARES', got feedback '---x✓'\n",
        "\n",
        "Possible words remaining: 7 -> ['STARS', 'DRATS', 'BRATS', 'FRATS', 'PRATS', 'ARTIS', 'ROTAS']\n",
        "Played 'FIORD', got feedback 'xxx✓x'\n",
        "\n",
        "Possible words remaining: 1 -> ['STARS']\n",
        "\n",
        "🎉 🎉 🎉 Congratulations! 'STARS' is the correct answer, after 3 plays!\n",
        "```\n",
        "The algorithmic version starts with the following assumptions:\n",
        " 1) we have a finite list of words (words_list)\n",
        " 2) we have a finite list of allowed guesses where (allowed_guesses <= words_list)\n",
        "\n",
        "After each guess, we get feedback on the letter positions which allows us to keep only the possible guesses\n",
        "First, the word 'TARES' provides us with the maximum amount of information ~6.3 bits\n",
        "from that feedback, there are now 7 possible words remaining.\n",
        "\n",
        "In order to guess which one of the 7, the idea behind the algorithm, is to propose a word in the allowed guesses that would provides a maximum information gain. This word 'FIORD' since we are left with only one remaining word.\n",
        "\n",
        "After playing with wordle_no_rl.ipynb you might be wondering like me why do we need RL in the first place if we can solve a problem with information theory.\n",
        "\n",
        "\n",
        "\n",
        "#### 1. The Heuristic Approach\n",
        "\n",
        "*   **How it works:**\n",
        "    1.  Divide all possible words into two simple buckets: \"Consistent\" (could be the answer) and \"Inconsistent\" (contradicts the clues).\n",
        "    2.  The \"Consistent\" words become the pool for our `positives`. The secret word is the best positive.\n",
        "    3.  The \"Inconsistent\" words become the pool for our `negatives`.\n",
        "    4.  **We then randomly sample** from these buckets to create the ranked lists. We don't try to determine if `REACT` is a \"better\" guess than `TRACE` among the consistent words.\n",
        "\n",
        "*   **Pros:**\n",
        "    *   **Fast and Simple:** This logic is very quick to compute. It only requires a few loops and set operations.\n",
        "    *   **Teaches the Core Task:** It effectively teaches the model the most important lesson: **don't make illogical guesses.** A model that learns to always pick from the \"Consistent\" bucket is already 90% of the way to being a good player.\n",
        "    *   **Good Enough for LoRA:** For parameter-efficient fine-tuning like LoRA, teaching this fundamental heuristic is often sufficient to see a massive improvement in performance.\n",
        "\n",
        "*   **Cons:**\n",
        "    *   **Not Truly Optimal:** It doesn't teach the model the subtle art of picking the *best* among all valid words. It treats all \"consistent\" words as equally good, which isn't true.\n",
        "\n",
        "#### 2. The Information Theory Approach\n",
        "\n",
        "*   **How it works:**\n",
        "    1.  This is the method used by solvers like 3Blue1Brown's. For *every possible guess* (from the entire dictionary of ~13,000 words)...\n",
        "    2.  ...and for *every remaining possible answer*...\n",
        "    3.  ...calculate the feedback pattern you would get.\n",
        "    4.  Group the possible answers by these feedback patterns.\n",
        "    5.  Calculate the \"entropy\" or \"information gain\" for that guess. A guess is good if it splits the remaining possible answers into many small, evenly-sized groups.\n",
        "    6.  The guess with the highest information gain is the **optimal** next move.\n",
        "\n",
        "*   **Pros:**\n",
        "    *   **Mathematically Optimal:** This method is guaranteed to find the guess that, on average, provides the most new information and narrows down the possibilities fastest.\n",
        "    *   **Creates a \"Perfect\" Ranking:** You could use this method to create a perfectly ranked list of `positives` for your GRPO dataset, teaching the model not just to be logical, but to be a master strategist.\n",
        "\n",
        "*   **Cons:**\n",
        "    *   **Extremely Computationally Expensive:** For a single turn where there are 2,000 possible answers left, this calculation involves `13,000 (guesses) * 2,000 (answers)` operations just to find the single best next word. Doing this to generate thousands of ranked lists for a dataset would take a very, very long time.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "\n",
        "# Training Steps\n",
        "\n",
        "| Stage | **SFT-LoRA (Phase 1)** | **GRPO-LoRA (Phase 2)** |\n",
        "| :--- | :--- | :--- |\n",
        "| **Purpose** | Teach format & style | Teach preference & strategy |\n",
        "| **Input Model**| Base LLM (e.g., gemma3) | SFT-LoRA-tuned model |\n",
        "| **Data Format**| `(prompt, completion)` | `(prompt, positives, negatives)` |\n",
        "| **Output** | A competent model | An optimized model |\n",
        "\n",
        "\n",
        "\n",
        "## Without SFT\n",
        "\n",
        "\n",
        "## With SFT\n",
        "\n",
        "Running with SFT did help a lot with rewards, since the model now outputs 5 letters and guess tags almost all the time.\n",
        "at step 34, started seeing better guesses: ❌ Did not guess 'REARM' in 6 trials. guesses: ['TRAYS', 'STARS', 'SERVE', 'SEARS', 'RESVE', 'SRESV']\n",
        "\n",
        "![SFT Training Loss Curve](./docs/images/lora_sft_loss_curve_20250629-081953.png)\n",
        "\n",
        "### Data generation\n",
        "\n",
        "* [sft_wordle_cot_data_generation.py](sft_wordle_cot_data_generation.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lessons learned\n",
        "\n",
        "1. Smaller models have hard time following instructions. In the begining I had loss nan and it turned out the model was not even outputing <guess></guess> tags\n",
        "\n",
        "2. Quantized models seems to have lose quality dramatically. The same model 3b-it-bf16 vs qat-4 have totally different qualtiy\n",
        "\n",
        "3. How often to update the reference model, since the small models are not very good at following instructions, we want to update the reference model more often to encourage the qualtiy to go higher faster.\n",
        "\n",
        "4. System prompt matters. Orginally I started with this prompt on the non quantized model but then when I passed to the quantized model, it seems too much for the quantized model to follow it. When I adjusted the prompt, I was able to start the training, it was not outputing guesses before.\n",
        "\n",
        "5. Rewards should not be all or nothing. For example when I added partial credit rewards function where the model gets rewards for guessing a letter or more in the word the rewards increased and the model training got better in the sense we had less and less no guesses. \n",
        "\n",
        "6. How to estimate if a model can run on my machine (see below)\n",
        "\n",
        "7. Model collapse from Gradient explosion, the model start just generating: `<think><pad><pad>....<pad></think>` this is cause by an jump in loss and then weights updates. here is a trace: \n",
        "```\n",
        "Step 0010 | Train Loss: 0.3947 | Best Guess: 'BRISKS' on 'LOYAL' (R: 1.10)\n",
        "GRPO Training Steps:   8%|▊         | 15/200 [02:13<26:22,  8.55s/it]\n",
        "Step 0015 | Train Loss: 1569.6305 | Best Guess: 'BRISKS' on 'BATTY' (R: 4.30)\n",
        "GRPO Training Steps:   8%|▊         | 15/200 [02:19<26:22,  8.55s/it, skipped=1]\n",
        "Skipping batch with max reward 0.00 on the following generations:\n",
        "--  <think><pad><pad>...</think>\n",
        "--  <think><pad><pad>...</think>\n",
        "```\n",
        "In order to prevent this, applied clipping on the advantages and the ratio.\n",
        "**The Goal:** To prevent a single outlier reward from having a disproportionate impact on the training update.\n",
        "\n",
        "First, remember what we do to the advantages: we **normalize** them. This means we adjust them so that their batch-wise average is 0 and their standard deviation is 1.\n",
        "\n",
        "-   **The \"Normal\" Case:** In a standard normal distribution, almost all values (~99.7%) fall within +/- 3 standard deviations of the mean. So, most of our normalized advantage values will naturally be in a small range like `[-3, 3]`.\n",
        "\n",
        "-   **The \"Outlier\" Problem:** What if one of your two generations is \"normal\" (reward: 4.0) but the other is a bizarre, malformed output that gets a reward of `0.1`? The difference is huge. After normalization, this could create an unusually large advantage value for the \"good\" generation. If we feed this huge advantage directly into the loss, it will dominate the gradient and tell the model to \"do this one thing at all costs,\" which can destabilize learning.\n",
        "\n",
        "Increase the number of generations from 2 to 4 should minimize those exploding ones since the advantage would be a bit smoother.\n",
        "\n",
        "8. Quantization and Dequantization on MLX: save a model that has a mix of layer types (some are QuantizedLinear from the base model, some are newly fused Linear layers). The MLX runtime doesn't know how to handle this inconsistent state.\n",
        "\n",
        "The solution is to de-quantize the entire model in memory right before you fuse the weights. This way, you get the best of both worlds:\n",
        "\n",
        "Training: You can continue to train using the memory-efficient gemma-3-1b-it-qat-8bit model, which works perfectly on your 16GB Mac.\n",
        "Merging: The script will load the quantized model and your adapters, convert the base model to full precision in memory, and then merge the weights. The final saved model will be a consistent, full-precision model, completely avoiding the Metal runtime error.\n",
        "\n",
        "9. Use LoRA with GRPO and only one base model to calculate the loss. This is done by only passing the trainable parameters (LoRA) since the base model is frozen and each time we calculate the loss based on the current policy compared with the old policy. Without this, my mac was getting out of memory and shutting down at step 1 or 2.\n",
        "\n",
        "\n",
        "10. Reward functions can have two purposes if we dont have enough data:\n",
        "   Let's walk through the first turn of a game for the secret word **`AUDIO`**.\n",
        "\n",
        "We will clearly distinguish between:\n",
        "*   **Part A: Game State Advancement** (What the player sees, how the game progresses)\n",
        "*   **Part B: What the Loss Function Sees** (The data collected for training)\n",
        "\n",
        "### Scenario Setup\n",
        "\n",
        "*   **Secret Word:** `AUDIO`\n",
        "*   **Config:** `config.rl.num_generations = 3` (The model will generate 3 guesses per turn)\n",
        "\n",
        "---\n",
        "\n",
        "### **Turn 1: The Starting Guess**\n",
        "\n",
        "#### Part A: Game State Advancement\n",
        "\n",
        "1.  **Create Prompt:** It's the first turn, so `past_feedback` is empty.\n",
        "    *   The `format_prompt_for_model` function creates the initial prompt: `\"This is the first turn. Please provide your best starting word.\"`\n",
        "\n",
        "2.  **Generate Guesses (Explore Actions):** The model receives this prompt and, because `num_generations=3`, it generates three different potential starting words. Let's imagine these are its outputs:\n",
        "\n",
        "    *   **Generation 1:** `ARISE`\n",
        "    *   **Generation 2:** `ADIEU`\n",
        "    *   **Generation 3:** `ROATE`\n",
        "\n",
        "3.  **Evaluate and Score Guesses:** The `play_wordle_game` loop now calculates the reward for each of these three guesses against the secret word `AUDIO`.\n",
        "\n",
        "    *   **Guess 1 (`ARISE`):** Finds the `A` and `I`. Let's say `calculate_total_reward` gives it a **Reward of 2.5**.\n",
        "    *   **Guess 2 (`ADIEU`):** Finds `A`, `D`, `I`, `U`. This is a fantastic starting guess for `AUDIO`. It will get a much higher reward. Let's say **Reward of 7.0**.\n",
        "    *   **Guess 3 (`ROATE`):** Finds `A` and `O`. Let's say **Reward of 3.0**.\n",
        "\n",
        "4.  **Select the Best Guess (Commit to a Path):** The system sees that `ADIEU` has the highest reward (7.0). It selects this as the \"official\" move for Turn 1.\n",
        "\n",
        "5.  **Advance the Game State:**\n",
        "    *   The `best_guess` is now `ADIEU`.\n",
        "    *   The system calculates the official Wordle feedback for this single best guess: `get_feedback(\"ADIEU\", \"AUDIO\")` returns `✓ - ✓ - ✓ x`. (A-correct, D-wrong pos, I-correct, E-not in word, U-correct)\n",
        "    *   This feedback `✓ - ✓ - ✓ x` is now stored in the `past_feedback` list.\n",
        "    *   The game checks if `ADIEU == AUDIO`. It's false, so the game continues to Turn 2.\n",
        "\n",
        "**End of Turn 1 - Game State Summary:** The game has officially played `ADIEU` and is ready to use the feedback `✓ - ✓ - ✓ x` to build the prompt for the next turn.\n",
        "\n",
        "---\n",
        "\n",
        "#### Part B: What the Loss Function Sees from Turn 1\n",
        "\n",
        "At the end of the game, the `GameRollout` object is passed to the loss function. For now, it contains the three `GenerationAttempt` objects created in Turn 1.\n",
        "\n",
        "1.  **Input Data:** The loss function receives the list of attempts.\n",
        "\n",
        "2.  **Grouping:** It groups the attempts by their `prompt_string`. Since all three attempts came from the same initial prompt, they all go into a single group.\n",
        "\n",
        "3.  **The Data Inside the Group:** This is the crucial \"contrastive\" data the loss function will use to learn. It looks like this:\n",
        "\n",
        "| `prompt_string` | `full_response` | `reward` | `log_probs` |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| \"This is the first turn...\" | `<think>...</think><guess>ARISE</guess>` | `2.5` | `[...]` |\n",
        "| \"This is the first turn...\" | `<think>...</think><guess>ADIEU</guess>` | `7.0` | `[...]` |\n",
        "| \"This is the first turn...\" | `<think>...</think><guess>ROATE</guess>` | `3.0` | `[...]` |\n",
        "\n",
        "4.  **Loss Calculation:** The `grpo_loss_fn` performs its calculations on this group:\n",
        "    *   **Calculates Mean Reward:** `mean(2.5, 7.0, 3.0) ≈ 4.17`\n",
        "    *   **Calculates Advantages:**\n",
        "        *   `ARISE`: `2.5 - 4.17 = -1.67` (discourage)\n",
        "        *   `ADIEU`: `7.0 - 4.17 = +2.83` (encourage)\n",
        "        *   `ROATE`: `3.0 - 4.17 = -1.17` (discourage)\n",
        "    *   **Calculates Policy Ratios:** It uses the `log_probs` to get the ratio of how likely the *current* model is to say each word vs. the *reference* model.\n",
        "    *   **Objective:** The final PPO objective will push the model's weights to **increase the probability of generating `ADIEU`** and **decrease the probability of generating `ARISE` and `ROATE`** when given the initial prompt.\n",
        "\n",
        "This process will repeat for Turn 2, but now with a completely different prompt and a new set of generated guesses. For example, the prompt for Turn 2 would be something like:\n",
        "`\"**Clues so far:**\\n* Guess 1: ADIEU → A(✓) D(-) I(✓) E(x) U(✓)\"`\n",
        "\n",
        "The model would then generate three new guesses, a new \"best\" guess would be chosen (e.g. `AUDIO`), and a new group of three attempts would be added to the `GameRollout` for the loss function to process.\n",
        "\n",
        "### Exploration vs Exploitation\n",
        "\n",
        "\n",
        "Let's break down exactly how it maps to our Wordle bot's strategy.\n",
        "\n",
        "### Defining the Terms in Our Context\n",
        "\n",
        "*   **Exploration:** Trying out several different guesses (actions) to discover which ones give the most information and lead to the highest reward. You are \"exploring\" the space of possibilities, even if some seem less likely to be correct, because they might reveal valuable clues.\n",
        "*   **Exploitation:** Using the knowledge you already have to choose the single best guess (action) that you believe will maximize your immediate reward and chance of winning. You are \"exploiting\" your current best strategy.\n",
        "\n",
        "### How Our `play_wordle_game` Balances the Two\n",
        "\n",
        "The genius of this GRPO-style approach is that it does **both exploration and exploitation within every single turn**.\n",
        "\n",
        "#### The Exploration Step (Generating Variety)\n",
        "\n",
        "This is the `for i in range(num_generations):` loop.\n",
        "\n",
        "```python\n",
        "for i in range(num_generations):\n",
        "    text, tokens, logits = generate_with_log_probs(...)\n",
        "```\n",
        "\n",
        "When the model generates `ARISE`, `ADIEU`, and `ROATE` for the same prompt, it is **exploring**. Instead of just outputting the single word it thinks is most probable, the `sampler` (with a temperature > 0) encourages it to generate a diverse set of candidates. It's asking \"what if\" questions:\n",
        "*   \"What if I try a word with common consonants like `ARISE`?\"\n",
        "*   \"What if I try to knock out as many vowels as possible with `ADIEU`?\"\n",
        "*   \"What if I try this other common starter, `ROATE`?\"\n",
        "\n",
        "This step is all about **gathering information**.\n",
        "\n",
        "#### The Exploitation Step (Committing to the Best)\n",
        "\n",
        "This happens immediately after the exploration loop, when the code finds the guess with the highest reward.\n",
        "\n",
        "```python\n",
        "# Finds the index of the guess with the maximum reward\n",
        "best_generation_idx = ... \n",
        "# Selects that single best guess to be the \"official\" move\n",
        "best_attempt = current_turn_attempts[best_generation_idx]\n",
        "```\n",
        "ß\n",
        "This is the **exploitation** step. After exploring three options, the agent uses the information it just gathered (the calculated rewards) to exploit the best available action. It concluded that `ADIEU` was the most promising path and committed to it to advance the game.\n",
        "\n",
        "### The Overall Strategy\n",
        "\n",
        "So, the agent's strategy on each turn is:\n",
        "\n",
        "1.  **Explore:** Generate a diverse set of `N` possible guesses.\n",
        "2.  **Evaluate:** Quickly score all `N` exploratory guesses.\n",
        "3.  **Exploit:** Commit to the single guess that received the best score to make your official move.\n",
        "4.  **Learn from Everything:** Crucially, the data from *all* `N` guesses (the good, the bad, and the ugly) is sent to the loss function. This teaches the model to make its future *explorations* more intelligent. It learns to explore better options over time.\n",
        "\n",
        "This is a very powerful and effective way to handle the trade-off. Instead of randomly choosing to explore or exploit for an entire turn, it does a small, controlled burst of exploration and then immediately exploits the results of that exploration.\n",
        "\n",
        "\n",
        "## Additional details on system estimates for training\n",
        "\n",
        "The key on an M1 Mac is the **Unified Memory**. Both the CPU and GPU (MPS) use the same 16GB of RAM, so we need to account for everything that will be loaded into it.\n",
        "\n",
        "Here are the main components that consume memory:\n",
        "\n",
        "1.  **Model Weights (The Base Cost)**\n",
        "2.  **KV Cache (The Generation Cost)**\n",
        "3.  **Training Overhead (Gradients, Optimizer States, Activations)**\n",
        "4.  **System/App Memory (The Buffer)**\n",
        "---\n",
        "\n",
        "### Step 1: Calculate Model Weight Memory\n",
        "\n",
        "This is the easiest part. The formula is:\n",
        "`Memory = (Number of Parameters) * (Bytes per Parameter)`\n",
        "\n",
        "First, the bytes per parameter for different quantization levels:\n",
        "*   **bf16 / float16:** 2 bytes\n",
        "*   **8-bit:** 1 byte\n",
        "*   **4-bit:** 0.5 bytes (on average)\n",
        "*   **3-bit:** ~0.375 bytes\n",
        "\n",
        "In our script, we load **two** models: the main `model` and the `ref_model`. So we must double this cost.\n",
        "\n",
        "**Example Calculation (for the risky `gemma-3-4b-it-qat-4bit`):**\n",
        "*   Parameters: 4 billion (4e9)\n",
        "*   Bytes per parameter: 0.5 (for 4-bit)\n",
        "*   Memory per model: `4e9 * 0.5 = 2e9 bytes = 2 GB`\n",
        "*   **Total for both models:** `2 GB * 2 =` **4 GB**\n",
        "\n",
        "### Step 2: Estimate KV Cache Memory\n",
        "\n",
        "This is the hidden memory hog during text generation. Its size depends on the sequence length, the number of parallel generations, and the model's architecture.\n",
        "\n",
        "A simple rule of thumb is that the KV Cache size grows **linearly with sequence length and the number of generations**.\n",
        "\n",
        "Let's estimate it for our settings (`MAX_COMPLETION_LENGTH = 128`, `NUM_GENERATIONS = 2`):\n",
        "*   **Sequence Length:** The prompt length + 128. Let's say ~300 tokens total.\n",
        "*   **Generations:** 2 are running in parallel.\n",
        "*   For a 1B model, this is very manageable, likely under **1 GB**.\n",
        "*   For a 4B model, it would be larger, maybe **1-2 GB**.\n",
        "\n",
        "This is why changing `MAX_COMPLETION_LENGTH` from 128 to 512 (a 4x increase) had such a dramatic impact on memory.\n",
        "\n",
        "### Step 3: Estimate Training Overhead\n",
        "\n",
        "This includes the memory for gradients, optimizer states (AdamW is heavy), and layer activations during the forward/backward pass.\n",
        "\n",
        "*   **Gradients & Optimizer:** For LoRA, this is very small because we are only training a tiny fraction of the weights. For our `0.459M` trainable parameters, this is only a few megabytes, so it's negligible.\n",
        "*   **Activations:** This is the main cost here. It scales with `sequence_length * batch_size` (where batch size is `NUM_GENERATIONS`). With our low settings (`128` length, `2` generations), this is manageable. Let's budget **1-2 GB** for this, to be safe.\n",
        "\n",
        "### Step 4: The Final Budget (Putting It All Together)\n",
        "\n",
        "Let's create a budget for running the **risky `gemma-3-4b-it-qat-4bit` model** on your 16GB M1:\n",
        "\n",
        "| Component | Estimated Memory | Notes |\n",
        "| :--- | :--- | :--- |\n",
        "| Model Weights (`model` + `ref_model`) | ~4.0 GB | The base cost of loading two 4B 4-bit models. |\n",
        "| KV Cache (Peak during generation) | ~2.0 GB | With `MAX_COMPLETION_LENGTH=128`, `NUM_GENERATIONS=2`. |\n",
        "| Training Overhead (Activations) | ~2.0 GB | A safe estimate for our batch size and length. |\n",
        "| **Subtotal (Script Usage)** | **~8.0 GB** | |\n",
        "| OS & Other Apps | ~4.0 GB | A realistic buffer for macOS and other background tasks. |\n",
        "| **Safety Buffer** | ~2.0 GB | It's crucial not to use 100% of RAM. |\n",
        "| **ESTIMATED TOTAL PEAK USAGE**| **~14.0 GB**| |\n",
        "\n",
        "**The Verdict:** The total estimate of **14.0 GB** is very close to your 16 GB limit. This is why I called it \"risky.\" It would probably work, but if you had a few Chrome tabs open or the sequence length was slightly longer, it could easily start swapping to the SSD, causing a massive slowdown.\n",
        "\n",
        "By contrast, the **`gemma-3-1b-it-qat-4bit`** model we are using now has a base cost of just `2 * (1e9 * 0.5) = 1 GB`, making the total peak usage much lower and safer.\n",
        "\n",
        "### The KV Cache Formula\n",
        "\n",
        "The size of the Key-Value cache can be calculated with the following formula:\n",
        "\n",
        "`Cache Size = 2 * num_layers * sequence_length * hidden_dim * NUM_GENERATIONS * bytes_per_element`\n",
        "\n",
        "Let's break down each term and plug in the numbers for our specific case.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Finding the Model's Architectural Details\n",
        "\n",
        "We need `num_layers` and `hidden_dim` for the `gemma-3-1b` model. Since that specific model isn't released yet, we'll use the numbers from a very similar public model, **Gemma 2B**, as a close proxy. (The actual Gemma 1B would be even smaller).\n",
        "\n",
        "*   `num_layers`: **18**\n",
        "*   `hidden_dim` (hidden dimension size): **2048**\n",
        "\n",
        "### 2. Getting Parameters from Our Script\n",
        "\n",
        "*   `sequence_length`: This is the length of the prompt plus the max completion length. Let's assume a generous average prompt length of ~170 tokens.\n",
        "    `170 (prompt) + 128 (completion) ≈` **300 tokens**\n",
        "*   `NUM_GENERATIONS`: We set this to **2** to save memory.\n",
        "*   `bytes_per_element`: The KV cache is typically stored in `float16` or `bfloat16` for performance, even when using a quantized model. So, we use **2 bytes**.\n",
        "*   The `2` at the start of the formula is because for each item, we store both a **K**ey and a **V**alue.\n",
        "\n",
        "### 3. The Calculation\n",
        "\n",
        "Now, let's plug everything into the formula:\n",
        "\n",
        "`Cache Size = 2 (K/V) * 18 (layers) * 300 (tokens) * 2048 (hidden_dim) * 2 (generations) * 2 (bytes)`\n",
        "\n",
        "Let's do the math:\n",
        "\n",
        "1.  `2 * 18 * 300 * 2048 * 2 * 2`\n",
        "2.  `= 36 * 300 * 2048 * 4`\n",
        "3.  `= 10,800 * 2048 * 4`\n",
        "4.  `= 22,118,400 * 4`\n",
        "5.  `=` **88,473,600 bytes**\n",
        "\n",
        "### 4. The Result\n",
        "\n",
        "To make that number easier to understand, let's convert it to megabytes:\n",
        "\n",
        "`88,473,600 bytes / (1024 * 1024) ≈` **84.4 MB**\n",
        "\n",
        "So, the precise estimate for the peak KV Cache size with our current settings is only about **84 MB**.\n",
        "\n",
        "My initial heuristic of \"under 1 GB\" was extremely safe. This calculation proves that with our optimized settings (`NUM_GENERATIONS=2`, `MAX_COMPLETION_LENGTH=128`), the memory impact of the KV cache is very small and easily manageable for your system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/charbelk/dev/wordle/wordle-rl/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model:  mlx-community/gemma-3-1b-it-qat-8bit\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 8 files: 100%|██████████| 8/8 [00:00<00:00, 138084.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- MLX Model Memory Details for 'mlx-community/gemma-3-1b-it-qat-8bit' ---\n",
            "[*] Target Data Type: int8\n",
            "--------------------------------------------------------------------------\n",
            "[Model State (All Parameters)]\n",
            "  - Count:      366.25 M\n",
            "  - Memory (GB): 0.341 GB\n",
            "\n",
            "[Gradients (Trainable Only)]\n",
            "  - Count:      0.134 M\n",
            "  - Memory (GB): 0.000 GB\n",
            "\n",
            "[Total Estimated Usage]\n",
            "  - Memory (GB): 0.341 GB\n",
            "--------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import mlx.core as mx\n",
        "import mlx.nn as nn\n",
        "from mlx.utils import tree_flatten\n",
        "from mlx_lm import load\n",
        "\n",
        "import mlx.core as mx\n",
        "import mlx.nn as nn\n",
        "from mlx.utils import tree_flatten\n",
        "from mlx_lm import load\n",
        "\n",
        "def calculate_mlx_memory_usage(\n",
        "    model: nn.Module,\n",
        "    model_name: str = None,\n",
        "    dtype: mx.Dtype = mx.float32,\n",
        "    pretty_print: bool = True\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates and optionally prints the estimated memory usage of an MLX model.\n",
        "\n",
        "    This function estimates the total memory required to store the model's\n",
        "    parameters and their gradients, providing a formatted printout.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The MLX model.\n",
        "        model_name (str, optional): The name of the model for display purposes.\n",
        "        dtype (mx.Dtype, optional): The data type to assume for parameters\n",
        "            and gradients. Defaults to mx.float32.\n",
        "        pretty_print (bool, optional): If True, prints a formatted summary\n",
        "            of the memory usage. Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the memory usage in gigabytes (GB) and\n",
        "              element counts for parameters, gradients, and the total.\n",
        "    \"\"\"\n",
        "    # Get the size in bytes of a single element of the given data type\n",
        "    element_size = mx.array(0, dtype=dtype).itemsize\n",
        "\n",
        "    # Calculate memory for all model parameters (the model's state)\n",
        "    total_params_elements = sum(p.size for _, p in tree_flatten(model.parameters()))\n",
        "    params_bytes = total_params_elements * element_size\n",
        "    params_gb = params_bytes / (1024**3)\n",
        "\n",
        "    # Calculate memory for gradients (only for trainable parameters)\n",
        "    grad_params_elements = sum(p.size for _, p in tree_flatten(model.trainable_parameters()))\n",
        "    grads_bytes = grad_params_elements * element_size\n",
        "    grads_gb = grads_bytes / (1024**3)\n",
        "\n",
        "    # Calculate total memory\n",
        "    total_memory_bytes = params_bytes + grads_bytes\n",
        "    total_gb = total_memory_bytes / (1024**3)\n",
        "\n",
        "    if pretty_print:\n",
        "        params_m = total_params_elements / 1e6\n",
        "        grads_m = grad_params_elements / 1e6\n",
        "\n",
        "        # [THE CHANGE] Create a dynamic header\n",
        "        header = \"--- MLX Model Memory Details\"\n",
        "        if model_name:\n",
        "            header += f\" for '{model_name}'\"\n",
        "        header += \" ---\"\n",
        "        \n",
        "        print(f\"\\n{header}\")\n",
        "        print(f\"[*] Target Data Type: {str(dtype).split('.')[-1]}\")\n",
        "        print(\"-\" * (len(header) - 1)) # Match the header length\n",
        "        print(\"[Model State (All Parameters)]\")\n",
        "        print(f\"  - Count:      {params_m:,.2f} M\")\n",
        "        print(f\"  - Memory (GB): {params_gb:.3f} GB\")\n",
        "        print(\"\\n[Gradients (Trainable Only)]\")\n",
        "        print(f\"  - Count:      {grads_m:,.3f} M\")\n",
        "        print(f\"  - Memory (GB): {grads_gb:.3f} GB\")\n",
        "        print(\"\\n[Total Estimated Usage]\")\n",
        "        print(f\"  - Memory (GB): {total_gb:.3f} GB\")\n",
        "        print(\"-\" * (len(header) - 1))\n",
        "\n",
        "    return {\n",
        "        \"params_gb\": params_gb,\n",
        "        \"grads_gb\": grads_gb,\n",
        "        \"total_gb\": total_gb,\n",
        "        \"total_params_elements\": total_params_elements,\n",
        "        \"grad_params_elements\": grad_params_elements,\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "# --- Example Usage ---\n",
        "model_id = \"mlx-community/gemma-3-1b-it-qat-8bit\"\n",
        "# 1. Load a model (e.g., the one from your training script)\n",
        "#    Using a small model for a quick example.\n",
        "print(\"Loading model: \", model_id)\n",
        "model, tokenizer = load(model_id)\n",
        "\n",
        "mem_f32 = calculate_mlx_memory_usage(model, model_id, dtype=mx.int8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Additional details on system prompt\n",
        "** Prompt used with non quantized model **\n",
        "```\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are an expert Wordle solver. Your task is to deduce a secret 5-letter word in 6 tries or less by making intelligent guesses and logically analyzing the feedback you receive.\n",
        "\n",
        "### GAME RULES\n",
        "1.  **Objective:** Guess a secret 5-letter English word.\n",
        "2.  **Guesses:** You have a maximum of 6 tries. Each guess must be a valid 5-letter word.\n",
        "3.  **Feedback:** After each guess, every letter receives feedback:\n",
        "    *   **✓ (Correct):** The letter is in the word and in the correct position.\n",
        "    *   **- (Misplaced):** The letter is in the word but in the wrong position.\n",
        "    *   **x (Incorrect):** The letter is not in the word at all.\n",
        "\n",
        "### YOUR TASK\n",
        "For each turn, you must respond in two parts: a thought process and a guess.\n",
        "\n",
        "1.  **Think (`<think>`):** First, carefully analyze all the feedback from previous guesses. Reason step-by-step to determine your next best guess. Your reasoning must include:\n",
        "    *   **Constraint Analysis:** Summarize what you know.\n",
        "        *   Correctly placed letters (`✓`).\n",
        "        *   Misplaced letters (`-`) and the positions they are known *not* to be in.\n",
        "        *   Incorrect letters (`x`) that must be excluded.\n",
        "    *   **Strategy Synthesis:** Combine all constraints to narrow down the possibilities. State your strategy for the next guess (e.g., testing a common letter, eliminating possibilities, or attempting a direct solution).\n",
        "    *   **Candidate Selection:** Propose a word that fits all the constraints and explain why it's a strong choice.\n",
        "\n",
        "2.  **Guess (`<guess>`):** After your reasoning, provide your chosen 5-letter word inside `<guess>` tags.\n",
        "\n",
        "---\n",
        "### EXAMPLES\n",
        "\n",
        "**Example 1: First Guess (No prior feedback)**\n",
        "\n",
        "<think>\n",
        "This is the first guess, so there is no feedback to analyze. A good starting word should use common, distinct letters to maximize the information gained. 'RAISE' is an excellent choice because it tests three common vowels (A, I, E) and two common consonants (R, S). This will help narrow down the possibilities significantly.\n",
        "</think>\n",
        "<guess>RAISE</guess>\n",
        "\n",
        "**Example 2: Guess with Prior Feedback**\n",
        "\n",
        "**Context:**\n",
        "*   **Guess 1:** STORM → Feedback: S(-) T(x) O(x) R(-) M(x)\n",
        "*   **Guess 2:** BRAVE → Feedback: B(✓) R(✓) A(x) V(x) E(x)\n",
        "\n",
        "<think>\n",
        "**1. Constraint Analysis:**\n",
        "*   **Correct (✓):** `B` is in position 1. `R` is in position 2. The word starts with `BR___`.\n",
        "*   **Misplaced (-):** `S` is in the word, but not in position 1 (from STORM).\n",
        "*   **Incorrect (x):** `T, O, M, A, V, E` are not in the word.\n",
        "\n",
        "**2. Strategy Synthesis:**\n",
        "*   The word has the structure `BR_ _ _`.\n",
        "*   It must contain an `S` somewhere in the last three positions.\n",
        "*   It must not contain any of the incorrect letters.\n",
        "\n",
        "**3. Candidate Selection:**\n",
        "I need to find a word that fits the pattern `BR_S_` or `BR__S`. Let's try to fill the blanks with the remaining available letters. A common word that fits this pattern is `BRISK`.\n",
        "*   `B` is in position 1. (✓)\n",
        "*   `R` is in position 2. (✓)\n",
        "*   It contains an `S`. (✓)\n",
        "*   It does not use any eliminated letters (`T, O, M, A, V, E`). (✓)\n",
        "*   `BRISK` is a valid 5-letter word. This is a very strong candidate.\n",
        "</think>\n",
        "<guess>BRISK</guess>\n",
        "\"\"\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2M8F1N0SPa7"
      },
      "source": [
        "# Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edOMHedgB8jq"
      },
      "source": [
        "## Pytorch helpers\n",
        "\n",
        "Some pytorch helper utils to allow us better understanding the next steps:\n",
        "\n",
        "* [gather](https://docs.pytorch.org/docs/stable/generated/torch.gather.html#torch-gather): Gathers values along an axis specified by dim and the index (which indicates the indices of elements to gather).\n",
        "* [unsqueeze](https://docs.pytorch.org/docs/stable/generated/torch.unsqueeze.html): Returns a new tensor with a dimension of size one inserted at the specified position.\n",
        "* [squeeze](https://docs.pytorch.org/docs/stable/generated/torch.squeeze.html#torch-squeeze): Returns a tensor with all specified dimensions of input of size 1 removed.\n",
        "\n",
        "* Unsqeeze: allows us to go from inputs_id with shape (batch_size, sequence_length) to (batch_size, sequence_length, 1). This is done to make the shape compatible with the matrices we have already in the transformer (batch_size, sequence_length, vocab_size).\n",
        "\n",
        "* Squeeze: allows us to go back from an unsqueeze."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k9hQhEKKvEy",
        "outputId": "3599274d-01c7-4c09-e0c5-acf6eeada7e2"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([1, 2, 3])\n",
        "print(f\"Original tensor x:\\n{x}\")\n",
        "print(f\"Original shape: {x.shape}\")\n",
        "\n",
        "x_unsqueezed_0 = x.unsqueeze(dim=0) # Add dimension at position 0\n",
        "print(f\"\\nAfter x.unsqueeze(dim=0):\\n{x_unsqueezed_0}\")\n",
        "print(f\"New shape: {x_unsqueezed_0.shape}\") # Shape: [1, 3]\n",
        "\n",
        "# Example 1.2: Adding a dimension at the end\n",
        "x_unsqueezed_neg1 = x.unsqueeze(dim=-1) # Add dimension at position -1 (last)\n",
        "# Equivalent to x.unsqueeze(dim=1) for a 1D tensor\n",
        "print(f\"\\nAfter x.unsqueeze(dim=-1):\\n{x_unsqueezed_neg1}\")\n",
        "print(f\"New shape: {x_unsqueezed_neg1.shape}\") # Shape: [3, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRzBSb-NK4Fc",
        "outputId": "694ad1a5-3070-4ada-9e65-7303aaf3e0cd"
      },
      "outputs": [],
      "source": [
        "y = torch.tensor([[1, 2], [3, 4]])\n",
        "print(f\"\\nOriginal tensor y:\\n{y}\")\n",
        "print(f\"Original shape: {y.shape}\") # Shape: [2, 2]\n",
        "\n",
        "y_unsqueezed_1 = y.unsqueeze(dim=1) # Add dimension at position 1\n",
        "print(f\"\\nAfter y.unsqueeze(dim=1):\\n{y_unsqueezed_1}\")\n",
        "print(f\"New shape: {y_unsqueezed_1.shape}\") # Shape: [2, 1, 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCji_NwJDxIT",
        "outputId": "e94da52b-4b69-4b4f-d60b-0a2be23bdf35"
      },
      "outputs": [],
      "source": [
        "input_ids, _, _ = prepare_inputs(prompt, \"fence and\")\n",
        "print('inputs_ids', input_ids)\n",
        "print('inputs_ids.shape', input_ids.shape)\n",
        "print('inputs_ids_unsqueeze(-1)', input_ids.unsqueeze(-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvDoF7bmEmZF"
      },
      "source": [
        "Squeeze it back"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HINxWc5lElVT",
        "outputId": "8718a373-2817-4c80-cd46-2e0cba70b9db"
      },
      "outputs": [],
      "source": [
        "print('inputs_ids_squeeze(-1)', input_ids.unsqueeze(dim=-1).squeeze(dim=-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uE0HvbKLCZ_",
        "outputId": "ed95d10a-7d08-496a-dd07-d3169712c106"
      },
      "outputs": [],
      "source": [
        "# Example 2.1: Using the output from unsqueeze\n",
        "z = torch.tensor([[[1, 2, 3]]]) # Shape: [1, 1, 3]\n",
        "print(f\"Original tensor z:\\n{z}\")\n",
        "print(f\"Original shape: {z.shape}\")\n",
        "\n",
        "z_squeezed_all = z.squeeze() # Remove all dimensions of size 1\n",
        "print(f\"\\nAfter z.squeeze():\\n{z_squeezed_all}\")\n",
        "print(f\"New shape: {z_squeezed_all.shape}\") # Shape: [3]\n",
        "\n",
        "# Example 2.2: Squeezing a specific dimension\n",
        "z_squeezed_dim0 = z.squeeze(dim=0) # Remove dimension 0 (if size 1)\n",
        "print(f\"\\nAfter z.squeeze(dim=0):\\n{z_squeezed_dim0}\")\n",
        "print(f\"New shape: {z_squeezed_dim0.shape}\") # Shape: [1, 3]\n",
        "\n",
        "# Example 2.3: Trying to squeeze a dimension not of size 1\n",
        "w = torch.tensor([[1, 2, 3], [4, 5, 6]]) # Shape: [2, 3]\n",
        "print(f\"\\nOriginal tensor w:\\n{w}\")\n",
        "print(f\"Original shape: {w.shape}\")\n",
        "\n",
        "w_squeezed_dim0 = w.squeeze(dim=0) # Dim 0 has size 2, so no change\n",
        "print(f\"\\nAfter w.squeeze(dim=0):\\n{w_squeezed_dim0}\")\n",
        "print(f\"New shape: {w_squeezed_dim0.shape}\") # Shape: [2, 3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2MQtwnoE2UZ",
        "outputId": "e3aafb6f-706a-41ad-a31e-e0d686e8b221"
      },
      "outputs": [],
      "source": [
        "source_tensor = torch.tensor([10, 20, 30, 40, 50])\n",
        "indices = torch.tensor([0, 2, 4, 1]) # Indices to pick from source_tensor\n",
        "print(f\"Source tensor:\\n{source_tensor}\")\n",
        "print(f\"Indices:\\n{indices}\")\n",
        "\n",
        "gathered_values_1d = torch.gather(source_tensor, dim=0, index=indices)\n",
        "print(f\"\\nGathered values (dim=0):\\n{gathered_values_1d}\") # Output: [10, 30, 50, 20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqBBqxXZaJ3F"
      },
      "source": [
        "## Cross Entropy vs Manual Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WyQkZ6YaQLh",
        "outputId": "e72c3454-5e9f-46b8-d81c-e52c4116e53b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Sample data\n",
        "logits = torch.tensor(\n",
        "    [[2.0, 1.0, 0.1],\n",
        "     [0.5, 2.5, 0.3]])  # Shape: (2, 3)\n",
        "targets = torch.tensor([0, 2])  # Shape: (2,)\n",
        "\n",
        "\n",
        "# Manual loss using torch.gather\n",
        "log_softmax_logits = F.log_softmax(logits, dim=1)  # Shape: (2, 3)\n",
        "selected_log_probs = torch.gather(\n",
        "    input=log_softmax_logits,\n",
        "    dim=1,\n",
        "    index=targets.unsqueeze(1), # Shape 2, 1\n",
        ").squeeze(1)  # Shape: (2,)\n",
        "manual_loss = -selected_log_probs.mean()  # Averaging over the batch\n",
        "\n",
        "\n",
        "# PyTorch loss\n",
        "cross_entropy_loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "print(manual_loss, cross_entropy_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObUKKw2naYPl"
      },
      "source": [
        "So, above, we can see that the two implementations are equivalent, but let's narrow down a bit further to the torch.gather mechanics\n",
        "Consider the following two tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-ewa7ZuaR3r"
      },
      "outputs": [],
      "source": [
        "t = torch.tensor(\n",
        "  [[1., 2.,],\n",
        "   [3., 4.]]\n",
        ")\n",
        "\n",
        "m = torch.tensor(\n",
        "  [[1, 1],\n",
        "   [0, 1]]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08q44TxgahcX"
      },
      "source": [
        "- Above, `t` is a tensor we want to select from, and `m` is a mask to specify how we want to select\n",
        " - For instance, since `m` contains `[1, 1]` n the first row, it will select two times the value of `t` in index position `1`, which is the value 2.\n",
        " - The second row of `m`, `[0, 1]`, selects index positions 0 and 1 in the second row or `t`, which are `3.` and `4.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hEEp2rtahJc",
        "outputId": "02885802-9b06-410d-8da7-7fcf9b6cb472"
      },
      "outputs": [],
      "source": [
        "torch.gather(input=t, dim=-1, index=m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UUdlK0RanWy"
      },
      "source": [
        "- In other words, `torch.gather` is a selection function\n",
        "- When we computed the loss earlier, we used it to retrieve the log probabilities corresponding to the correct token in the 50,257-token vocabulary\n",
        "- The \"correct\" tokens are the tokens given in the response entry.\n",
        "\n",
        "- We use `torch.gather` because it gives us a bit more control than `cross_entropy`, but is, in essence, a similar idea\n",
        "- The `selection_mask` we use there is to optionally ignore prompt and padding tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HERoV41EakZC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0557f1f45f0b4907af863a8a72835681": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07be4a333b4b4a2b9b8b4bb5d50959b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ced5eefafca4f22a7782a43218f4973": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_allow_html": false,
            "disabled": false,
            "layout": "IPY_MODEL_9fa176cec32647efbde9f55db23cdb05",
            "placeholder": "​",
            "style": "IPY_MODEL_86aacef10471405abd0141c3519a10e5",
            "tabbable": null,
            "tooltip": null,
            "value": ""
          }
        },
        "161a74daffd341c1a30ee4216e802aa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_83b1205ce14f4b07a1cf31a0f0246920",
            "placeholder": "​",
            "style": "IPY_MODEL_1eefccc6b30a4e369a69e97abe45ca39",
            "tabbable": null,
            "tooltip": null,
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "1eefccc6b30a4e369a69e97abe45ca39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HTMLStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "44543b607c02463a9141ce91fca56f42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HTMLStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "4ce285969691429aa3d9aaca171909df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_0557f1f45f0b4907af863a8a72835681",
            "placeholder": "​",
            "style": "IPY_MODEL_44543b607c02463a9141ce91fca56f42",
            "tabbable": null,
            "tooltip": null,
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "7165a31c15ce4cf0a3b4fcf3a5647208": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "805af9e5f5954cb48d416f9c6d6071cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "CheckboxStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "CheckboxStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": ""
          }
        },
        "83b1205ce14f4b07a1cf31a0f0246920": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86aacef10471405abd0141c3519a10e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "TextStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "TextStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "92b13518080d40b3bef2fc5d2407c642": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_family": null,
            "font_size": null,
            "font_style": null,
            "font_variant": null,
            "font_weight": null,
            "text_color": null,
            "text_decoration": null
          }
        },
        "9caac590a10946058e19156009fbd95a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "9fa176cec32647efbde9f55db23cdb05": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "2.0.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d204612862634a1a82b146269bfbbfc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_allow_html": false,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_07be4a333b4b4a2b9b8b4bb5d50959b8",
            "style": "IPY_MODEL_805af9e5f5954cb48d416f9c6d6071cb",
            "tabbable": null,
            "tooltip": null,
            "value": true
          }
        },
        "efed8287ef094ebb80f6c5e3ad414201": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ce285969691429aa3d9aaca171909df",
              "IPY_MODEL_0ced5eefafca4f22a7782a43218f4973",
              "IPY_MODEL_d204612862634a1a82b146269bfbbfc0",
              "IPY_MODEL_fde49001fb57479ba34610192d175a62",
              "IPY_MODEL_161a74daffd341c1a30ee4216e802aa0"
            ],
            "layout": "IPY_MODEL_9caac590a10946058e19156009fbd95a",
            "tabbable": null,
            "tooltip": null
          }
        },
        "fde49001fb57479ba34610192d175a62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "2.0.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_7165a31c15ce4cf0a3b4fcf3a5647208",
            "style": "IPY_MODEL_92b13518080d40b3bef2fc5d2407c642",
            "tabbable": null,
            "tooltip": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
